{"cells":[{"cell_type":"markdown","metadata":{"id":"rX47r-g42bUQ"},"source":["# Text classification with Transformer\n","\n","**Author:** [Apoorv Nandan](https://twitter.com/NandanApoorv)<br>\n","**Date created:** 2020/05/10<br>\n","**Last modified:** 2020/05/10<br>\n","**Description:** Implement a Transformer block as a Keras layer and use it for text classification."]},{"cell_type":"markdown","metadata":{"id":"OfCfl4_r2bUV"},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OJFYHDws2bUW"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers"]},{"cell_type":"markdown","metadata":{"id":"Po_reW2e2bUb"},"source":["## Implement a Transformer block as a layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C3G0KJcJ2bUc"},"outputs":[],"source":["class TransformerBlock(layers.Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n","        super().__init__()\n","        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.ffn = keras.Sequential(\n","            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = layers.Dropout(rate)\n","        self.dropout2 = layers.Dropout(rate)\n","\n","    def call(self, inputs, training):\n","        attn_output = self.att(inputs, inputs)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)"]},{"cell_type":"markdown","metadata":{"id":"BIGO4WRG2bUd"},"source":["## Implement embedding layer\n","\n","Two seperate embedding layers, one for tokens, one for token index (positions)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"niH4E5c82bUd"},"outputs":[],"source":["class TokenAndPositionEmbedding(layers.Layer):\n","    def __init__(self, maxlen, vocab_size, embed_dim):\n","        super().__init__()\n","        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n","        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n","\n","    def call(self, x):\n","        maxlen = tf.shape(x)[-1]\n","        positions = tf.range(start=0, limit=maxlen, delta=1)\n","        positions = self.pos_emb(positions)\n","        x = self.token_emb(x)\n","        return x + positions"]},{"cell_type":"markdown","metadata":{"id":"8NO3WmMR2bUe"},"source":["## Download and prepare dataset"]},{"cell_type":"code","source":[],"metadata":{"id":"j5gCw0v1JieZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TNQOFEf92bUf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677069650981,"user_tz":-60,"elapsed":4846,"user":{"displayName":"Jean-Christophe Bouquin","userId":"01239462757709491259"}},"outputId":"9d4b7f6b-ad84-4126-f6d0-642fc3fbf99f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17464789/17464789 [==============================] - 0s 0us/step\n","25000 Training sequences\n","25000 Validation sequences\n"]}],"source":["vocab_size = 20000  # Only consider the top 20k words\n","maxlen = 200  # Only consider the first 200 words of each movie review\n","(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n","print(len(x_train), \"Training sequences\")\n","print(len(x_val), \"Validation sequences\")\n","x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n","x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)"]},{"cell_type":"markdown","metadata":{"id":"kVcUcgp42bUf"},"source":["## Create classifier model using transformer layer\n","\n","Transformer layer outputs one vector for each time step of our input sequence.\n","Here, we take the mean across all time steps and\n","use a feed forward network on top of it to classify text."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w_UYPqN-2bUg"},"outputs":[],"source":["embed_dim = 32  # Embedding size for each token\n","num_heads = 2  # Number of attention heads\n","ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n","\n","inputs = layers.Input(shape=(maxlen,))\n","embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n","x = embedding_layer(inputs)\n","transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n","x = transformer_block(x)\n","x = layers.GlobalAveragePooling1D()(x)\n","x = layers.Dropout(0.1)(x)\n","x = layers.Dense(20, activation=\"relu\")(x)\n","x = layers.Dropout(0.1)(x)\n","outputs = layers.Dense(2, activation=\"softmax\")(x)\n","\n","model = keras.Model(inputs=inputs, outputs=outputs)"]},{"cell_type":"markdown","metadata":{"id":"RRXa3ks02bUg"},"source":["## Train and Evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QNuVuz1j2bUh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677069817721,"user_tz":-60,"elapsed":145293,"user":{"displayName":"Jean-Christophe Bouquin","userId":"01239462757709491259"}},"outputId":"e6d9d301-25d3-46ed-a2aa-4e853b29cc03"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","782/782 [==============================] - 75s 85ms/step - loss: 0.3893 - accuracy: 0.8091 - val_loss: 0.2849 - val_accuracy: 0.8800\n","Epoch 2/2\n","782/782 [==============================] - 22s 28ms/step - loss: 0.1945 - accuracy: 0.9264 - val_loss: 0.3064 - val_accuracy: 0.8734\n"]}],"source":["model.compile(\n","    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",")\n","history = model.fit(\n","    x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val)\n",")"]},{"cell_type":"code","source":["from tensorflow.keras.datasets import imdb\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Charger les données IMDb\n","(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n","\n","# Charger le dictionnaire de mapping mot-entier\n","word_index = imdb.get_word_index()\n","\n","# Inverser le dictionnaire pour mapper les entiers aux mots correspondants\n","reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n","\n","# Convertir les séquences d'entiers en séquences de mots\n","train_texts = []\n","for sequence in train_data:\n","    text = ' '.join([reverse_word_index.get(i - 3, '?') for i in sequence])\n","    train_texts.append(text)\n","\n","# Définir le tokenizer basé sur les données d'entraînement\n","tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(train_texts)\n","\n","# Définir la longueur maximale de séquence pour le padding\n","maxlen = 200\n","\n","# Définir le modèle\n","# ...\n","\n","# Définir les phrases à tester\n","texts = [\n","    \"my taylor is very happy today\",\n","    \"the movie was really boring and uninteresting\",\n","    \"the acting in the movie was great and the story was captivating\",\n","    \"this is one of the best movies I have ever seen\",\n","    \"I would not recommend this movie to anyone\"\n","]\n","\n","# Convertir les phrases en séquences d'entiers et remplir pour obtenir la même longueur\n","sequences = tokenizer.texts_to_sequences(texts)\n","padded_sequences = pad_sequences(sequences, maxlen=maxlen, padding=\"post\", truncating=\"post\")\n","\n","# Faire une prédiction avec le modèle\n","predictions = model.predict(padded_sequences)\n","\n","# Afficher les classes prédites pour chaque phrase\n","for i in range(len(texts)):\n","    if predictions[i, 0] > predictions[i, 1]:\n","        print(f\"Phrase {i+1} : {texts[i]} => La phrase est négative.\")\n","    else:\n","        print(f\"Phrase {i+1} : {texts[i]} => La phrase est positive.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O6y_oYIAJFxh","executionInfo":{"status":"ok","timestamp":1677070870980,"user_tz":-60,"elapsed":9724,"user":{"displayName":"Jean-Christophe Bouquin","userId":"01239462757709491259"}},"outputId":"8cfad3fb-5d9f-4a7d-8c1f-380e5a4a4479"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 31ms/step\n","Phrase 1 : my taylor is very happy today => La phrase est positive.\n","Phrase 2 : the movie was really boring and uninteresting => La phrase est positive.\n","Phrase 3 : the acting in the movie was great and the story was captivating => La phrase est positive.\n","Phrase 4 : this is one of the best movies I have ever seen => La phrase est positive.\n","Phrase 5 : I would not recommend this movie to anyone => La phrase est positive.\n"]}]},{"cell_type":"code","source":["  "],"metadata":{"id":"NJahikoNLVZv"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/keras-team/keras-io/blob/master/examples/nlp/ipynb/text_classification_with_transformer.ipynb","timestamp":1676460804338}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.11"}},"nbformat":4,"nbformat_minor":0}