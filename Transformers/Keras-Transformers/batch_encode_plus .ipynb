{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNm6hrraKGJNQvxFaB7mvVd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"NQxJT_74CF7G"},"outputs":[],"source":["! pip install transformers"]},{"cell_type":"markdown","source":["batch_encode_plus est une méthode pratique pour encoder une liste de textes pour de nombreuses tâches de traitement du langage naturel. Cependant, il n'est pas toujours nécessaire de l'utiliser.\n","\n","batch_encode_plus est une fonction fournie par la bibliothèque Hugging Face Transformers qui permet de convertir une liste de textes en entrées pour un modèle de langage pré-entraîné, tels que BERT ou RoBERTa. Cette fonction est utile pour traiter plusieurs textes en même temps, de manière efficace.\n","\n","Voici les principaux arguments de la fonction batch_encode_plus :\n","\n","**texts**: une liste de chaînes de caractères représentant les textes d'entrée à encoder.\n","\n","**tokenizer**: un objet Tokenizer qui sera utilisé pour encoder les textes. Ce peut être n'importe quel objet Tokenizer, tel que BertTokenizer ou RobertaTokenizer.\n","\n","**padding**: un booléen indiquant si le résultat doit être rembourré pour avoir la même longueur.\n","\n","**truncation**: un booléen indiquant si les textes doivent être tronqués pour avoir la même longueur maximale.\n","\n","**max_length**: la longueur maximale des entrées, après rembourrage ou troncature. Les entrées plus longues que cette longueur maximale seront tronquées, tandis \n","\n","que les entrées plus courtes seront rembourrées.\n","La fonction batch_encode_plus retourne un dictionnaire contenant les entrées encodées pour chaque texte en entrée. Les clés de ce dictionnaire sont input_ids, attention_mask, et éventuellement token_type_ids pour les modèles qui les utilisent.\n","\n","En résumé, batch_encode_plus est une fonction pratique pour encoder plusieurs textes en entrée à la fois, en utilisant un objet Tokenizer pour les transformer en entrées pour un modèle de langage pré-entraîné."],"metadata":{"id":"VVufcfYqCK4U"}},{"cell_type":"markdown","source":["Exemple d'utilisation de batch_encode_plus avec le tokenizer de BERT pour encoder une liste de textes : "],"metadata":{"id":"tWfmHfYWCt1b"}},{"cell_type":"code","source":["from transformers import BertTokenizer\n","\n","# Création d'un objet Tokenizer de BERT\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Liste de textes à encoder\n","texts = ['Bonjour, comment vas-tu ?', 'Je vais bien, merci pour demander.']\n","\n","# Encodage des textes avec batch_encode_plus\n","encoded_texts = tokenizer.batch_encode_plus(\n","    texts,             # Liste de textes à encoder\n","    padding=True,      # Rembourrage pour avoir la même longueur\n","    truncation=True,   # Troncature pour avoir la même longueur maximale\n","    max_length=32      # Longueur maximale des entrées\n",")\n","\n","# Affichage des entrées encodées pour chaque texte\n","print(encoded_texts)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yNdP_l0pCy8r","executionInfo":{"status":"ok","timestamp":1677574501585,"user_tz":-60,"elapsed":248,"user":{"displayName":"Jean-Christophe Bouquin","userId":"01239462757709491259"}},"outputId":"d4d85994-d1f9-4792-c3f5-5b063e3fe05f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': [[101, 14753, 23099, 2099, 1010, 7615, 12436, 2015, 1011, 10722, 1029, 102, 0], [101, 15333, 12436, 2483, 29316, 1010, 21442, 6895, 10364, 5157, 2121, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"]}]},{"cell_type":"markdown","source":["Noter que Le 0 entre ( 102, 0], [101,) est ajouté pour remplir la fin de la séquence qui a été tronquée. Il indique que la fin de la séquence a été atteinte et que les éléments suivants sont vides. Cela se produit car l'argument padding est défini sur True dans la méthode batch_encode_plus, ce qui ajoute du padding à la fin des séquences pour qu'elles aient toutes la même longueur. Dans ce cas, la longueur maximale est définie à 32, donc la séquence la plus courte est remplie de padding pour atteindre cette longueur maximale. Le 0 est le token d'ID correspondant au padding pour le tokenizer BERT. "],"metadata":{"id":"g8VXB_UoPFmc"}},{"cell_type":"markdown","source":["Dans cet exemple, nous créons un objet Tokenizer de BERT en utilisant from_pretrained. Ensuite, nous définissons une liste de textes à encoder. Nous passons cette liste de textes à batch_encode_plus avec les paramètres padding=True, truncation=True, et max_length=32. Cela permettra de rembourrer ou de tronquer les textes pour avoir une longueur maximale de 32.\n","\n","Le résultat de batch_encode_plus est stocké dans la variable encoded_texts, qui est un dictionnaire contenant les entrées encodées pour chaque texte en entrée. Chaque clé du dictionnaire (input_ids, attention_mask, et éventuellement token_type_ids) correspond à un tableau numpy contenant les entrées encodées pour chaque texte. Les formes de ces tableaux numpy dépendent du nombre de textes en entrée et de la longueur maximale fixée."],"metadata":{"id":"AQS1lYXEDGWF"}},{"cell_type":"markdown","source":["Le nombre de paramètres pour la méthode batch_encode_plus dépendra de la tâche spécifique que vous essayez de résoudre et des exigences de votre modèle de langage pré-entraîné.\n","\n","Par exemple, si vous effectuez une tâche de classification de texte, vous pourriez avoir besoin de renvoyer des tenseurs supplémentaires en plus des identifiants d'entrée encodés, tels que des masques d'attention (return_attention_mask) et des identifiants de type de jeton (return_token_type_ids). Ces informations peuvent aider le modèle à mieux comprendre la structure du texte et à effectuer la classification avec plus de précision.\n","\n","D'un autre côté, si vous effectuez simplement une tâche de traitement de texte, comme la génération de texte ou l'extraction de texte, vous pourriez avoir besoin de moins d'informations supplémentaires. Dans ce cas, vous pouvez simplement encoder les textes avec les arguments padding, truncation, et max_length, comme nous l'avons vu dans l'exemple précédent.\n","\n","En fin de compte, le nombre de paramètres pour la méthode batch_encode_plus dépendra des spécifications de votre tâche et de votre modèle, ainsi que des informations supplémentaires dont vous avez besoin pour effectuer l'encodage."],"metadata":{"id":"ihGbxpOLDU9D"}},{"cell_type":"markdown","source":["**Exemple 1 : Question-réponse**"],"metadata":{"id":"SISzmMWTDZSK"}},{"cell_type":"code","source":["import tensorflow as tf\n","from transformers import AutoTokenizer\n","\n","# Charger le tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n","\n","# Exemple de texte et de question pour la tâche de question-réponse\n","context = \"Le chat est un animal. La mer est bleue. Les roses sont rouges.\"\n","question = \"Quelles sont les couleurs des roses ?\"\n","\n","# Encodage du texte et de la question avec le tokenizer\n","encoded_inputs = tokenizer.batch_encode_plus(\n","    [(context, question)],          # Tuple contenant le contexte et la question\n","    padding=\"max_length\",           # Rembourrage pour avoir la même longueur\n","    truncation=\"only_second\",       # Troncature de la question uniquement\n","    max_length=128,                 # Longueur maximale des entrées\n","    return_tensors=\"tf\"             # Retourne des tenseurs TensorFlow\n",")\n","\n","# Affichage des entrées encodées pour la tâche de question-réponse\n","print(\"Encoded inputs for question-answering:\")\n","print(encoded_inputs)\n"],"metadata":{"id":"3_7jrckwD73A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dans cet exemple, nous avons chargé le tokenizer pour le modèle DistilBERT et encodé un exemple de texte et de question pour la tâche de question-réponse. Les entrées sont encodées avec batch_encode_plus en spécifiant les options pour le rembourrage, la troncature et la longueur maximale des entrées, ainsi que le retour de tenseurs TensorFlow. Nous utilisons également un tuple avec le contexte et la question pour spécifier les textes à encoder.\n","\n","Notez que cet exemple n'utilise pas de modèle pour effectuer la tâche de question-réponse, il s'agit uniquement d'un exemple d'encodage des entrées. Pour effectuer la tâche de question-réponse, il est nécessaire d'utiliser un modèle approprié, tel que TFAutoModelForQuestionAnswering dans le cas de TensorFlow."],"metadata":{"id":"MsW1VR64G9Sb"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","# Chargement du tokenizer pour le modèle BERT\n","model_name = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","# Liste de textes à encoder\n","texts = [\"Le chat est un animal.\", \"La mer est bleue.\", \"Les roses sont rouges.\"]\n","\n","# Encodage des textes avec le tokenizer\n","encoded_texts = tokenizer.batch_encode_plus(\n","    texts,             # Liste de textes à encoder\n","    padding=True,      # Rembourrage pour avoir la même longueur\n","    truncation=True,   # Troncature pour avoir la même longueur maximale\n","    max_length=32      # Longueur maximale des entrées\n",")\n","\n","# Affichage des textes encodés\n","print(\"Encoded texts :\")\n","print(encoded_texts)\n"],"metadata":{"id":"WKT-Hp-vEAKm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exemple 2 : Résumé automatique**"],"metadata":{"id":"OZCmb_i1FlDr"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","\n","# Chargement du modèle de Résumé automatique T5\n","tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n","\n","# Texte à résumer\n","text = \"DistilBERT is a lightweight natural language processing model that can be used for text classification, text extraction, and question answering. It has been pre-trained on the SQuAD dataset and fine-tuned on the MRPC dataset.\"\n","\n","# Encodage du texte avec batch_encode_plus\n","encoded = tokenizer.batch_encode_plus(\n","    [text],              # Texte à encoder\n","    padding=True,        # Rembourrage pour avoir la même longueur\n","    truncation=True,     # Troncature pour avoir la même longueur maximale\n","    max_length=128,      # Longueur maximale des entrées\n","    return_attention_mask=True,# Renvoyer des masques d'attention\n","    return_tensors=\"pt\"  # Renvoyer des tenseurs PyTorch\n",")\n","\n","# Résumé de l'entrée encodée\n","outputs = model.generate(\n","    **encoded,           # Entrée encodée à résumer\n","    max_length=32,       # Longueur maximale du résumé\n","    early_stopping=True, # Arrêter la génération lorsque le modèle a fini de produire des résumés cohérents\n","    num_beams=2,         # Utiliser le décodage avec beam search avec 2 faisceaux\n","    length_penalty=2.0,  # Appliquer une pénalité de longueur pour favoriser les résumés plus courts\n","    no_repeat_ngram_size=3,# Empêcher la génération de trigrammes en double dans le résumé\n","    num_return_sequences=1 # Nombre de résumés à renvoyer\n",")\n","\n","# Conversion du résumé en texte\n","summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","# Affichage du résumé\n","print(\"Résumé :\", summary)\n"],"metadata":{"id":"juDqLUw7FIbd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dans cet exemple, nous utilisons batch_encode_plus pour encoder un texte pour la tâche de Résumé automatique avec un modèle T5 pré-entraîné. Nous passons le texte à batch_encode_plus avec les paramètres padding=True, truncation=True, max_length=128, return_attention_mask=True, et return_tensors=\"pt\". Le résultat de batch_encode_plus est stocké dans la variable encoded, qui est un dictionnaire contenant l'entrée encodée pour le texte en tant que tenseur PyTorch.\n","\n","Ensuite, nous passons cette entrée encodée à notre modèle T5 en utilisant la méthode generate(**encoded) avec **encoded pour passer les arguments d'encodage en tant qu'arguments nommés. Nous spécifions les paramètres de génération de résumé tels que max_length=32, early_stopping=True, num_beams=2, length_penalty=2.0, no_repeat_ngram_size=3, et num_return_sequences=1. Le résultat de generate est stocké dans la variable outputs, qui contient le résumé généré sous forme d'encodage de token."],"metadata":{"id":"Kde51oTlHTg8"}},{"cell_type":"markdown","source":["**Exemple 3 : Traduction**"],"metadata":{"id":"xxMp3KCaHwzG"}},{"cell_type":"code","source":["\n","!pip install sentencepiece"],"metadata":{"id":"szjHkjHSIoeN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install sacremoses "],"metadata":{"id":"2Uf6UJQCKdvF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from transformers import AutoTokenizer\n","\n","# Charger le tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"facebook/wmt19-en-de\")\n","\n","# Exemple de texte à traduire\n","text = \"The cat is sleeping on the sofa.\"\n","\n","# Encodage du texte avec le tokenizer pour la traduction en français\n","encoded_inputs = tokenizer.batch_encode_plus(\n","    [text],                        # Liste contenant le texte à traduire\n","    padding=\"max_length\",          # Rembourrage pour avoir la même longueur\n","    truncation=True,               # Troncature pour avoir la même longueur maximale\n","    max_length=128,                # Longueur maximale des entrées\n","    return_tensors=\"tf\",           # Retourne des tenseurs TensorFlow\n","    src_lang=\"en\",                 # Langue source (anglais)\n","    tgt_lang=\"fr\"                  # Langue cible (français)\n",")\n","\n","# Affichage du texte encodé pour la traduction\n","print(\"Encoded input for translation:\")\n","print(encoded_inputs)\n"],"metadata":{"id":"UNzWyCSTFPNj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM\n","\n","# Charger le modèle et le tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"facebook/wmt19-en-de\")\n","model = TFAutoModelForSeq2SeqLM.from_pretrained(\"opus-mt-en-de\")\n","\n","# Encoder les phrases en français\n","inputs = tokenizer.batch_encode_plus(\n","    [\"Bonjour, comment allez-vous?\", \"Comment vous appelez-vous?\", \"Où habitez-vous?\"],\n","    padding=\"max_length\",\n","    truncation=True,\n","    max_length=128,\n","    return_attention_mask=True,\n","    return_tensors=\"tf\"\n",")\n","\n","# Faire la traduction\n","outputs = model.generate(\n","    input_ids=inputs[\"input_ids\"],\n","    attention_mask=inputs[\"attention_mask\"],\n","    max_length=128,\n","    num_beams=4,\n","    early_stopping=True\n",")\n","\n","# Décodez les traductions en anglais\n","translations = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n","\n","# Afficher les traductions\n","print(translations)\n"],"metadata":{"id":"NAl01CbrK5TW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dans cet exemple, nous utilisons batch_encode_plus pour encoder une seule phrase en anglais pour la traduction en français avec un modèle T5 pré-entraîné. Nous passons la phrase d'entrée à batch_encode_plus avec les paramètres padding=True, truncation=True, max_length=128, return_attention_mask=True, et return_tensors=\"pt\". Le résultat de batch_encode_plus est stocké dans la variable encoded, qui est un dictionnaire contenant l'entrée encodée pour la phrase d'entrée en tant que tenseur PyTorch.\n","\n","Ensuite, nous passons cette entrée encodée à notre modèle T5 en utilisant la méthode generate() avec différents paramètres pour obtenir la traduction. Enfin, nous utilisons la méthode decode(outputs[0], skip_special_tokens=True) pour convertir le tenseur de la traduction en français en une chaîne de caractères Python, que nous affichons."],"metadata":{"id":"uR1dQIx8H9v2"}},{"cell_type":"markdown","source":["**Exemple 4 : NER**"],"metadata":{"id":"7YHyR5P5OBIF"}},{"cell_type":"code","source":["import tensorflow as tf\n","from transformers import AutoTokenizer,TFAutoModelForTokenClassification\n","\n","# Charger le tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n","\n","# Encoder les phrases en entrée\n","encoded_texts = tokenizer.batch_encode_plus(\n","    [\n","        \"European authorities fined Google a record $5.1 billion on Wednesday.\",\n","        \"Steve Jobs was the CEO of Apple Corp.\",\n","        \"Elon Musk is the founder of Tesla.\",\n","    ],\n","    padding=True,\n","    truncation=True,\n","    max_length=128,\n","    return_attention_mask=True,\n","    return_token_type_ids=False,\n","    return_offsets_mapping=True,\n","    return_special_tokens_mask=False,\n","    return_overflowing_tokens=False,\n","    return_length=False,\n",")\n","\n","# Convertir les encodages en tenseurs TensorFlow\n","input_ids = tf.constant(encoded_texts[\"input_ids\"])\n","attention_mask = tf.constant(encoded_texts[\"attention_mask\"])\n","offsets = tf.constant(encoded_texts[\"offset_mapping\"])\n","\n","# Charger le modèle NER\n","model = TFAutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n","\n","# Faire la prédiction de NER\n","outputs = model(input_ids, attention_mask=attention_mask)\n","\n","# Extraire les prédictions pour les entités nommées\n","predictions = tf.argmax(outputs.logits, axis=-1)\n","\n","# Décoder les prédictions en entités nommées\n","decoded_predictions = []\n","for i, offsets_for_text in enumerate(offsets):\n","    decoded = []\n","    for j, offset in enumerate(offsets_for_text):\n","        if offset[0] == 0 and j != 0:\n","            decoded.append(\"|\")\n","        decoded.append(tokenizer.convert_ids_to_tokens([predictions[i][j].numpy()])[0])\n","    decoded_predictions.append(\"\".join(decoded))\n","\n","# Afficher les prédictions en entités nommées\n","print(decoded_predictions)\n"],"metadata":{"id":"k3GOgUQdH_4a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dans cet exemple, nous utilisons batch_encode_plus pour encoder une liste de phrases d'entrée pour l'extraction d'entités nommées avec un modèle de classification d'entités nommées NER de BERT pré-entraîné sur le corpus CoNLL-2003 en anglais. Nous passons la liste de phrases d'entrée à batch_encode_plus avec les paramètres padding=True, truncation=True, max_length=128, return_attention_mask=True, return_token_type_ids=False, return_offsets_mapping=True, et return_tensors=\"pt\".\n","\n","Le résultat de batch_encode_plus est stocké dans la variable encoded_texts, qui est un dictionnaire contenant les entrées encodées pour chaque phrase d'entrée en tant que tenseurs PyTorch. Ensuite, nous passons ces entrées encodées à notre modèle NER de BERT en utilisant la méthode __call__ (model(**encoded_texts)), ce qui renvoie un tenseur contenant les prédictions de notre modèle."],"metadata":{"id":"Dj-8W-ZkOcVk"}},{"cell_type":"code","source":[],"metadata":{"id":"ZIXouP_ZNffC"},"execution_count":null,"outputs":[]}]}