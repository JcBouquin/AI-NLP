{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HuggingFace-Multilingual.ipynb","provenance":[{"file_id":"https://github.com/huggingface/notebooks/blob/master/transformers_doc/tensorflow/multilingual.ipynb","timestamp":1611044093049}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"3s-Qz1Ydte2D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611579926902,"user_tz":-60,"elapsed":8430,"user":{"displayName":"Jean-Christophe Bouquin","photoUrl":"","userId":"01239462757709491259"}},"outputId":"b3dd831e-7008-4c81-f978-1b9a521733f1"},"source":["# Transformers installation\n","! pip install transformers\n","# To install from source instead of the last release, comment the command above and uncomment the following one.\n","# ! pip install git+https://github.com/huggingface/transformers.git\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/b1/41130a228dd656a1a31ba281598a968320283f48d42782845f6ba567f00b/transformers-4.2.2-py3-none-any.whl (1.8MB)\n","\u001b[K     |████████████████████████████████| 1.8MB 7.4MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Collecting tokenizers==0.9.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 38.3MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.3.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 26.4MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=409082f824e9d8ec758cc0fc9f31017d37f2a559f8ee2326613e2a523f57f7b1\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.2.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qxNHZmY2pasM"},"source":["from IPython.display import display, Javascript\n","from google.colab.output import eval_js\n","from base64 import b64decode\n","\n","def take_photo(filename='photo.jpg', quality=0.8):\n","  js = Javascript('''\n","    async function takePhoto(quality) {\n","      const div = document.createElement('div');\n","      const capture = document.createElement('button');\n","      capture.textContent = 'Capture';\n","      div.appendChild(capture);\n","\n","      const video = document.createElement('video');\n","      video.style.display = 'block';\n","      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n","\n","      document.body.appendChild(div);\n","      div.appendChild(video);\n","      video.srcObject = stream;\n","      await video.play();\n","\n","      // Resize the output to fit the video element.\n","      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n","\n","      // Wait for Capture to be clicked.\n","      await new Promise((resolve) => capture.onclick = resolve);\n","\n","      const canvas = document.createElement('canvas');\n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      canvas.getContext('2d').drawImage(video, 0, 0);\n","      stream.getVideoTracks()[0].stop();\n","      div.remove();\n","      return canvas.toDataURL('image/jpeg', quality);\n","    }\n","    ''')\n","  display(js)\n","  data = eval_js('takePhoto({})'.format(quality))\n","  binary = b64decode(data.split(',')[1])\n","  with open(filename, 'wb') as f:\n","    f.write(binary)\n","  return filename"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kD-7JSEApasT"},"source":["from IPython.display import Image\n","try:\n","  filename = take_photo()\n","  print('Saved to {}'.format(filename))\n","  \n","  # Show the image which was just taken.\n","  display(Image(filename))\n","except Exception as err:\n","  # Errors will be thrown if the user does not have a webcam or if they do not\n","  # grant the page permission to access it.\n","  print(str(err))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"72d31KDRte2I"},"source":["# Multi-lingual models"]},{"cell_type":"markdown","metadata":{"id":"67ODcXdOte2K"},"source":["Most of the models available in this library are mono-lingual models (English, Chinese and German). A few multi-lingual\n","models are available and have a different mechanisms than mono-lingual models. This page details the usage of these\n","models.\n","\n","The two models that currently support multiple languages are BERT and XLM."]},{"cell_type":"markdown","metadata":{"id":"llcIOqwgte2K"},"source":["## XLM"]},{"cell_type":"markdown","metadata":{"id":"u23NXypote2L"},"source":["XLM has a total of 10 different checkpoints, only one of which is mono-lingual. The 9 remaining model checkpoints can\n","be split in two categories: the checkpoints that make use of language embeddings, and those that don't"]},{"cell_type":"markdown","metadata":{"id":"HtdUJwBCte2L"},"source":["### XLM & Language Embeddings"]},{"cell_type":"markdown","metadata":{"id":"dijUdCEjte2L"},"source":["This section concerns the following checkpoints:\n","\n","- `xlm-mlm-ende-1024` (Masked language modeling, English-German)\n","- `xlm-mlm-enfr-1024` (Masked language modeling, English-French)\n","- `xlm-mlm-enro-1024` (Masked language modeling, English-Romanian)\n","- `xlm-mlm-xnli15-1024` (Masked language modeling, XNLI languages)\n","- `xlm-mlm-tlm-xnli15-1024` (Masked language modeling + Translation, XNLI languages)\n","- `xlm-clm-enfr-1024` (Causal language modeling, English-French)\n","- `xlm-clm-ende-1024` (Causal language modeling, English-German)\n","\n","These checkpoints require language embeddings that will specify the language used at inference time. These language\n","embeddings are represented as a tensor that is of the same shape as the input ids passed to the model. The values in\n","these tensors depend on the language used and are identifiable using the `lang2id` and `id2lang` attributes from\n","the tokenizer.\n","\n","Here is an example using the `xlm-clm-enfr-1024` checkpoint (Causal language modeling, English-French):"]},{"cell_type":"code","metadata":{"id":"82-4rlhpte2M","executionInfo":{"status":"ok","timestamp":1611579990159,"user_tz":-60,"elapsed":13240,"user":{"displayName":"Jean-Christophe Bouquin","photoUrl":"","userId":"01239462757709491259"}},"outputId":"38996586-87be-4cff-a88b-ae91dbf950de","colab":{"base_uri":"https://localhost:8080/","height":187,"referenced_widgets":["e62f392998dd4d41a9a3012f7e33d605","fc4a0d8f03c64841a467a98f9f6c1d7b","cd6fc9641a10465f99efb6b629519793","45e915be69894f61bf87931885e07d07"]}},"source":["import torch\n","from transformers import XLMTokenizer, XLMWithLMHeadModel\n","tokenizer = XLMTokenizer.from_pretrained(\"xlm-clm-enfr-1024\")\n","model = XLMWithLMHeadModel.from_pretrained(\"xlm-clm-enfr-1024\")"],"execution_count":4,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e62f392998dd4d41a9a3012f7e33d605","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1452741.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fc4a0d8f03c64841a467a98f9f6c1d7b","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1008321.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cd6fc9641a10465f99efb6b629519793","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1020.0, style=ProgressStyle(description…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"45e915be69894f61bf87931885e07d07","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=830122454.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of XLMWithLMHeadModel were not initialized from the model checkpoint at xlm-clm-enfr-1024 and are newly initialized: ['transformer.position_ids']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"LKrVY2l5te2M"},"source":["The different languages this model/tokenizer handles, as well as the ids of these languages are visible using the\n","`lang2id` attribute:"]},{"cell_type":"code","metadata":{"id":"kvb6e6dSte2M","executionInfo":{"status":"ok","timestamp":1611579996555,"user_tz":-60,"elapsed":539,"user":{"displayName":"Jean-Christophe Bouquin","photoUrl":"","userId":"01239462757709491259"}},"outputId":"0b9653d1-2ec9-4f40-e4f9-844321203b86","colab":{"base_uri":"https://localhost:8080/"}},"source":["print(tokenizer.lang2id)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["{'en': 0, 'fr': 1}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7qjF5Cl6te2N"},"source":["These ids should be used when passing a language parameter during a model pass. Let's define our inputs:"]},{"cell_type":"code","metadata":{"id":"YQJ3w1rgte2N","executionInfo":{"status":"ok","timestamp":1611579998361,"user_tz":-60,"elapsed":847,"user":{"displayName":"Jean-Christophe Bouquin","photoUrl":"","userId":"01239462757709491259"}}},"source":["input_ids = torch.tensor([tokenizer.encode(\"Wikipedia was used to\")]) # batch size of 1"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b2RpWuAVte2N"},"source":["We should now define the language embedding by using the previously defined language id. We want to create a tensor\n","filled with the appropriate language ids, of the same size as input_ids. For english, the id is 0:"]},{"cell_type":"code","metadata":{"id":"JcEhxqZCte2O","executionInfo":{"status":"ok","timestamp":1611579999334,"user_tz":-60,"elapsed":548,"user":{"displayName":"Jean-Christophe Bouquin","photoUrl":"","userId":"01239462757709491259"}}},"source":["language_id = tokenizer.lang2id['en']  # 0\n","langs = torch.tensor([language_id] * input_ids.shape[1])  # torch.tensor([0, 0, 0, ..., 0])\n","# We reshape it to be of size (batch_size, sequence_length)\n","langs = langs.view(1, -1) # is now of shape [1, sequence_length] (we have a batch size of 1)"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V5pUVC6Tte2O"},"source":["You can then feed it all as input to your model:"]},{"cell_type":"code","metadata":{"id":"7TvhR7h4te2O","executionInfo":{"status":"ok","timestamp":1611580001014,"user_tz":-60,"elapsed":585,"user":{"displayName":"Jean-Christophe Bouquin","photoUrl":"","userId":"01239462757709491259"}}},"source":["outputs = model(input_ids, langs=langs)"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gtR-I3P-te2O"},"source":["The example [run_generation.py](https://github.com/huggingface/transformers/blob/master/examples/text-generation/run_generation.py) can generate\n","text using the CLM checkpoints from XLM, using the language embeddings."]},{"cell_type":"markdown","metadata":{"id":"qh_DVUbXte2O"},"source":["### XLM without Language Embeddings"]},{"cell_type":"markdown","metadata":{"id":"lR8k4TZHte2P"},"source":["This section concerns the following checkpoints:\n","\n","- `xlm-mlm-17-1280` (Masked language modeling, 17 languages)\n","- `xlm-mlm-100-1280` (Masked language modeling, 100 languages)\n","\n","These checkpoints do not require language embeddings at inference time. These models are used to have generic sentence\n","representations, differently from previously-mentioned XLM checkpoints."]},{"cell_type":"markdown","metadata":{"id":"skC504Ikte2P"},"source":["## BERT"]},{"cell_type":"markdown","metadata":{"id":"vEjrW0b4te2P"},"source":["BERT has two checkpoints that can be used for multi-lingual tasks:\n","\n","- `bert-base-multilingual-uncased` (Masked language modeling + Next sentence prediction, 102 languages)\n","- `bert-base-multilingual-cased` (Masked language modeling + Next sentence prediction, 104 languages)\n","\n","These checkpoints do not require language embeddings at inference time. They should identify the language used in the\n","context and infer accordingly."]},{"cell_type":"markdown","metadata":{"id":"tv-hMTDMte2P"},"source":["## XLM-RoBERTa"]},{"cell_type":"markdown","metadata":{"id":"09zc6lIgte2P"},"source":["XLM-RoBERTa was trained on 2.5TB of newly created clean CommonCrawl data in 100 languages. It provides strong gains\n","over previously released multi-lingual models like mBERT or XLM on downstream tasks like classification, sequence\n","labeling and question answering.\n","\n","Two XLM-RoBERTa checkpoints can be used for multi-lingual tasks:\n","\n","- `xlm-roberta-base` (Masked language modeling, 100 languages)\n","- `xlm-roberta-large` (Masked language modeling, 100 languages)"]},{"cell_type":"code","metadata":{"id":"ptiizK5Fp1TN","executionInfo":{"status":"ok","timestamp":1611580006041,"user_tz":-60,"elapsed":588,"user":{"displayName":"Jean-Christophe Bouquin","photoUrl":"","userId":"01239462757709491259"}}},"source":[""],"execution_count":12,"outputs":[]}]}