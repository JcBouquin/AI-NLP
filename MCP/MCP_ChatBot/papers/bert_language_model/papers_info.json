{
  "2402.14408v1": {
    "title": "Transferring BERT Capabilities from High-Resource to Low-Resource Languages Using Vocabulary Matching",
    "authors": [
      "Piotr Rybak"
    ],
    "summary": "Pre-trained language models have revolutionized the natural language\nunderstanding landscape, most notably BERT (Bidirectional Encoder\nRepresentations from Transformers). However, a significant challenge remains\nfor low-resource languages, where limited data hinders the effective training\nof such models. This work presents a novel approach to bridge this gap by\ntransferring BERT capabilities from high-resource to low-resource languages\nusing vocabulary matching. We conduct experiments on the Silesian and Kashubian\nlanguages and demonstrate the effectiveness of our approach to improve the\nperformance of BERT models even when the target language has minimal training\ndata. Our results highlight the potential of the proposed technique to\neffectively train BERT models for low-resource languages, thus democratizing\naccess to advanced language understanding models.",
    "pdf_url": "http://arxiv.org/pdf/2402.14408v1",
    "published": "2024-02-22"
  },
  "2111.02188v2": {
    "title": "BERT-DRE: BERT with Deep Recursive Encoder for Natural Language Sentence Matching",
    "authors": [
      "Ehsan Tavan",
      "Ali Rahmati",
      "Maryam Najafi",
      "Saeed Bibak",
      "Zahed Rahmati"
    ],
    "summary": "This paper presents a deep neural architecture, for Natural Language Sentence\nMatching (NLSM) by adding a deep recursive encoder to BERT so called BERT with\nDeep Recursive Encoder (BERT-DRE). Our analysis of model behavior shows that\nBERT still does not capture the full complexity of text, so a deep recursive\nencoder is applied on top of BERT. Three Bi-LSTM layers with residual\nconnection are used to design a recursive encoder and an attention module is\nused on top of this encoder. To obtain the final vector, a pooling layer\nconsisting of average and maximum pooling is used. We experiment our model on\nfour benchmarks, SNLI, FarsTail, MultiNLI, SciTail, and a novel Persian\nreligious questions dataset. This paper focuses on improving the BERT results\nin the NLSM task. In this regard, comparisons between BERT-DRE and BERT are\nconducted, and it is shown that in all cases, BERT-DRE outperforms BERT. The\nBERT algorithm on the religious dataset achieved an accuracy of 89.70%, and\nBERT-DRE architectures improved to 90.29% using the same dataset.",
    "pdf_url": "http://arxiv.org/pdf/2111.02188v2",
    "published": "2021-11-03"
  },
  "2211.17201v1": {
    "title": "ExtremeBERT: A Toolkit for Accelerating Pretraining of Customized BERT",
    "authors": [
      "Rui Pan",
      "Shizhe Diao",
      "Jianlin Chen",
      "Tong Zhang"
    ],
    "summary": "In this paper, we present ExtremeBERT, a toolkit for accelerating and\ncustomizing BERT pretraining. Our goal is to provide an easy-to-use BERT\npretraining toolkit for the research community and industry. Thus, the\npretraining of popular language models on customized datasets is affordable\nwith limited resources. Experiments show that, to achieve the same or better\nGLUE scores, the time cost of our toolkit is over $6\\times$ times less for BERT\nBase and $9\\times$ times less for BERT Large when compared with the original\nBERT paper. The documentation and code are released at\nhttps://github.com/extreme-bert/extreme-bert under the Apache-2.0 license.",
    "pdf_url": "http://arxiv.org/pdf/2211.17201v1",
    "published": "2022-11-30"
  },
  "2011.04266v1": {
    "title": "BERT-JAM: Boosting BERT-Enhanced Neural Machine Translation with Joint Attention",
    "authors": [
      "Zhebin Zhang",
      "Sai Wu",
      "Dawei Jiang",
      "Gang Chen"
    ],
    "summary": "BERT-enhanced neural machine translation (NMT) aims at leveraging\nBERT-encoded representations for translation tasks. A recently proposed\napproach uses attention mechanisms to fuse Transformer's encoder and decoder\nlayers with BERT's last-layer representation and shows enhanced performance.\nHowever, their method doesn't allow for the flexible distribution of attention\nbetween the BERT representation and the encoder/decoder representation. In this\nwork, we propose a novel BERT-enhanced NMT model called BERT-JAM which improves\nupon existing models from two aspects: 1) BERT-JAM uses joint-attention modules\nto allow the encoder/decoder layers to dynamically allocate attention between\ndifferent representations, and 2) BERT-JAM allows the encoder/decoder layers to\nmake use of BERT's intermediate representations by composing them using a gated\nlinear unit (GLU). We train BERT-JAM with a novel three-phase optimization\nstrategy that progressively unfreezes different components of BERT-JAM. Our\nexperiments show that BERT-JAM achieves SOTA BLEU scores on multiple\ntranslation tasks.",
    "pdf_url": "http://arxiv.org/pdf/2011.04266v1",
    "published": "2020-11-09"
  },
  "1902.04094v2": {
    "title": "BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model",
    "authors": [
      "Alex Wang",
      "Kyunghyun Cho"
    ],
    "summary": "We show that BERT (Devlin et al., 2018) is a Markov random field language\nmodel. This formulation gives way to a natural procedure to sample sentences\nfrom BERT. We generate from BERT and find that it can produce high-quality,\nfluent generations. Compared to the generations of a traditional left-to-right\nlanguage model, BERT generates sentences that are more diverse but of slightly\nworse quality.",
    "pdf_url": "http://arxiv.org/pdf/1902.04094v2",
    "published": "2019-02-11"
  }
}