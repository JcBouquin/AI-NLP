# Serveur MCP LangGraph RAG Pharmaceutique

## ğŸ“‹ Description

Ce serveur MCP (Model Context Protocol) fournit une analyse intelligente de documents pharmaceutiques en utilisant **LangGraph** pour orchestrer un workflow multi-agents et **RAG (Retrieval Augmented Generation)** pour la recherche sÃ©mantique dans une base documentaire.

Le systÃ¨me utilise un graphe de traitement avec trois nodes spÃ©cialisÃ©s qui collaborent pour rÃ©pondre aux questions :
- **Researcher** : Recherche les informations pertinentes via RAG (recherche sÃ©mantique vectorielle)
- **Analyst** : Analyse et synthÃ©tise les informations trouvÃ©es
- **Expert** : Fournit une rÃ©ponse experte finale

## ğŸ—ï¸ Architecture

### Composants principaux

1. **Vectorstore (SKLearn)** : Stocke les embeddings des documents pour la recherche sÃ©mantique rapide
   - Format : Parquet (`sklearn_vectorstore.parquet`)
   - Embeddings : OpenAI `text-embedding-3-small`
   - Chunks : 1000 tokens avec overlap de 200

2. **LangGraph Workflow** : Orchestration sÃ©quentielle des agents
   ```
   Question â†’ Researcher â†’ Analyst â†’ Expert â†’ RÃ©ponse finale
   ```

3. **Documents source** : Fichiers texte dans `pharmacy_docs/`
   - complements_alimentaires.txt
   - cosmetiques.txt
   - dermatologie.txt
   - fidelisation_client.txt
   - medicaments_otc.txt

## ğŸ› ï¸ Outils disponibles

### 1. `analyze_pharmacy_question`
Analyse une question pharmaceutique en utilisant le workflow LangGraph complet avec RAG.

**ParamÃ¨tres :**
- `question` (string, requis) : La question Ã  analyser

**Exemple d'utilisation :**
```
Question : "Quels sont les meilleurs complÃ©ments alimentaires pour la rÃ©cupÃ©ration musculaire ?"
```

**Processus :**
1. Le **Researcher** rÃ©cupÃ¨re les 5 chunks les plus pertinents via recherche sÃ©mantique
2. L'**Analyst** synthÃ©tise les informations trouvÃ©es
3. L'**Expert** formule une rÃ©ponse professionnelle complÃ¨te

---

### 2. `list_pharmacy_documents`
Liste tous les documents pharmaceutiques disponibles dans le systÃ¨me.

**ParamÃ¨tres :** Aucun

**Retourne :** Liste des noms de fichiers disponibles

---

### 3. `get_document_content`
RÃ©cupÃ¨re le contenu complet d'un document spÃ©cifique.

**ParamÃ¨tres :**
- `filename` (string, requis) : Nom du fichier (ex: "medicaments_otc.txt")

**Exemple :**
```
filename: "dermatologie.txt"
```

---

### 4. `reload_documents`
Recharge tous les documents depuis le rÃ©pertoire et reconstruit le vectorstore.

**ParamÃ¨tres :** Aucun

**UtilitÃ© :** Si vous avez ajoutÃ© ou modifiÃ© des documents dans `pharmacy_docs/`

## ğŸš€ Installation et Configuration

### PrÃ©requis
- Python 3.10+
- ClÃ© API OpenAI

### DÃ©pendances
```bash
pip install mcp langchain langchain-openai langchain-community langgraph scikit-learn tiktoken
```

### Configuration Claude Desktop

Ajoutez cette section dans `%APPDATA%\Claude\claude_desktop_config.json` :

```json
{
  "mcpServers": {
    "langgraph-RAG_pharma-mcp": {
      "command": "python",
      "args": [
        "C:/Users/kosmo/pycode/mcp/Langgraph_mcp_RAGV1/server.py"
      ],
      "env": {
        "OPENAI_API_KEY": "votre-clÃ©-api-openai"
      }
    }
  }
}
```

### Fichier .env (optionnel)

CrÃ©ez un fichier `.env` dans le dossier du projet :
```
OPENAI_API_KEY=votre-clÃ©-api-openai
```

## ğŸ“Š Fonctionnement du RAG

### 1. Initialisation (premiÃ¨re utilisation)
Au premier lancement, le systÃ¨me :
1. Charge tous les documents `.txt` du dossier `pharmacy_docs/`
2. DÃ©coupe les documents en chunks de 1000 tokens
3. CrÃ©e les embeddings avec OpenAI
4. Sauvegarde le vectorstore dans `sklearn_vectorstore.parquet`

**Note :** Cette Ã©tape prend quelques secondes et utilise l'API OpenAI pour crÃ©er les embeddings.

### 2. Utilisations suivantes
Le vectorstore est chargÃ© directement depuis le fichier `.parquet`, permettant des recherches ultra-rapides sans recrÃ©er les embeddings.

### 3. Recherche sÃ©mantique
Lors d'une question :
- Le systÃ¨me recherche les 5 chunks les plus similaires sÃ©mantiquement
- Utilise la similaritÃ© cosinus sur les embeddings vectoriels
- Retourne les passages les plus pertinents avec leurs sources

## ğŸ’¡ Cas d'usage

### Exemple 1 : Conseil produit
```
Question : "Quel produit recommander pour les peaux sensibles avec de l'acnÃ© ?"

â†’ Le Researcher trouve les passages pertinents dans dermatologie.txt
â†’ L'Analyst synthÃ©tise les options disponibles
â†’ L'Expert formule une recommandation professionnelle
```

### Exemple 2 : StratÃ©gie de vente
```
Question : "Comment amÃ©liorer la fidÃ©lisation des clients en pharmacie ?"

â†’ Recherche dans fidelisation_client.txt
â†’ SynthÃ¨se des meilleures pratiques
â†’ Recommandations actionnables
```

### Exemple 3 : Information mÃ©dicament
```
Question : "Quelles sont les interactions possibles entre l'ibuprofÃ¨ne et les anticoagulants ?"

â†’ Recherche dans medicaments_otc.txt
â†’ Analyse des contre-indications
â†’ Conseil pharmaceutique expert
```

## ğŸ”§ Personnalisation

### Ajouter de nouveaux documents
1. Placez vos fichiers `.txt` dans le dossier `pharmacy_docs/`
2. Utilisez l'outil `reload_documents` pour reconstruire le vectorstore

### Modifier les paramÃ¨tres du retriever
Dans `pharmacy_graph_RAG.py` ligne 76-79 :
```python
self.retriever = self.vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 5},  # Nombre de chunks Ã  rÃ©cupÃ©rer
)
```

### Modifier la taille des chunks
Dans `pharmacy_graph_RAG.py` ligne 145-148 :
```python
text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=1000,    # Taille des chunks
    chunk_overlap=200,  # Overlap entre chunks
)
```

### Changer le modÃ¨le LLM
Dans `pharmacy_graph_RAG.py` ligne 66 :
```python
self.llm = ChatOpenAI(temperature=0.2, model="gpt-4o-mini")
```

ModÃ¨les disponibles : `gpt-4o`, `gpt-4o-mini`, `gpt-4-turbo`, etc.

## ğŸ“ Structure du projet

```
Langgraph_mcp_RAGV1/
â”œâ”€â”€ server.py                        # Serveur MCP principal
â”œâ”€â”€ pharmacy_graph_RAG.py            # Classe PharmacyGraph avec workflow LangGraph
â”œâ”€â”€ pharmacy_docs/                   # Documents source
â”‚   â”œâ”€â”€ complements_alimentaires.txt
â”‚   â”œâ”€â”€ cosmetiques.txt
â”‚   â”œâ”€â”€ dermatologie.txt
â”‚   â”œâ”€â”€ fidelisation_client.txt
â”‚   â””â”€â”€ medicaments_otc.txt
â”œâ”€â”€ sklearn_vectorstore.parquet      # Vectorstore (crÃ©Ã© automatiquement)
â”œâ”€â”€ .env                             # Configuration API (optionnel)
â”œâ”€â”€ pyproject.toml                   # MÃ©tadonnÃ©es du projet
â””â”€â”€ claude.md                        # Cette documentation
```

## ğŸ› DÃ©pannage

### Le vectorstore n'est pas crÃ©Ã©
- VÃ©rifiez que le dossier `pharmacy_docs/` contient des fichiers `.txt`
- VÃ©rifiez que votre clÃ© API OpenAI est valide
- Consultez les logs dans Claude Desktop

### Erreur "No module named..."
```bash
pip install --upgrade mcp langchain langchain-openai langchain-community langgraph scikit-learn tiktoken
```

### Le serveur ne dÃ©marre pas
- VÃ©rifiez le chemin Python dans la configuration Claude Desktop
- Assurez-vous que le fichier `server.py` est au bon emplacement
- VÃ©rifiez les logs d'erreur dans Claude Desktop

### Recherches peu pertinentes
- Augmentez le paramÃ¨tre `k` du retriever (plus de chunks rÃ©cupÃ©rÃ©s)
- RÃ©duisez la `chunk_size` pour des chunks plus prÃ©cis
- AmÃ©liorez la qualitÃ© des documents source

## ğŸ“ˆ Performance

### Temps de traitement
- **PremiÃ¨re initialisation** : 10-30 secondes (crÃ©ation des embeddings)
- **Chargement du vectorstore** : < 1 seconde
- **Recherche sÃ©mantique** : < 200ms
- **Analyse complÃ¨te (question â†’ rÃ©ponse)** : 5-15 secondes

### CoÃ»ts API OpenAI
- **Embeddings** (premiÃ¨re fois) : ~0.001$ par document
- **RequÃªtes LLM** : ~0.01-0.05$ par question (selon le modÃ¨le)

## ğŸ” SÃ©curitÃ©

- Ne commitez jamais votre fichier `.env` avec votre clÃ© API
- Utilisez des variables d'environnement pour la clÃ© API en production
- Le vectorstore ne contient que des embeddings (pas de donnÃ©es sensibles lisibles)

## ğŸ“š Ressources

- [Documentation LangGraph](https://langchain-ai.github.io/langgraph/)
- [Documentation LangChain](https://python.langchain.com/)
- [Model Context Protocol](https://modelcontextprotocol.io/)
- [OpenAI Embeddings](https://platform.openai.com/docs/guides/embeddings)

## ğŸ“ Notes techniques

### Pourquoi SKLearnVectorStore ?
- LÃ©ger et rapide pour des datasets < 10k documents
- SÃ©rialisation Parquet pour un stockage efficace
- Pas besoin de serveur externe (vs Chroma, Pinecone)

### Workflow sÃ©quentiel vs parallÃ¨le
Le graphe LangGraph utilise un flux sÃ©quentiel pour garantir que :
1. Le Researcher trouve d'abord les informations
2. L'Analyst les synthÃ©tise avec le contexte complet
3. L'Expert formule une rÃ©ponse cohÃ©rente basÃ©e sur l'analyse

### Gestion des chemins
Le systÃ¨me utilise des **chemins absolus** pour Ã©viter les problÃ¨mes de working directory :
```python
script_dir = os.path.dirname(os.path.abspath(__file__))
```

Cela garantit que le vectorstore est toujours crÃ©Ã© au bon endroit, peu importe d'oÃ¹ le serveur est lancÃ©.

---

**Version** : 1.0.0
**Auteur** : Serveur MCP LangGraph RAG Pharmaceutique
**Licence** : MIT
