"""
Module contenant la classe PharmacyGraph pour l'analyse de documents pharmaceutiques avec LangGraph et RAG
"""

import os
import re
import tiktoken
from typing import TypedDict, Annotated

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import SKLearnVectorStore
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_core.documents import Document

from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages


class GraphState(TypedDict):
    """√âtat partag√© entre les nodes du graphe"""

    question: str
    research_output: str
    analysis_output: str
    final_answer: str
    messages: Annotated[list, add_messages]


class PharmacyGraph:
    """Workflow LangGraph pour la recherche pharmaceutique avec RAG"""

    def __init__(
        self,
        docs_directory=None,
        vectorstore_path=None,
    ):
        """
        Initialise le workflow LangGraph avec RAG

        Args:
            docs_directory: Chemin vers le r√©pertoire contenant les documents
            vectorstore_path: Chemin vers le fichier vectorstore
        """
        # Obtenir le r√©pertoire du script actuel
        script_dir = os.path.dirname(os.path.abspath(__file__))

        # D√©finir les chemins par d√©faut relatifs au script
        if docs_directory is None:
            self.docs_directory = os.path.join(script_dir, "pharmacy_docs")
        else:
            self.docs_directory = docs_directory

        if vectorstore_path is None:
            self.vectorstore_path = os.path.join(script_dir, "sklearn_vectorstore.parquet")
        else:
            self.vectorstore_path = vectorstore_path

        print(f"üìÅ R√©pertoire des documents: {self.docs_directory}")
        print(f"üíæ Chemin du vectorstore: {self.vectorstore_path}")

        # Initialiser le dictionnaire des documents
        self.documents = {}

        # Initialiser le LLM
        self.llm = ChatOpenAI(temperature=0.2, model="gpt-4o-mini")

        # Initialiser ou charger le vectorstore
        self.vectorstore = self._initialize_vectorstore()

        # Charger les documents dans self.documents si pas d√©j√† fait
        if not self.documents:
            self._load_documents_list()

        # Cr√©er le retriever
        self.retriever = self.vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 5},  # R√©cup√®re les 5 chunks les plus pertinents
        )

        # Construire le graphe
        self.graph = self._build_graph()

    def _load_documents_list(self):
        """
        Charge la liste des documents dans self.documents sans recr√©er le vectorstore
        """
        if not os.path.exists(self.docs_directory):
            print(f"‚ö†Ô∏è Le r√©pertoire {self.docs_directory} n'existe pas")
            return

        # Charger tous les fichiers .txt
        for filename in os.listdir(self.docs_directory):
            if filename.endswith('.txt'):
                filepath = os.path.join(self.docs_directory, filename)
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        self.documents[filename] = f.read()
                except Exception as e:
                    print(f"‚ö†Ô∏è Erreur lors de la lecture de {filename}: {e}")

        print(f"üìö {len(self.documents)} documents charg√©s dans le dictionnaire")

    def count_tokens(self, text: str, model: str = "cl100k_base") -> int:
        """
        Compte le nombre de tokens dans le texte en utilisant tiktoken.

        Args:
            text (str): Le texte √† compter
            model (str): Le mod√®le de tokenizer √† utiliser (default: cl100k_base pour GPT-4)

        Returns:
            int: Nombre de tokens dans le texte
        """
        encoder = tiktoken.get_encoding(model)
        return len(encoder.encode(text))

    def _load_documents(self):
        """
        Charge tous les documents depuis le r√©pertoire pharmacy_docs

        Returns:
            list: Liste de documents LangChain
        """
        print(f"Chargement des documents depuis {self.docs_directory}...")

        if not os.path.exists(self.docs_directory):
            print(f"Erreur: Le r√©pertoire {self.docs_directory} n'existe pas")
            return []

        # Utiliser DirectoryLoader pour charger tous les fichiers .txt
        loader = DirectoryLoader(
            self.docs_directory,
            glob="**/*.txt",
            loader_cls=TextLoader,
            loader_kwargs={"encoding": "utf-8"},
        )

        documents = loader.load()

        print(f"‚úÖ Charg√© {len(documents)} documents")

        # Peupler le dictionnaire documents et afficher les infos
        for i, doc in enumerate(documents):
            filename = os.path.basename(doc.metadata.get("source", "Unknown"))
            tokens = self.count_tokens(doc.page_content)
            print(f"  {i+1}. {filename} ({tokens} tokens)")

            # Ajouter au dictionnaire documents
            self.documents[filename] = doc.page_content

        # Compter les tokens totaux
        total_tokens = sum(self.count_tokens(doc.page_content) for doc in documents)
        print(f"Total tokens dans les documents: {total_tokens}")

        return documents

    def _split_documents(self, documents):
        """
        D√©coupe les documents en chunks pour un meilleur retrieval

        Args:
            documents (list): Liste de documents √† d√©couper

        Returns:
            list: Liste de documents d√©coup√©s
        """
        print("D√©coupage des documents en chunks...")

        # Initialiser le text splitter avec tiktoken
        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
            chunk_size=1000,  # Chunks plus petits pour la pharmacie
            chunk_overlap=200,  # Overlap pour maintenir le contexte
        )

        # D√©couper les documents
        split_docs = text_splitter.split_documents(documents)

        print(f"‚úÖ Cr√©√© {len(split_docs)} chunks depuis les documents")

        # Compter les tokens totaux
        total_tokens = sum(self.count_tokens(doc.page_content) for doc in split_docs)
        print(f"Total tokens dans les chunks: {total_tokens}")

        return split_docs

    def _create_vectorstore(self, splits):
        """
        Cr√©e un vectorstore depuis les chunks de documents

        Args:
            splits (list): Liste de documents d√©coup√©s √† embedder

        Returns:
            SKLearnVectorStore: Vectorstore contenant les documents embedd√©s
        """
        print("Cr√©ation du SKLearnVectorStore...")

        # Initialiser les embeddings OpenAI
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

        # Cr√©er le vectorstore
        vectorstore = SKLearnVectorStore.from_documents(
            documents=splits,
            embedding=embeddings,
            persist_path=self.vectorstore_path,
            serializer="parquet",
        )

        print("‚úÖ SKLearnVectorStore cr√©√© avec succ√®s")

        # Persister le vectorstore
        vectorstore.persist()

        # Afficher le chemin absolu du vectorstore
        abs_path = os.path.abspath(self.vectorstore_path)
        print(f"‚úÖ Vectorstore persist√© dans {self.vectorstore_path}")
        print(f"üéØ CHEMIN ABSOLU DU VECTORSTORE: {abs_path}")

        return vectorstore

    def _initialize_vectorstore(self):
        """
        Initialise ou charge le vectorstore

        Returns:
            SKLearnVectorStore: Vectorstore pr√™t √† l'emploi
        """
        # Si le vectorstore existe d√©j√†, le charger
        if os.path.exists(self.vectorstore_path):
            print(
                f"Chargement du vectorstore existant depuis {self.vectorstore_path}..."
            )
            embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
            vectorstore = SKLearnVectorStore(
                embedding=embeddings,
                persist_path=self.vectorstore_path,
                serializer="parquet",
            )
            print("‚úÖ Vectorstore charg√© avec succ√®s")
            return vectorstore

        # Sinon, cr√©er un nouveau vectorstore
        print("Cr√©ation d'un nouveau vectorstore...")
        documents = self._load_documents()

        if not documents:
            raise ValueError("Aucun document trouv√© dans le r√©pertoire pharmacy_docs")

        splits = self._split_documents(documents)
        vectorstore = self._create_vectorstore(splits)

        return vectorstore

    def rebuild_vectorstore(self):
        """
        Reconstruit le vectorstore depuis les documents sources
        Utile si les documents ont √©t√© modifi√©s
        """
        print("Reconstruction du vectorstore...")

        # Supprimer l'ancien vectorstore s'il existe
        if os.path.exists(self.vectorstore_path):
            os.remove(self.vectorstore_path)
            print(f"Ancien vectorstore supprim√©: {self.vectorstore_path}")

        # Cr√©er un nouveau vectorstore
        documents = self._load_documents()
        splits = self._split_documents(documents)
        self.vectorstore = self._create_vectorstore(splits)

        # Recr√©er le retriever
        self.retriever = self.vectorstore.as_retriever(
            search_type="similarity", search_kwargs={"k": 5}
        )

        print("‚úÖ Vectorstore reconstruit avec succ√®s")

    def researcher_node(self, state: GraphState) -> GraphState:
        """
        Node Chercheur : Recherche les informations pertinentes via RAG
        """
        question = state["question"]

        # R√©cup√©rer les documents pertinents via le retriever
        retrieved_docs = self.retriever.invoke(question)

        # Cr√©er le contexte √† partir des documents r√©cup√©r√©s
        context = "\n\n=== DOCUMENTS PERTINENTS TROUV√âS ===\n"
        for i, doc in enumerate(retrieved_docs):
            source = os.path.basename(doc.metadata.get("source", "Unknown"))
            context += (
                f"\n--- Extrait {i+1} (Source: {source}) ---\n{doc.page_content}\n"
            )

        prompt = f"""Tu es un chercheur pharmaceutique exp√©riment√© avec une connaissance
approfondie des m√©dicaments, des suppl√©ments et des produits de sant√©.

Tu as acc√®s aux extraits de documents suivants, r√©cup√©r√©s par recherche s√©mantique:
{context}

QUESTION: {question}

T√ÇCHE: Analyse ces extraits et identifie toutes les informations pertinentes pour r√©pondre √† cette question.
Extrais les passages cl√©s, les donn√©es importantes et toute information utile.

FORMAT DE SORTIE: Liste des informations pertinentes trouv√©es dans les extraits, avec r√©f√©rences aux sources."""

        response = self.llm.invoke(prompt)

        state["research_output"] = response.content
        state["messages"] = [("system", f"[RESEARCHER] {response.content}")]

        return state

    def analyst_node(self, state: GraphState) -> GraphState:
        """
        Node Analyste : Analyse et synth√©tise les informations trouv√©es
        """
        question = state["question"]
        research_output = state["research_output"]

        prompt = f"""Tu es un analyste de donn√©es sp√©cialis√© dans le domaine pharmaceutique.
Tu excelles dans la synth√®se d'informations complexes et la pr√©sentation de donn√©es
de mani√®re claire et concise.

QUESTION INITIALE: {question}

INFORMATIONS TROUV√âES PAR LE CHERCHEUR:
{research_output}

T√ÇCHE: Analyse ces informations et cr√©e une synth√®se structur√©e qui r√©pond √† la question.
Organise les donn√©es de mani√®re logique et hi√©rarchis√©e.

FORMAT DE SORTIE: Synth√®se structur√©e des informations pertinentes avec organisation claire."""

        response = self.llm.invoke(prompt)

        state["analysis_output"] = response.content
        state["messages"].append(("system", f"[ANALYST] {response.content}"))

        return state

    def expert_node(self, state: GraphState) -> GraphState:
        """
        Node Expert : Fournit la r√©ponse finale experte
        """
        question = state["question"]
        analysis_output = state["analysis_output"]

        prompt = f"""Tu es un pharmacien avec des d√©cennies d'exp√©rience dans la vente et
la recommandation de produits pharmaceutiques. Tu comprends les besoins des clients
et sais comment communiquer des informations m√©dicales complexes de mani√®re accessible.

QUESTION: {question}

ANALYSE FOURNIE:
{analysis_output}

T√ÇCHE: En te basant sur l'analyse fournie, r√©ponds de mani√®re compl√®te et pr√©cise √† la question.
Utilise ton expertise pour fournir une r√©ponse professionnelle, structur√©e et fiable.

FORMAT DE SORTIE: R√©ponse compl√®te et pr√©cise √† la question, r√©dig√©e de mani√®re professionnelle et accessible."""

        response = self.llm.invoke(prompt)

        state["final_answer"] = response.content
        state["messages"].append(("system", f"[EXPERT] {response.content}"))

        return state

    def _build_graph(self) -> StateGraph:
        """
        Construit le graphe LangGraph avec les 3 nodes s√©quentiels
        """
        # Cr√©er le graphe
        workflow = StateGraph(GraphState)

        # Ajouter les nodes
        workflow.add_node("researcher", self.researcher_node)
        workflow.add_node("analyst", self.analyst_node)
        workflow.add_node("expert", self.expert_node)

        # D√©finir les edges (flux s√©quentiel)
        workflow.set_entry_point("researcher")
        workflow.add_edge("researcher", "analyst")
        workflow.add_edge("analyst", "expert")
        workflow.add_edge("expert", END)

        # Compiler le graphe
        return workflow.compile()

    def answer_question(self, question: str) -> str:
        """
        R√©pond √† une question en utilisant le workflow LangGraph avec RAG

        Args:
            question: La question pos√©e

        Returns:
            R√©ponse √† la question
        """
        # √âtat initial
        initial_state = {
            "question": question,
            "research_output": "",
            "analysis_output": "",
            "final_answer": "",
            "messages": [],
        }

        # Ex√©cuter le graphe
        result = self.graph.invoke(initial_state)

        # Retourner la r√©ponse finale
        return result["final_answer"]

    def get_stats(self):
        """
        Retourne des statistiques sur le vectorstore

        Returns:
            dict: Statistiques du syst√®me RAG
        """
        # Compter les documents dans le vectorstore
        # Note: SKLearnVectorStore ne fournit pas directement un count
        # On peut utiliser une requ√™te factice pour estimer

        stats = {
            "vectorstore_path": self.vectorstore_path,
            "vectorstore_exists": os.path.exists(self.vectorstore_path),
            "docs_directory": self.docs_directory,
            "retriever_k": 5,
            "llm_model": "gpt-4o-mini",
            "embedding_model": "text-embedding-3-small",
        }

        return stats
