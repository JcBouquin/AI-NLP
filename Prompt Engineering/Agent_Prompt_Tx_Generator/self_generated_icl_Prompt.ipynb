{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U langgraph langsmith\n",
    "! pip install langchain_community\n",
    "! pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END ,state\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage,AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration de la clé API\n",
    "os.environ['OPENAI_API_KEY'] = \"sk-proj-0xUZ6aBpi14Q FfHS_cUMhXQMX6_U0pycw_XiZUUtZ4V6Gc5xEwhMZOsYA6xKN4HruNnPRcA\"\n",
    "\n",
    "\n",
    "# Création du modèle LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Generated In-Context Learning (SG-ICL)\n",
    "\n",
    "Self-Generated In-Context Learning, introduced by Kim et al. (2022), is an innovative method that enhances language models' (LLMs) performance by generating their own instructive examples when real training data is scarce or unavailable.\n",
    "\n",
    "## How Does Self-Generated In-Context Learning Work?\n",
    "\n",
    "Imagine you're preparing for an exam but have no practice questions. Instead of waiting for someone to provide examples, you could create your own practice problems and solutions, then refine them as you identify weaknesses.\n",
    "\n",
    "Self-Generated In-Context Learning follows this logic in three key steps:\n",
    "\n",
    "1. **Example Generation**: The LLM automatically creates relevant examples aligned with the specific test question or task it needs to solve.\n",
    "\n",
    "2. **Quality Assessment**: The generated examples are evaluated for their relevance, format appropriateness, diversity, and complexity level. This assessment helps identify examples that may not provide optimal learning context.\n",
    "\n",
    "3. **Targeted Refinement**: Examples that don't meet quality criteria are refined to better match the test question's domain and format, creating more effective in-context examples.\n",
    "\n",
    "By using these self-generated, refined examples as context, the LLM can produce more accurate responses even without access to real training data. This approach effectively turns the model into both teacher and student, leveraging its own knowledge to improve performance on new tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_generated_icl_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "    Generate examples using Self-Generated In-Context Learning (SG-ICL) for a given user question.\n",
    "    \n",
    "    SG-ICL (Kim et al., 2022) is a technique where AI automatically generates relevant examples when real training data is not available. These examples then serve as context for answering the user's question.\n",
    "    \n",
    "    Process to follow:\n",
    "    1. Carefully analyze the test question to identify the specific task requested\n",
    "    2. Generate 3-5 examples that are directly aligned with the format and domain of the test question\n",
    "    3. Systematically evaluate each example according to the following criteria:\n",
    "       - Relevance: Is the example similar to the test question?\n",
    "       - Format: Does the example follow the same structure as what is expected?\n",
    "       - Diversity: Do the examples cover different facets of the task?\n",
    "       - Complexity: Do the examples reflect the appropriate level of difficulty?\n",
    "    4. Refine examples that do not meet the above criteria\n",
    "    5. Use the refined examples as in-context context to answer the original question\n",
    "    \n",
    "    Example:\n",
    "    Test question: \"Classify this tweet as positive, negative or neutral: 'The new movie was disappointing despite the good reviews.'\"\n",
    "    \n",
    "    Analysis: This is a sentiment classification task for a tweet about a movie.\n",
    "    \n",
    "    Generated examples:\n",
    "    Example 1:\n",
    "    Q: Classify this tweet as positive, negative or neutral: \"I love this new app, it has changed my life!\"\n",
    "    A: Positive\n",
    "    \n",
    "    Example 2:\n",
    "    Q: Classify this tweet as positive, negative or neutral: \"Customer service hasn't responded to my request for three days.\"\n",
    "    A: Negative\n",
    "    \n",
    "    Example 3:\n",
    "    Q: Classify this tweet as positive, negative or neutral: \"The concert starts at 8pm tonight at the municipal stadium.\"\n",
    "    A: Neutral\n",
    "    \n",
    "    Evaluation:\n",
    "    - Relevance: The examples follow the same type of task but not all relate to movies/entertainment\n",
    "    - Format: The format is correct and consistent\n",
    "    - Diversity: All three categories are represented\n",
    "    - Complexity: The examples are too simple and direct compared to the nuance in the test question\n",
    "    \n",
    "    Refined examples:\n",
    "    Example 1:\n",
    "    Q: Classify this tweet as positive, negative or neutral: \"This movie was exactly what I hoped for, with amazing special effects!\"\n",
    "    A: Positive\n",
    "    \n",
    "    Example 2:\n",
    "    Q: Classify this tweet as positive, negative or neutral: \"The series had a good start but ended in an unsatisfying way.\"\n",
    "    A: Negative\n",
    "    \n",
    "    Example 3:\n",
    "    Q: Classify this tweet as positive, negative or neutral: \"The film's release has been postponed until next year according to yesterday's announcement.\"\n",
    "    A: Neutral\n",
    "    \n",
    "    Final answer to the test question:\n",
    "    Negative\n",
    "    \"\"\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "    (\"assistant\", \"\"\"\n",
    "    Using Self-Generated In-Context Learning (SG-ICL), I will generate examples specifically adapted to your question, since we don't have real training data available.\n",
    "    \n",
    "    I will first analyze the structure and specific domain of your question, then generate relevant examples that follow the same format and reflect similar complexity. I will then evaluate these examples based on their relevance, format, diversity, and appropriate level of complexity before refining them if necessary. Finally, I will use these examples as context to formulate my answer to your question.\n",
    "    \"\"\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Most Suitable Questions for Self-Generated In-Context Learning (SG-ICL)\n",
    "\n",
    "For Self-Generated In-Context Learning, the most suitable questions are those where the model benefits from generating its own examples when real training data is unavailable or limited. Here are types of questions that would be particularly well-suited to this approach:\n",
    "\n",
    "### Classification Tasks\n",
    "- Sentiment analysis requiring nuanced understanding of tone\n",
    "- Content categorization across diverse domains\n",
    "- Intent recognition in conversational contexts\n",
    "- Anomaly detection where patterns must be inferred\n",
    "\n",
    "### Language Generation Tasks\n",
    "- Summarization of specialized content like technical documents\n",
    "- Style transfer between different writing formats\n",
    "- Paraphrasing that preserves specific semantic elements\n",
    "- Translation for low-resource language pairs\n",
    "\n",
    "### Reasoning Tasks\n",
    "- Common sense reasoning requiring implicit knowledge\n",
    "- Logical deduction from limited information\n",
    "- Multi-hop inference tasks with complex relationships\n",
    "- Analogical reasoning requiring pattern recognition\n",
    "\n",
    "### Pattern Recognition\n",
    "- Time series prediction with subtle patterns\n",
    "- Entity relationship extraction from text\n",
    "- Sequence completion with domain-specific rules\n",
    "- Document structure analysis\n",
    "\n",
    "### Concrete Examples of Questions\n",
    "\n",
    "1. \"Classify this medical text as relevant to cardiology, neurology, or oncology.\"\n",
    "   - SG-ICL would generate varied examples of medical texts across specialties before classification\n",
    "\n",
    "2. \"Summarize this technical documentation about a machine learning framework.\"\n",
    "   - SG-ICL would create examples of technical summaries with appropriate terminology and abstraction levels\n",
    "\n",
    "3. \"What would be the most appropriate response to this customer service inquiry?\"\n",
    "   - SG-ICL would generate examples of different inquiry types and appropriate response styles\n",
    "\n",
    "4. \"Detect whether this product review contains any safety concerns.\"\n",
    "   - SG-ICL would create examples of reviews with and without safety concerns\n",
    "\n",
    "5. \"Rephrase this academic text for a high school audience while maintaining accuracy.\"\n",
    "   - SG-ICL would generate examples of academic rephrasing at different complexity levels\n",
    "\n",
    "Self-Generated In-Context Learning is particularly effective for these types of questions because:\n",
    "- It creates tailored examples when domain-specific training data is unavailable\n",
    "- It allows for customization of examples to match the specific test question format\n",
    "- It helps bridge knowledge gaps through synthetic examples\n",
    "- It can generate diverse examples that cover edge cases\n",
    "- It mimics the human learning process of creating practice problems to master new concepts\n",
    "\n",
    "This approach is especially powerful when dealing with specialized domains, emerging tasks, or situations where high-quality labeled examples are scarce, but the model contains enough knowledge to generate its own instructive examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION:\n",
      "Classify this tweet as positive, negative or neutral: 'The new movie was disappointing despite the good reviews.'\n",
      "\n",
      "FINAL ANSWER:\n",
      "Given the context provided by the examples, the sentiment of the tweet \"The new movie was disappointing despite the good reviews.\" is classified as **Negative**.\n",
      "\n",
      "(Complete results saved in 'sgicl_result.txt')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from typing import Optional, TypedDict, Annotated\n",
    "from langchain.schema import AIMessage, HumanMessage\n",
    "\n",
    "# Function to generate the SG-ICL prompt and extract the final answer\n",
    "def generate_sgicl_node(state):\n",
    "    question = state['messages'][-1].content  # Get the last question\n",
    "    prompt_value = self_generated_icl_template.invoke({\"question\": question})\n",
    "    messages = prompt_value.to_messages()\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Keep the full response for processing\n",
    "    full_response = response.content\n",
    "    \n",
    "    # Multiple patterns to capture different ways the final answer might be formatted\n",
    "    patterns = [\n",
    "        # Standard format with heading\n",
    "        r'(?:Final [Aa]nswer to the [Tt]est [Qq]uestion:|Final Answer:)([\\s\\S]*?)(?=$|Generated [Ee]xamples:|Evaluation:|###)',\n",
    "        # Format with heading and line breaks\n",
    "        r'###\\s*Final\\s*Answer[^:]*:([\\s\\S]*?)(?=$|###|Generated)',\n",
    "        # Classification result format\n",
    "        r'classified as\\s*\\*\\*(Positive|Negative|Neutral)\\*\\*',\n",
    "        # Simple statement format\n",
    "        r'The sentiment (?:of|in) the tweet is\\s*(Positive|Negative|Neutral)',\n",
    "        # Direct answer format\n",
    "        r'(?:Answer|Result):\\s*(Positive|Negative|Neutral)',\n",
    "        # Last resort - any standalone sentiment classification\n",
    "        r'(Positive|Negative|Neutral)\\.?$'\n",
    "    ]\n",
    "    \n",
    "    final_solution = None\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, full_response, re.IGNORECASE)\n",
    "        if match:\n",
    "            # For classification patterns, we want the sentiment directly\n",
    "            if \"Positive|Negative|Neutral\" in pattern:\n",
    "                group_index = 1 if len(match.groups()) > 0 else 0\n",
    "                final_solution = match.group(group_index).strip()\n",
    "            else:\n",
    "                final_solution = match.group(1).strip()\n",
    "            break\n",
    "    \n",
    "    # If no pattern matched, provide a fallback\n",
    "    if not final_solution:\n",
    "        final_solution = \"Final solution not found in the expected format.\"\n",
    "    \n",
    "    # Save both the question, final answer, and full processing to a text file\n",
    "    with open('sgicl_result.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=== QUESTION ===\\n\\n\")\n",
    "        f.write(question)\n",
    "        f.write(\"\\n\\n=== FINAL ANSWER ===\\n\\n\")\n",
    "        f.write(final_solution)\n",
    "        f.write(\"\\n\\n=== COMPLETE PROCESSING (FOR REFERENCE) ===\\n\\n\")\n",
    "        f.write(full_response)\n",
    "    \n",
    "    # Return only the final answer in messages, plus the original question for reference\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=final_solution)],\n",
    "        \"question\": question,\n",
    "        \"full_response\": full_response\n",
    "    }\n",
    "\n",
    "# Definition of state\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    question: Optional[str]\n",
    "    full_response: Optional[str]\n",
    "\n",
    "# Graph creation\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"generate_sgicl\", generate_sgicl_node)\n",
    "\n",
    "# Graph configuration\n",
    "graph_builder.set_entry_point(\"generate_sgicl\")\n",
    "graph_builder.add_edge(\"generate_sgicl\", END)\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# Usage example\n",
    "inputs = {\n",
    "    \"messages\": [HumanMessage(content=\"Classify this tweet as positive, negative or neutral: 'The new movie was disappointing despite the good reviews.'\")],\n",
    "    \"question\": None,\n",
    "    \"full_response\": None\n",
    "}\n",
    "\n",
    "# Print the question at the beginning\n",
    "print(\"QUESTION:\")\n",
    "print(inputs[\"messages\"][0].content)\n",
    "print(\"\\nFINAL ANSWER:\")\n",
    "\n",
    "# Graph execution - only display final answer\n",
    "original_question = \"\"\n",
    "for output in graph.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        if key == \"generate_sgicl\":\n",
    "            messages = value['messages']\n",
    "            for message in messages:\n",
    "                if isinstance(message, AIMessage):\n",
    "                    print(message.content)\n",
    "            \n",
    "            # Store question for reference\n",
    "            if \"question\" in value:\n",
    "                original_question = value[\"question\"]\n",
    "\n",
    "# Print confirmation that results were saved\n",
    "print(\"\\n(Complete results saved in 'sgicl_result.txt')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
