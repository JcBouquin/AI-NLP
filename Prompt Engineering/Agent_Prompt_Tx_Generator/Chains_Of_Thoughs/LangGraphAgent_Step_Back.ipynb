{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U langgraph langsmith\n",
    "! pip install langchain_community\n",
    "! pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END ,state\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage,AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "# Configuration de la clé API\n",
    "os.environ['OPENAI_API_KEY'] = \"sk-proj-0xUZ6aBpi14QWLtzQC2nF0B2gQTojxukve mq8WeoK0yCT3BlbkFJ6nuNH0hRFwAy9HhFfHS_cUMhXQMX6_U0pycw_XiZUUtZ4V6Gc5xEwhMZOsYA6xKN4HruNnPRcA\"\n",
    "\n",
    "\n",
    "# Création du modèle LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-Back Prompting: A Strategic Approach to Strengthen LLM Reasoning\n",
    "\n",
    "Step-Back Prompting, introduced by Zheng et al. (2023c), is an innovative method that modifies the classic Chain-of-Thought reasoning approach by adding an essential preliminary phase.\n",
    "\n",
    "## How Does Step-Back Prompting Work?\n",
    "\n",
    "Imagine a mathematician faced with a complex problem. Before diving into detailed calculations, they would first take a step back to identify general concepts and relevant theoretical principles.\n",
    "\n",
    "Step-Back Prompting reproduces this intellectual approach in two distinct phases:\n",
    "\n",
    "1. **Generic preliminary question**: Before addressing the specific problem, the LLM is asked to reflect on high-level concepts, general principles, or fundamental facts related to the problem domain.\n",
    "\n",
    "2. **Detailed reasoning**: Only after this conceptual contextualization step does the model engage in detailed reasoning to solve the specific problem.\n",
    "\n",
    "## Why Is This So Effective?\n",
    "\n",
    "This approach offers considerable advantages by allowing the model to:\n",
    "\n",
    "* Activate relevant knowledge before applying it\n",
    "* Establish a solid conceptual framework to guide reasoning\n",
    "* Avoid errors due to an overly direct and hasty approach\n",
    "* Break down complex problems into theoretical and then practical components\n",
    "\n",
    "The results are particularly impressive in:\n",
    "* Mathematical reasoning problems\n",
    "* Complex scientific questions\n",
    "* Problems requiring the application of abstract principles to concrete cases\n",
    "\n",
    "It's like teaching a student to take the time to understand the theory before tackling exercises.\n",
    "\n",
    "This method has demonstrated significant improvements on several reasoning benchmarks for both PaLM2L and GPT-4 models, confirming its robustness and effectiveness across different LLM architectures.\n",
    "\n",
    "The next time you ask an AI a complex question, try inviting it to first reflect on relevant general concepts before asking for the specific answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_back_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "    Generate Step-Back Prompting based on user questions.\n",
    "    \n",
    "    Step-Back Prompting is a modification of Chain-of-Thought (CoT) where the LLM is first asked to consider high-level concepts or principles before delving into detailed reasoning.\n",
    "    \n",
    "    This technique significantly improves performance on complex reasoning tasks by encouraging the model to first understand the broader context and applicable principles before addressing specific details.\n",
    "    \n",
    "    The Step-Back process includes the following steps:\n",
    "    1. High-level abstraction: First, identify the general principles, concepts, or frameworks that are relevant to the question.\n",
    "    2. Conceptual analysis: Explore how these high-level concepts apply to the specific problem.\n",
    "    3. Detailed reasoning: Once the conceptual framework is established, work through the specific problem step by step.\n",
    "    4. Answer the original question: Provide a final answer based on the analysis.\n",
    "    \n",
    "    Please structure your response in the following format:\n",
    "    \n",
    "    Step 1: [High-level concepts and principles]\n",
    "    Step 2: [Application of these concepts to the problem]\n",
    "    Step 3: [Step-by-step reasoning]\n",
    "    Step 4: [Final Answer]\n",
    "    \n",
    "    Example:\n",
    "    Question: \"A store is having a 25% off sale. If an item originally costs $80, what is the final price after the discount and 7% sales tax?\"\n",
    "    \n",
    "    Step 1: High-level concepts and principles:\n",
    "    Before diving into calculations, let me consider the key concepts involved in this problem:\n",
    "    - Percentage discounts: When an item is discounted by X%, we multiply the price by (100%-X%) or (1-X/100)\n",
    "    - Sales tax: Tax is applied after discounts, and we add X% tax by multiplying by (100%+X%) or (1+X/100)\n",
    "    - Order of operations: In retail pricing, discounts are applied first, then tax is calculated on the discounted amount\n",
    "    \n",
    "    Step 2: Application of these concepts:\n",
    "    For this specific problem:\n",
    "    - We need to apply a 25% discount to the original price of $80\n",
    "    - Then we need to apply a 7% sales tax to the discounted price\n",
    "    \n",
    "    Step 3: Step-by-step reasoning:\n",
    "    1. Calculate the discounted price:\n",
    "       - Original price = $80\n",
    "       - Discount rate = 25% = 0.25\n",
    "       - Discount amount = $80 × 0.25 = $20\n",
    "       - Discounted price = $80 - $20 = $60\n",
    "       - Alternatively: Discounted price = $80 × (1 - 0.25) = $80 × 0.75 = $60\n",
    "    \n",
    "    2. Calculate the tax on the discounted price:\n",
    "       - Discounted price = $60\n",
    "       - Tax rate = 7% = 0.07\n",
    "       - Tax amount = $60 × 0.07 = $4.20\n",
    "    \n",
    "    3. Calculate the final price:\n",
    "       - Final price = Discounted price + Tax amount\n",
    "       - Final price = $60 + $4.20 = $64.20\n",
    "    \n",
    "    Step 4: The final price of the item after applying the 25% discount and 7% sales tax is $64.20.\n",
    "    \"\"\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "    (\"assistant\", \"\"\"\n",
    "    Step 1: High-level concepts and principles:\n",
    "    [Identify and explain the general principles, theories, or frameworks that are relevant to the question. What domain knowledge is necessary to understand this problem at a conceptual level?]\n",
    "    \n",
    "    Step 2: Application of these concepts:\n",
    "    [Explain how these high-level concepts specifically relate to the problem at hand. What aspects of these principles are most relevant? How do they provide a framework for solving the specific question?]\n",
    "    \n",
    "    Step 3: Step-by-step reasoning:\n",
    "    [Now that the conceptual groundwork is laid, work through the problem methodically:\n",
    "    1. [First step in the solution process]\n",
    "    2. [Second step in the solution process]\n",
    "    3. [Continue with additional steps as needed]\n",
    "    ...]\n",
    "    \n",
    "    Step 4: [Provide a clear, concise answer to the original question, drawing directly from the reasoning process above]\n",
    "    \"\"\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Step-Back Prompting, the most suitable questions are those that benefit from first establishing high-level conceptual foundations before diving into specific problem-solving. Here are types of questions that would be particularly well-suited to this approach:\n",
    "\n",
    "### Scientific Problem Solving\n",
    "- Physics problems requiring application of fundamental laws\n",
    "- Chemistry questions involving reaction mechanisms or principles\n",
    "- Biology problems relating to systemic processes or evolutionary principles\n",
    "- Environmental science questions requiring multi-disciplinary understanding\n",
    "\n",
    "### Mathematical Reasoning\n",
    "- Word problems that map to specific mathematical concepts\n",
    "- Proof-based questions in geometry or algebra\n",
    "- Applied mathematics problems in real-world contexts\n",
    "- Probability and statistics questions requiring conceptual clarity first\n",
    "\n",
    "### Conceptual Analysis\n",
    "- Philosophical questions requiring definitional clarity\n",
    "- Questions involving ethical frameworks or principles\n",
    "- Policy analysis requiring understanding of underlying theories\n",
    "- Technical questions that benefit from domain knowledge\n",
    "\n",
    "### Concrete Examples of Questions\n",
    "\n",
    "1. \"How would you calculate the trajectory of a projectile fired at a 45-degree angle with an initial velocity of 20 m/s?\"\n",
    "   - Step-back would first explore concepts of projectile motion, gravity, and kinematics\n",
    "\n",
    "2. \"What would be the most appropriate statistical test to determine if there's a significant difference between these two treatment groups?\"\n",
    "   - Step-back would establish statistical principles, hypothesis testing, and data type considerations\n",
    "\n",
    "3. \"How would you design a sustainable urban transportation system that reduces carbon emissions by 50%?\"\n",
    "   - Step-back would explore transportation system principles, sustainability concepts, and emission factors\n",
    "\n",
    "4. \"What would be the environmental impact of replacing 30% of beef consumption with plant-based alternatives?\"\n",
    "   - Step-back would establish concepts of carbon footprints, land use, agricultural systems, and nutritional requirements\n",
    "\n",
    "5. \"How should we approach the ethical question of data privacy in AI-driven healthcare systems?\"\n",
    "   - Step-back would first explore broader principles of medical ethics, data ownership, and stakeholder interests\n",
    "\n",
    "Step-Back Prompting is particularly effective for these types of questions because:\n",
    "\n",
    "- It activates relevant conceptual knowledge before application\n",
    "- It establishes a solid theoretical foundation that guides the specific reasoning\n",
    "- It helps avoid jumping to solutions before understanding the underlying principles\n",
    "- It improves reasoning by connecting specific problems to general knowledge domains\n",
    "- It mimics expert problem-solving approaches which typically involve contextualizing problems within broader knowledge frameworks\n",
    "\n",
    "This approach is especially powerful when dealing with problems that require application of established principles or when proper framing of the problem is crucial to finding the correct solution path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION:\n",
      "How would you calculate the trajectory of a projectile fired at a 45-degree angle with an initial velocity of 20 m/s?\n",
      "\n",
      "FINAL ANSWER (STEP 4):\n",
      "The trajectory of the projectile fired at a 45-degree angle with an initial velocity of 20 m/s results in a range of approximately 40.65 meters, a maximum height of approximately 10.1 meters, and a total time of flight of approximately 2.88 seconds.\n",
      "\n",
      "(Complete results saved in 'step_back_result.txt')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from typing import Optional, TypedDict, Annotated\n",
    "from langchain.schema import AIMessage, HumanMessage\n",
    "\n",
    "# Function to generate the Step-Back prompt and extract the final answer (Step 4)\n",
    "def generate_step_back_node(state):\n",
    "    question = state['messages'][-1].content  # Get the last question\n",
    "    prompt_value = step_back_prompt_template.invoke({\"question\": question})\n",
    "    messages = prompt_value.to_messages()\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Keep the full response for processing\n",
    "    full_response = response.content\n",
    "    \n",
    "    # Extract the final answer from Step 4\n",
    "    final_answer_pattern = r'Step 4:([\\s\\S]*?)(?=$|Step \\d:)'\n",
    "    final_answer_match = re.search(final_answer_pattern, full_response)\n",
    "    \n",
    "    if final_answer_match:\n",
    "        final_solution = final_answer_match.group(1).strip()\n",
    "    else:\n",
    "        # Alternative pattern\n",
    "        alternative_pattern = r'Step 4: \\[Final Answer\\]([\\s\\S]*?)(?=$|Step \\d:)'\n",
    "        alternative_match = re.search(alternative_pattern, full_response)\n",
    "        \n",
    "        if alternative_match:\n",
    "            final_solution = alternative_match.group(1).strip()\n",
    "        else:\n",
    "            final_solution = \"Final solution not found in the expected format.\"\n",
    "    \n",
    "    # Save both the question, final answer, and full processing to a text file\n",
    "    with open('step_back_result.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=== QUESTION ===\\n\\n\")\n",
    "        f.write(question)\n",
    "        f.write(\"\\n\\n=== FINAL ANSWER (STEP 4) ===\\n\\n\")\n",
    "        f.write(final_solution)\n",
    "        f.write(\"\\n\\n=== COMPLETE PROCESSING (FOR REFERENCE) ===\\n\\n\")\n",
    "        f.write(full_response)\n",
    "    \n",
    "    # Return only the final answer in messages, plus the original question for reference\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=final_solution)],\n",
    "        \"question\": question,\n",
    "        \"full_response\": full_response\n",
    "    }\n",
    "\n",
    "# Definition of state\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    question: Optional[str]\n",
    "    full_response: Optional[str]\n",
    "\n",
    "# Graph creation\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"generate_step_back\", generate_step_back_node)\n",
    "\n",
    "# Graph configuration\n",
    "graph_builder.set_entry_point(\"generate_step_back\")\n",
    "graph_builder.add_edge(\"generate_step_back\", END)\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# Usage example\n",
    "inputs = {\n",
    "    \"messages\": [HumanMessage(content=\"How would you calculate the trajectory of a projectile fired at a 45-degree angle with an initial velocity of 20 m/s?\")],\n",
    "    \"question\": None,\n",
    "    \"full_response\": None\n",
    "}\n",
    "\n",
    "# Print the question at the beginning\n",
    "print(\"QUESTION:\")\n",
    "print(inputs[\"messages\"][0].content)\n",
    "print(\"\\nFINAL ANSWER (STEP 4):\")\n",
    "\n",
    "# Graph execution - only display Step 4\n",
    "original_question = \"\"\n",
    "for output in graph.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        if key == \"generate_step_back\":\n",
    "            messages = value['messages']\n",
    "            for message in messages:\n",
    "                if isinstance(message, AIMessage):\n",
    "                    print(message.content)\n",
    "            \n",
    "            # Store question for reference\n",
    "            if \"question\" in value:\n",
    "                original_question = value[\"question\"]\n",
    "\n",
    "# Print confirmation that results were saved\n",
    "print(\"\\n(Complete results saved in 'step_back_result.txt')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION:\n",
      "If a factory produces widgets at a rate of 800 per day with 5% defective, and each widget costs $3 to make and sells for $8, what is the expected daily profit after accounting for defects?\n",
      "\n",
      "FINAL SOLUTION:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'uncertainty_routed_cot_template' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 77\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Graph execution - only display the final solution\u001b[39;00m\n\u001b[0;32m     76\u001b[0m original_question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgenerate_uncertainty_routed_cot\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kosmo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2340\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[0;32m   2334\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[0;32m   2335\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[0;32m   2336\u001b[0m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[0;32m   2337\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[0;32m   2338\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[0;32m   2339\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[1;32m-> 2340\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2341\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2342\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2343\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2344\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2345\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   2346\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[0;32m   2347\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2348\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kosmo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langgraph\\pregel\\runner.py:158\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[0;32m    156\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m                \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m                \u001b[49m\u001b[43mreraise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\kosmo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[0;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[1;32mc:\\Users\\kosmo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langgraph\\utils\\runnable.py:606\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    602\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[0;32m    603\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    604\u001b[0m )\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 606\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\kosmo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langgraph\\utils\\runnable.py:371\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[1;32m--> 371\u001b[0m         ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m, in \u001b[0;36mgenerate_uncertainty_routed_cot_node\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_uncertainty_routed_cot_node\u001b[39m(state):\n\u001b[0;32m      8\u001b[0m     question \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent  \u001b[38;5;66;03m# Get the last question\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     prompt_value \u001b[38;5;241m=\u001b[39m \u001b[43muncertainty_routed_cot_template\u001b[49m\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: question})\n\u001b[0;32m     10\u001b[0m     messages \u001b[38;5;241m=\u001b[39m prompt_value\u001b[38;5;241m.\u001b[39mto_messages()\n\u001b[0;32m     11\u001b[0m     response \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(messages)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'uncertainty_routed_cot_template' is not defined",
      "\u001b[0mDuring task with name 'generate_uncertainty_routed_cot' and id '4744d8f5-1b11-faef-c6fb-8c44ded5db3d'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from typing import Optional, TypedDict, Annotated\n",
    "from langchain.schema import AIMessage, HumanMessage\n",
    "\n",
    "# Function to generate the Uncertainty-Routed CoT prompt and extract the final answer\n",
    "def generate_uncertainty_routed_cot_node(state):\n",
    "    question = state['messages'][-1].content  # Get the last question\n",
    "    prompt_value = uncertainty_routed_cot_template.invoke({\"question\": question})\n",
    "    messages = prompt_value.to_messages()\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Keep the full response for processing\n",
    "    full_response = response.content\n",
    "    \n",
    "    # Extract the final solution from Step 4\n",
    "    final_solution_pattern = r'Step 4: Final Solution\\s*([\\s\\S]*?)(?=$|Step \\d:)'\n",
    "    final_solution_match = re.search(final_solution_pattern, full_response)\n",
    "    \n",
    "    if final_solution_match:\n",
    "        final_solution = final_solution_match.group(1).strip()\n",
    "    else:\n",
    "        # Alternative pattern for different formatting\n",
    "        alternative_pattern = r'Step 4:([\\s\\S]*?)(?=$|Step \\d:)'\n",
    "        alternative_match = re.search(alternative_pattern, full_response)\n",
    "        \n",
    "        if alternative_match:\n",
    "            final_solution = alternative_match.group(1).strip()\n",
    "        else:\n",
    "            final_solution = \"Final solution not found in the expected format.\"\n",
    "    \n",
    "    # Save both the question, final answer, and full processing to a text file\n",
    "    with open('uncertainty_routed_cot_result.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=== QUESTION ===\\n\\n\")\n",
    "        f.write(question)\n",
    "        f.write(\"\\n\\n=== FINAL SOLUTION ===\\n\\n\")\n",
    "        f.write(final_solution)\n",
    "        f.write(\"\\n\\n=== COMPLETE PROCESSING (FOR REFERENCE) ===\\n\\n\")\n",
    "        f.write(full_response)\n",
    "    \n",
    "    # Return only the final solution in messages, plus the original question for reference\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=final_solution)],\n",
    "        \"question\": question,\n",
    "        \"full_response\": full_response\n",
    "    }\n",
    "\n",
    "# Definition of state\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    question: Optional[str]\n",
    "    full_response: Optional[str]\n",
    "\n",
    "# Graph creation\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"generate_uncertainty_routed_cot\", generate_uncertainty_routed_cot_node)\n",
    "\n",
    "# Graph configuration\n",
    "graph_builder.set_entry_point(\"generate_uncertainty_routed_cot\")\n",
    "graph_builder.add_edge(\"generate_uncertainty_routed_cot\", END)\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# Usage example\n",
    "inputs = {\n",
    "    \"messages\": [HumanMessage(content=\"If a factory produces widgets at a rate of 800 per day with 5% defective, and each widget costs $3 to make and sells for $8, what is the expected daily profit after accounting for defects?\")],\n",
    "    \"question\": None,\n",
    "    \"full_response\": None\n",
    "}\n",
    "\n",
    "# Print the question at the beginning\n",
    "print(\"QUESTION:\")\n",
    "print(inputs[\"messages\"][0].content)\n",
    "print(\"\\nFINAL SOLUTION:\")\n",
    "\n",
    "# Graph execution - only display the final solution\n",
    "original_question = \"\"\n",
    "for output in graph.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        if key == \"generate_uncertainty_routed_cot\":\n",
    "            messages = value['messages']\n",
    "            for message in messages:\n",
    "                if isinstance(message, AIMessage):\n",
    "                    print(message.content)\n",
    "            \n",
    "            # Store question for reference\n",
    "            if \"question\" in value:\n",
    "                original_question = value[\"question\"]\n",
    "\n",
    "# Print confirmation that results were saved\n",
    "print(\"\\n(Complete results saved in 'uncertainty_routed_cot_result.txt')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
