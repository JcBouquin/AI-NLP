{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOAOuyI/YzKVh8eU8mO7v4v"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"5SoupIEtyKNd","executionInfo":{"status":"ok","timestamp":1718121299993,"user_tz":-120,"elapsed":23256,"user":{"displayName":"Jean-Christophe Bouquin","userId":"01239462757709491259"}},"outputId":"c460258c-44f3-4586-e642-5872c9aa803b","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain_community\n","  Downloading langchain_community-0.2.4-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tiktoken\n","  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langchain-openai\n","  Downloading langchain_openai-0.1.8-py3-none-any.whl (38 kB)\n","Collecting langchainhub\n","  Downloading langchainhub-0.1.18-py3-none-any.whl (4.8 kB)\n","Collecting chromadb\n","  Downloading chromadb-0.5.0-py3-none-any.whl (526 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.8/526.8 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langchain\n","  Downloading langchain-0.2.3-py3-none-any.whl (974 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.0/974.0 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.1)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.30)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.9.5)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Collecting langchain-core<0.3.0,>=0.2.0 (from langchain_community)\n","  Downloading langchain_core-0.2.5-py3-none-any.whl (314 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.7/314.7 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langsmith<0.2.0,>=0.1.0 (from langchain_community)\n","  Downloading langsmith-0.1.76-py3-none-any.whl (124 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.9/124.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.25.2)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.31.0)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.3.0)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n","Collecting openai<2.0.0,>=1.26.0 (from langchain-openai)\n","  Downloading openai-1.33.0-py3-none-any.whl (325 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.5/325.5 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n","  Downloading types_requests-2.32.0.20240602-py3-none-any.whl (15 kB)\n","Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.1)\n","Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.7.3)\n","Collecting chroma-hnswlib==0.7.3 (from chromadb)\n","  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb)\n","  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n","  Downloading uvicorn-0.30.1-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting posthog>=2.4.0 (from chromadb)\n","  Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.12.1)\n","Collecting onnxruntime>=1.14.1 (from chromadb)\n","  Downloading onnxruntime-1.18.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_api-1.25.0-py3-none-any.whl (59 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_grpc-1.25.0-py3-none-any.whl (18 kB)\n","Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n","  Downloading opentelemetry_instrumentation_fastapi-0.46b0-py3-none-any.whl (11 kB)\n","Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_sdk-1.25.0-py3-none-any.whl (107 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.19.1)\n","Collecting pypika>=0.48.9 (from chromadb)\n","  Downloading PyPika-0.48.9.tar.gz (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n","\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25h"]}],"source":["! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain"]},{"cell_type":"code","source":["import pprint\n","import os\n","import openai\n","import getpass\n","env_var = os.environ"],"metadata":{"id":"Z1s1xAlmy8G9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def SetEnv(name):\n","    secret = getpass.getpass(f\"Enter value of {name}: \")   #ls__1c8f2b7063b24917b1b1375627a268e4\n","    os.environ[name] = secret\n","SetEnv('LANGCHAIN_API_KEY')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q90rh09bypgT","executionInfo":{"status":"ok","timestamp":1707539864767,"user_tz":-60,"elapsed":8786,"user":{"displayName":"Jean-Christophe Bouquin","userId":"01239462757709491259"}},"outputId":"e34e82f6-7803-4d37-a7da-fd283b21ff94"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter value of LANGCHAIN_API_KEY: ··········\n"]}]},{"cell_type":"code","source":["os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n","os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n","os.environ['LANGCHAIN_API_KEY'] = 'ls__1c8f2b7063b24917b1b1375627a268e4'"],"metadata":{"id":"bY3Vbwimytnh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import bs4\n","from langchain import hub\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_community.document_loaders import WebBaseLoader\n","from langchain_community.vectorstores import Chroma\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain_openai import ChatOpenAI, OpenAIEmbeddings"],"metadata":{"id":"yOxKJyTpzI5M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#### INDEXING ####\n","\n","# Load Documents\n","loader = WebBaseLoader(\n","    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n","    bs_kwargs=dict(\n","        parse_only=bs4.SoupStrainer(\n","            class_=(\"post-content\", \"post-title\", \"post-header\")\n","        )\n","    ),\n",")\n","docs = loader.load()"],"metadata":{"id":"R4r3TanMzNm0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def SetEnv(name):\n","    secret = getpass.getpass(f\"Enter value of {name}: \")   #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\n","    os.environ[name] = secret\n","SetEnv('OPENAI_API_KEY')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6EhhnkmqzjFW","executionInfo":{"status":"ok","timestamp":1707539917723,"user_tz":-60,"elapsed":14810,"user":{"displayName":"Jean-Christophe Bouquin","userId":"01239462757709491259"}},"outputId":"ab163814-b0b9-43ba-8e6f-ffcd53a0c1ac"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter value of OPENAI_API_KEY: ··········\n"]}]},{"cell_type":"code","source":["# Split\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n","splits = text_splitter.split_documents(docs)\n","\n","# Embed\n","k = Chroma.from_documents(documents=splits,\n","                                    embedding=OpenAIEmbeddings())\n","\n","retriever = vectorstore.as_retriever()"],"metadata":{"id":"UkDIK25HzVNz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#### RETRIEVAL and GENERATION ####\n","\n","# Prompt\n","prompt = hub.pull(\"rlm/rag-prompt\")\n"],"metadata":{"id":"dwLhEvgSzop-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LLM\n","llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n"],"metadata":{"id":"kAcEBBIwzsqj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"-iRRBydb3QXt"}},{"cell_type":"code","source":["# Post-processing\n","def format_docs(docs):\n","    return \"\\n\\n\".join(doc.page_content for doc in docs)\n"],"metadata":{"id":"W26H1mwozv25"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Chain\n","rag_chain = (\n","    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n","    | prompt\n","    | llm\n","    | StrOutputParser()\n",")"],"metadata":{"id":"uKWg-aOTzx64"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Question\n","rag_chain.invoke(\"What is Task Decomposition?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"cTUK_kyEz0kR","executionInfo":{"status":"ok","timestamp":1707539943090,"user_tz":-60,"elapsed":3691,"user":{"displayName":"Jean-Christophe Bouquin","userId":"01239462757709491259"}},"outputId":"f37a0fe6-e2fb-445b-82e4-4f9c2b882db7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done through various methods such as using prompting techniques, task-specific instructions, or human inputs. The goal is to make the task more manageable and facilitate the interpretation of the model's thinking process.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["# Documents\n","question = \"What kinds of pets do I like?\"\n","document = \"My favorite pet is a cat.\""],"metadata":{"id":"YGq3dqt6z4Zs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tiktoken\n","\n","def num_tokens_from_string(string: str, encoding_name: str) -> int:\n","    \"\"\"Returns the number of tokens in a text string.\"\"\"\n","    encoding = tiktoken.get_encoding(encoding_name)\n","    num_tokens = len(encoding.encode(string))\n","    return num_tokens\n","\n","num_tokens_from_string(question, \"cl100k_base\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I90Ivls8z7Vs","executionInfo":{"status":"ok","timestamp":1707498003739,"user_tz":-60,"elapsed":428,"user":{"displayName":"Jean-Christophe Bouquin","userId":"01239462757709491259"}},"outputId":"f3f98236-6cdb-42ba-81af-71927da2a50d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["8"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["from langchain_openai import OpenAIEmbeddings\n","embd = OpenAIEmbeddings()\n","query_result = embd.embed_query(question)\n","document_result = embd.embed_query(document)\n","len(query_result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HmFhsCKfz7Ym","executionInfo":{"status":"ok","timestamp":1707498035896,"user_tz":-60,"elapsed":1280,"user":{"displayName":"Jean-Christophe Bouquin","userId":"01239462757709491259"}},"outputId":"e8f0714d-0f6c-4731-c2f0-0a18f46cb2e7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1536"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["import numpy as np\n","\n","def cosine_similarity(vec1, vec2):\n","    dot_product = np.dot(vec1, vec2)\n","    norm_vec1 = np.linalg.norm(vec1)\n","    norm_vec2 = np.linalg.norm(vec2)\n","    return dot_product / (norm_vec1 * norm_vec2)\n","\n","similarity = cosine_similarity(query_result, document_result)\n","print(\"Cosine Similarity:\", similarity)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yhq9OcCJz7bp","executionInfo":{"status":"ok","timestamp":1707498046572,"user_tz":-60,"elapsed":392,"user":{"displayName":"Jean-Christophe Bouquin","userId":"01239462757709491259"}},"outputId":"c7a0d0c5-ba6b-4f89-b2c0-e2147f82868a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cosine Similarity: 0.8807044730847653\n"]}]},{"cell_type":"code","source":["#### INDEXING ####\n","\n","# Load blog\n","import bs4\n","from langchain_community.document_loaders import WebBaseLoader\n","loader = WebBaseLoader(\n","    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n","    bs_kwargs=dict(\n","        parse_only=bs4.SoupStrainer(\n","            class_=(\"post-content\", \"post-title\", \"post-header\")\n","        )\n","    ),\n",")\n","blog_docs = loader.load()"],"metadata":{"id":"EtcuTeJJz7fC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n","    chunk_size=300,\n","    chunk_overlap=50)\n","\n","# Make splits\n","splits = text_splitter.split_documents(blog_docs)"],"metadata":{"id":"WeNzW2gO0L6P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Index\n","from langchain_openai import OpenAIEmbeddings\n","from langchain_community.vectorstores import Chroma\n","vectorstore = Chroma.from_documents(documents=splits,\n","                                    embedding=OpenAIEmbeddings())\n","\n","retriever = vectorstore.as_retriever()"],"metadata":{"id":"9kM89bHS0L93"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Index\n","from langchain_openai import OpenAIEmbeddings\n","from langchain_community.vectorstores import Chroma\n","vectorstore = Chroma.from_documents(documents=splits,\n","                                    embedding=OpenAIEmbeddings())\n","\n","\n","retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"],"metadata":{"id":"L3-gc0a00Rml"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["docs = retriever.get_relevant_documents(\"What is Task Decomposition?\")"],"metadata":{"id":"r3m-oDV40MBY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zFrKWXp10MFV","executionInfo":{"status":"ok","timestamp":1707498159275,"user_tz":-60,"elapsed":297,"user":{"displayName":"Jean-Christophe Bouquin","userId":"01239462757709491259"}},"outputId":"8fb09fe2-1643-4846-b241-44a2fc5ae141"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","from langchain.prompts import ChatPromptTemplate\n","\n","# Prompt\n","template = \"\"\"Answer the question based only on the following context:\n","{context}\n","\n","Question: {question}\n","\"\"\"\n","\n","prompt = ChatPromptTemplate.from_template(template)\n","prompt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ezx0e3V10mHV","executionInfo":{"status":"ok","timestamp":1707498171767,"user_tz":-60,"elapsed":290,"user":{"displayName":"Jean-Christophe Bouquin","userId":"01239462757709491259"}},"outputId":"959b59f2-c258-47c9-d079-fd8816ab71c7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'))])"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["# LLM\n","llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"],"metadata":{"id":"jD9paqmj0ni2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Chain\n","chain = prompt | llm"],"metadata":{"id":"U3OVIRkW0nmZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Run\n","chain.invoke({\"context\":docs,\"question\":\"What is Task Decomposition?\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b75ikTyp0t8W","executionInfo":{"status":"ok","timestamp":1707498214588,"user_tz":-60,"elapsed":2943,"user":{"displayName":"Jean-Christophe Bouquin","userId":"01239462757709491259"}},"outputId":"f0501124-b843-4763-8a5e-775f5a2cffd1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It involves transforming big tasks into multiple manageable tasks, allowing an agent to plan ahead and utilize more computation to effectively complete the task.')"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["from langchain import hub\n","prompt_hub_rag = hub.pull(\"rlm/rag-prompt\")"],"metadata":{"id":"Q-0097Oc0uAC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt_hub_rag"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yDsP6EZg0uIx","executionInfo":{"status":"ok","timestamp":1707498229092,"user_tz":-60,"elapsed":2,"user":{"displayName":"Jean-Christophe Bouquin","userId":"01239462757709491259"}},"outputId":"d438e5dc-956e-4734-fd28-0cd051b6d5a7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","\n","rag_chain = (\n","    {\"context\": retriever, \"question\": RunnablePassthrough()}\n","    | prompt\n","    | llm\n","    | StrOutputParser()\n",")\n","\n","rag_chain.invoke(\"What is Task Decomposition?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"tEZV-NtE01uV","executionInfo":{"status":"ok","timestamp":1707498250950,"user_tz":-60,"elapsed":6037,"user":{"displayName":"Jean-Christophe Bouquin","userId":"01239462757709491259"}},"outputId":"9bb721b8-3384-4324-c337-0dec73ba896f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It involves transforming big tasks into multiple manageable tasks, allowing an agent to plan ahead and utilize more computation to effectively complete the task. Task decomposition can be achieved through various methods, such as using prompting techniques like Chain of Thought (CoT) or Tree of Thoughts, task-specific instructions, or human inputs.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":[],"metadata":{"id":"UK9c-QSB010s"},"execution_count":null,"outputs":[]}]}