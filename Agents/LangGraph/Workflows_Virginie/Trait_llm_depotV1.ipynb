{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db916746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Classificateur médical avec analyse sémantique améliorée\n",
    "Format simple - sans classes ni dépendances externes\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Configuration\n",
    "BASE_DIR = os.path.abspath(r\"C:\\Users\\kosmo\\pycode\\Iqvia_process\")\n",
    "CHEMIN_SOURCE = os.path.join(BASE_DIR, \"ProcessEx\")\n",
    "CHEMIN_DEPOTS = os.path.join(BASE_DIR, \"Depots\")\n",
    "\n",
    "# Initialiser le LLM\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not api_key:\n",
    "    print(\"⚠️ ATTENTION: Clé API non trouvée dans les variables d'environnement\")\n",
    "    api_key = \"votre_clé_api_openai_ici\"  # À remplacer par votre vraie clé\n",
    "\n",
    "llm = None\n",
    "if api_key != \"votre_clé_api_openai_ici\":\n",
    "    try:\n",
    "        llm = ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0.1,\n",
    "            api_key=api_key\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur initialisation LLM: {e}\")\n",
    "        llm = None\n",
    "\n",
    "def detection_mots_cles_medicaux(titre: str) -> dict:\n",
    "    \"\"\"Détection fallback par mots-clés enrichie avec clients et bases de données\"\"\"\n",
    "    titre_lower = titre.lower()\n",
    "    \n",
    "    # Dictionnaire des clients pharmaceutiques\n",
    "    clients_pharma = {\n",
    "        'iqvia': 'iqvia', 'abbvie': 'abbvie', 'sanofi': 'sanofi', 'pfizer': 'pfizer',\n",
    "        'novartis': 'novartis', 'roche': 'roche', 'merck': 'merck', 'gsk': 'gsk',\n",
    "        'glaxosmithkline': 'gsk', 'astrazeneca': 'astrazeneca', 'bayer': 'bayer',\n",
    "        'boehringer': 'boehringer', 'eli lilly': 'lilly', 'lilly': 'lilly',\n",
    "        'johnson': 'jnj', 'j&j': 'jnj', 'bristol': 'bms', 'takeda': 'takeda',\n",
    "        'amgen': 'amgen', 'gilead': 'gilead', 'biogen': 'biogen', 'celgene': 'celgene'\n",
    "    }\n",
    "    \n",
    "    # Dictionnaire des bases de données IQVIA\n",
    "    bases_donnees = {\n",
    "        'xponent': 'xponent', 'channel dynamics': 'channel_dynamics', 'channel': 'channel_dynamics',\n",
    "        'bilan marché': 'bilan_marche', 'bilan': 'bilan_marche', 'marche': 'bilan_marche',\n",
    "        'midas': 'midas', 'pharmetrics': 'pharmetrics', 'longitudinal': 'longitudinal',\n",
    "        'plantrak': 'plantrak', 'hospital': 'hospital_data', 'retail': 'retail_data',\n",
    "        'oncology': 'oncology_monitor', 'monitor': 'monitor', 'therapy': 'therapy_monitor',\n",
    "        'payor': 'payor_data', 'claims': 'claims_data', 'ema': 'ema_data',\n",
    "        'sales force': 'sales_force', 'salesforce': 'sales_force'\n",
    "    }\n",
    "    \n",
    "    # Dictionnaire enrichi avec contextes sémantiques (pathologies)\n",
    "    termes_contextes = {\n",
    "        # Pathologies oncologiques\n",
    "        'cancer': ('oncologie', 'clinique'), 'tumeur': ('oncologie', 'clinique'), \n",
    "        'oncologie': ('oncologie', 'clinique'), 'prostate': ('oncologie', 'clinique'),\n",
    "        'sein': ('oncologie', 'clinique'), 'poumon': ('oncologie', 'clinique'),\n",
    "        'métastase': ('oncologie', 'clinique'), 'chimiothérapie': ('oncologie', 'traitement'),\n",
    "        \n",
    "        # Pathologies endocriniennes\n",
    "        'diabète': ('diabetologie', 'clinique'), 'diabete': ('diabetologie', 'clinique'), \n",
    "        'glycémie': ('diabetologie', 'clinique'), 'insuline': ('diabetologie', 'traitement'),\n",
    "        'hba1c': ('diabetologie', 'diagnostic'),\n",
    "        \n",
    "        # Pathologies cardiovasculaires\n",
    "        'cardiaque': ('cardiologie', 'clinique'), 'coeur': ('cardiologie', 'clinique'), \n",
    "        'cœur': ('cardiologie', 'clinique'), 'hypertension': ('cardiologie', 'clinique'),\n",
    "        'infarctus': ('cardiologie', 'urgence'), 'arythmie': ('cardiologie', 'clinique'),\n",
    "        \n",
    "        # Pathologies neurologiques\n",
    "        'alzheimer': ('neurologie', 'clinique'), 'parkinson': ('neurologie', 'clinique'), \n",
    "        'neurologie': ('neurologie', 'clinique'), 'épilepsie': ('neurologie', 'clinique'),\n",
    "        'avc': ('neurologie', 'urgence'), 'sclérose': ('neurologie', 'clinique'),\n",
    "        \n",
    "        # Pathologies respiratoires\n",
    "        'asthme': ('pneumologie', 'clinique'), 'pneumonie': ('pneumologie', 'urgence'), \n",
    "        'bronchite': ('pneumologie', 'clinique'), 'bpco': ('pneumologie', 'clinique'),\n",
    "        \n",
    "        # Pathologies dermatologiques\n",
    "        'eczéma': ('dermatologie', 'clinique'), 'eczema': ('dermatologie', 'clinique'), \n",
    "        'psoriasis': ('dermatologie', 'clinique'), 'dermatite': ('dermatologie', 'clinique'),\n",
    "        \n",
    "        # Contextes non-pathologiques\n",
    "        'formation': ('formation', 'commercial'), 'training': ('formation', 'commercial'),\n",
    "        'vente': ('commercial', 'business'), 'marketing': ('commercial', 'business'),\n",
    "        'étude': ('recherche', 'scientifique'), 'essai': ('recherche', 'scientifique'),\n",
    "        'protocole': ('recherche', 'scientifique'), 'phase': ('recherche', 'scientifique'),\n",
    "        'réglementation': ('reglementaire', 'administratif'), 'autorisation': ('reglementaire', 'administratif'),\n",
    "        'surveillance': ('pharmacovigilance', 'securite'), 'observance': ('pharmacovigilance', 'therapeutique')\n",
    "    }\n",
    "    \n",
    "    # Détection des clients\n",
    "    clients_detectes = []\n",
    "    for client_terme, client_nom in clients_pharma.items():\n",
    "        if client_terme in titre_lower:\n",
    "            clients_detectes.append(client_nom)\n",
    "    \n",
    "    # Détection des bases de données\n",
    "    bases_detectees = []\n",
    "    for base_terme, base_nom in bases_donnees.items():\n",
    "        if base_terme in titre_lower:\n",
    "            bases_detectees.append(base_nom)\n",
    "    \n",
    "    # Détection des termes médicaux/contextuels\n",
    "    termes_detectes = []\n",
    "    contextes_detectes = []\n",
    "    for terme, (domaine, contexte) in termes_contextes.items():\n",
    "        if terme in titre_lower:\n",
    "            termes_detectes.append(terme)\n",
    "            contextes_detectes.append((domaine, contexte))\n",
    "    \n",
    "    # Logique de classification prioritaire\n",
    "    if clients_detectes:\n",
    "        # Si client détecté, c'est prioritaire\n",
    "        client_principal = clients_detectes[0]\n",
    "        return {\n",
    "            \"contient_maladie\": False,\n",
    "            \"maladies_detectees\": termes_detectes,\n",
    "            \"categorie_medicale\": \"client\",\n",
    "            \"contexte_principal\": \"client\",\n",
    "            \"client_detecte\": client_principal,\n",
    "            \"clients_detectes\": clients_detectes,\n",
    "            \"bases_detectees\": bases_detectees,\n",
    "            \"confiance\": \"haute\" if len(clients_detectes) > 1 else \"moyenne\",\n",
    "            \"titre_normalise\": titre,\n",
    "            \"score_semantique\": 0.8\n",
    "        }\n",
    "    \n",
    "    elif bases_detectees:\n",
    "        # Si base de données détectée\n",
    "        base_principale = bases_detectees[0]\n",
    "        return {\n",
    "            \"contient_maladie\": False,\n",
    "            \"maladies_detectees\": termes_detectes,\n",
    "            \"categorie_medicale\": \"base_donnees\",\n",
    "            \"contexte_principal\": \"base_donnees\",\n",
    "            \"base_detectee\": base_principale,\n",
    "            \"clients_detectes\": clients_detectes,\n",
    "            \"bases_detectees\": bases_detectees,\n",
    "            \"confiance\": \"haute\" if len(bases_detectees) > 1 else \"moyenne\",\n",
    "            \"titre_normalise\": titre,\n",
    "            \"score_semantique\": 0.8\n",
    "        }\n",
    "    \n",
    "    elif termes_detectes:\n",
    "        # Classification médicale/contextuelle classique\n",
    "        domaine_principal, contexte_principal = contextes_detectes[0]\n",
    "        confiance = \"haute\" if len(termes_detectes) > 1 else \"moyenne\"\n",
    "        \n",
    "        # Déterminer si c'est médical\n",
    "        domaines_medicaux = ['oncologie', 'diabetologie', 'cardiologie', 'neurologie', 'pneumologie', 'dermatologie']\n",
    "        est_medical = domaine_principal in domaines_medicaux\n",
    "        \n",
    "        return {\n",
    "            \"contient_maladie\": est_medical,\n",
    "            \"maladies_detectees\": termes_detectes,\n",
    "            \"categorie_medicale\": domaine_principal,\n",
    "            \"contexte_principal\": contexte_principal,\n",
    "            \"clients_detectes\": clients_detectes,\n",
    "            \"bases_detectees\": bases_detectees,\n",
    "            \"confiance\": confiance,\n",
    "            \"titre_normalise\": titre,\n",
    "            \"score_semantique\": 0.7 if confiance == \"haute\" else 0.5\n",
    "        }\n",
    "    \n",
    "    # Aucune détection\n",
    "    return {\n",
    "        \"contient_maladie\": False,\n",
    "        \"maladies_detectees\": [],\n",
    "        \"categorie_medicale\": \"aucune\",\n",
    "        \"contexte_principal\": \"autre\",\n",
    "        \"clients_detectes\": clients_detectes,\n",
    "        \"bases_detectees\": bases_detectees,\n",
    "        \"confiance\": \"faible\",\n",
    "        \"titre_normalise\": titre,\n",
    "        \"score_semantique\": 0.2\n",
    "    }\n",
    "\n",
    "def analyser_titre_avec_llm_semantique(titre: str) -> dict:\n",
    "    \"\"\"Analyse sémantique avancée avec LLM - prompt enrichi\"\"\"\n",
    "    if not llm:\n",
    "        print(f\"❌ LLM non disponible, utilisation fallback pour '{titre[:30]}...'\")\n",
    "        return detection_mots_cles_medicaux(titre)\n",
    "    \n",
    "    try:\n",
    "        analysis_prompt_template = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"Tu es un expert en analyse sémantique médicale et business pharmaceutique. \n",
    "            Ton rôle est de comprendre le CONTEXTE et l'INTENTION derrière chaque titre, pas seulement chercher des mots-clés.\n",
    "            \n",
    "            Tu maîtrises ces domaines :\n",
    "            - CLINIQUE : pathologies, diagnostics, traitements, soins patients\n",
    "            - RECHERCHE : études, essais, protocoles, développement\n",
    "            - COMMERCIAL : formation équipes, marketing, ventes, business\n",
    "            - RÉGLEMENTAIRE : autorisations, compliance, pharmacovigilance\n",
    "            - FORMATION : training, éducation, guidelines\n",
    "            \"\"\"),\n",
    "            (\"human\", \"\"\"\n",
    "Analyse sémantique COMPLÈTE du titre : \"{titre}\"\n",
    "\n",
    "Ne te contente pas de chercher des mots-clés ! Analyse le CONTEXTE :\n",
    "\n",
    "EXEMPLES D'ANALYSE SÉMANTIQUE :\n",
    "- \"Formation équipe vente oncologie Q1 2025\" → CONTEXTE=commercial, DOMAINE=oncologie, TYPE=formation\n",
    "- \"Suivi patients diabète hôpital\" → CONTEXTE=clinique, DOMAINE=diabetologie, TYPE=suivi_medical  \n",
    "- \"Protocole étude phase III cancer poumon\" → CONTEXTE=recherche, DOMAINE=oncologie, TYPE=essai_clinique\n",
    "- \"Rapport Xponent Sanofi Q4 2024\" → CONTEXTE=client, CLIENT=sanofi, BASE=xponent\n",
    "- \"Analyse Channel Dynamics ABBVIE\" → CONTEXTE=base_donnees, CLIENT=abbvie, BASE=channel_dynamics\n",
    "- \"Présentation IQVIA Bilan Marché\" → CONTEXTE=client, CLIENT=iqvia, BASE=bilan_marche\n",
    "\n",
    "MISSION : Identifie le CONTEXTE principal, détecte les CLIENTS et BASES DE DONNÉES.\n",
    "\n",
    "**CLIENTS PRIORITAIRES :** IQVIA, ABBVIE, Sanofi, Pfizer, Novartis, Roche, Merck, GSK, AstraZeneca, Bayer, Boehringer, Lilly, J&J, Bristol, Takeda, Amgen, Gilead, Biogen\n",
    "\n",
    "**BASES DE DONNÉES :** Xponent, Channel Dynamics, Bilan Marché, MIDAS, Pharmetrics, Longitudinal, PlanTrak, Hospital, Retail, Oncology Monitor, Therapy Monitor, Claims Data, Sales Force\n",
    "\n",
    "Réponds UNIQUEMENT en JSON valide :\n",
    "{{\n",
    "    \"contexte_principal\": \"clinique/recherche/commercial/reglementaire/formation/client/base_donnees/autre\",\n",
    "    \"domaine_medical\": \"oncologie/cardiologie/neurologie/diabetologie/pneumologie/dermatologie/pharmacovigilance/business/client/base_donnees/aucun\",\n",
    "    \"type_document\": \"formation/etude/rapport/suivi/presentation/protocole/guide/autre\",\n",
    "    \"population_cible\": \"patients/professionnels/equipes_vente/chercheurs/regulateurs/clients/mixte\",\n",
    "    \"maladies_detectees\": [\"terme1\", \"terme2\"],\n",
    "    \"clients_detectes\": [\"client1\", \"client2\"],\n",
    "    \"bases_detectees\": [\"base1\", \"base2\"],\n",
    "    \"client_principal\": \"nom_client_principal_ou_null\",\n",
    "    \"base_principale\": \"nom_base_principale_ou_null\",\n",
    "    \"contient_maladie\": true,\n",
    "    \"confiance\": \"haute/moyenne/faible\",\n",
    "    \"score_semantique\": 0.8,\n",
    "    \"justification\": \"Explication courte du contexte détecté\",\n",
    "    \"titre_normalise\": \"{titre}\"\n",
    "}}\n",
    "            \"\"\")\n",
    "        ])\n",
    "        \n",
    "        prompt_value = analysis_prompt_template.invoke({\"titre\": titre})\n",
    "        response = llm.invoke(prompt_value.to_messages())\n",
    "        content = response.content.strip()\n",
    "        \n",
    "        # Nettoyer le JSON\n",
    "        if \"```json\" in content:\n",
    "            content = content.split(\"```json\")[1].split(\"```\")[0]\n",
    "        elif \"```\" in content:\n",
    "            content = content.split(\"```\")[1].split(\"```\")[0]\n",
    "        \n",
    "        resultat = json.loads(content.strip())\n",
    "        \n",
    "        # Validation et enrichissement\n",
    "        if \"categorie_medicale\" not in resultat:\n",
    "            resultat[\"categorie_medicale\"] = resultat.get(\"domaine_medical\", \"aucune\")\n",
    "        \n",
    "        # Assurer la cohérence clients/bases de données\n",
    "        if not resultat.get(\"clients_detectes\"):\n",
    "            resultat[\"clients_detectes\"] = []\n",
    "        if not resultat.get(\"bases_detectees\"):\n",
    "            resultat[\"bases_detectees\"] = []\n",
    "        \n",
    "        # Déterminer le client/base principal\n",
    "        if resultat.get(\"clients_detectes\") and not resultat.get(\"client_principal\"):\n",
    "            resultat[\"client_principal\"] = resultat[\"clients_detectes\"][0]\n",
    "        if resultat.get(\"bases_detectees\") and not resultat.get(\"base_principale\"):\n",
    "            resultat[\"base_principale\"] = resultat[\"bases_detectees\"][0]\n",
    "        \n",
    "        # Ajuster le contexte si client ou base détecté\n",
    "        if resultat.get(\"client_principal\"):\n",
    "            resultat[\"contexte_principal\"] = \"client\"\n",
    "            resultat[\"categorie_medicale\"] = \"client\"\n",
    "        elif resultat.get(\"base_principale\"):\n",
    "            resultat[\"contexte_principal\"] = \"base_donnees\"\n",
    "            resultat[\"categorie_medicale\"] = \"base_donnees\"\n",
    "        \n",
    "        # Calculer score sémantique si absent\n",
    "        if \"score_semantique\" not in resultat or not isinstance(resultat[\"score_semantique\"], (int, float)):\n",
    "            contexte = resultat.get(\"contexte_principal\", \"autre\")\n",
    "            domaine = resultat.get(\"domaine_medical\", \"aucun\")\n",
    "            \n",
    "            score = 0.3  # Base\n",
    "            if contexte != \"autre\": score += 0.2\n",
    "            if domaine != \"aucun\": score += 0.2\n",
    "            if len(resultat.get(\"maladies_detectees\", [])) > 0: score += 0.2\n",
    "            if resultat.get(\"confiance\") == \"haute\": score += 0.1\n",
    "            \n",
    "            resultat[\"score_semantique\"] = min(0.95, score)\n",
    "        \n",
    "        return resultat\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur LLM sémantique pour '{titre[:30]}...': {e}\")\n",
    "        return detection_mots_cles_medicaux(titre)\n",
    "\n",
    "def rechercher_fichiers_filtres():\n",
    "    \"\"\"Phase 1: Recherche et filtre les fichiers par extension, date et taille\"\"\"\n",
    "    print(\"🔍 PHASE 1: Recherche et filtrage des fichiers...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    if not os.path.exists(CHEMIN_SOURCE):\n",
    "        print(f\"❌ Erreur: {CHEMIN_SOURCE} n'existe pas\")\n",
    "        return []\n",
    "    \n",
    "    # Définition des filtres\n",
    "    DATE_LIMITE = datetime.now() - timedelta(days=365)  # Fichiers de moins d'un an\n",
    "    TAILLE_MIN_OCTETS = 1 * 1024         # 1 KB minimum (pour éviter fichiers vides)\n",
    "    TAILLE_MAX_OCTETS = 50 * 1024 * 1024  # 50 MB maximum\n",
    "    TAILLE_CLASSIFICATION = 3000 * 1024   # 2 MB - Seuil pour classification sémantique\n",
    "    \n",
    "    print(f\"📅 Filtre date: fichiers modifiés après le {DATE_LIMITE.strftime('%d/%m/%Y')}\")\n",
    "    print(f\"📏 Filtre taille: entre {TAILLE_MIN_OCTETS//1024} KB et {TAILLE_MAX_OCTETS//1024//1024} MB\")\n",
    "    print(f\"🧠 Classification sémantique: fichiers > {TAILLE_CLASSIFICATION//1024} KB seulement\")\n",
    "    print(f\"📁 Fichiers ≤ {TAILLE_CLASSIFICATION//1024} KB → dossier AUTRES automatiquement\")\n",
    "    print()\n",
    "    \n",
    "    fichiers_trouves = []\n",
    "    dossiers_stats = {}\n",
    "    stats_filtrage = {\n",
    "        'total_examines': 0,\n",
    "        'rejetes_extension': 0,\n",
    "        'rejetes_taille': 0, \n",
    "        'rejetes_date': 0,\n",
    "        'rejetes_acces': 0,\n",
    "        'acceptes': 0\n",
    "    }\n",
    "    \n",
    "    for item in os.listdir(CHEMIN_SOURCE):\n",
    "        item_path = os.path.join(CHEMIN_SOURCE, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            print(f\"📂 Analyse du dossier: {item}\")\n",
    "            count_dossier = 0\n",
    "            \n",
    "            for root, dirs, files in os.walk(item_path):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    stats_filtrage['total_examines'] += 1\n",
    "                    \n",
    "                    # Vérification de l'extension\n",
    "                    if not file.lower().endswith(('.ppt', '.pptx', '.pdf')):\n",
    "                        stats_filtrage['rejetes_extension'] += 1\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        # Récupération des métadonnées\n",
    "                        taille = os.path.getsize(file_path)\n",
    "                        date_mod = datetime.fromtimestamp(os.path.getmtime(file_path))\n",
    "                        \n",
    "                        # Filtrage par taille\n",
    "                        if not (TAILLE_MIN_OCTETS <= taille <= TAILLE_MAX_OCTETS):\n",
    "                            stats_filtrage['rejetes_taille'] += 1\n",
    "                            continue\n",
    "                        \n",
    "                        # Filtrage par date  \n",
    "                        if date_mod < DATE_LIMITE:\n",
    "                            stats_filtrage['rejetes_date'] += 1\n",
    "                            continue\n",
    "                        \n",
    "                        # Fichier accepté\n",
    "                        fichier_info = {\n",
    "                            'chemin': file_path,\n",
    "                            'nom': file,\n",
    "                            'taille': taille,\n",
    "                            'date_modification': date_mod,\n",
    "                            'taille_mb': round(taille / (1024*1024), 2),\n",
    "                            'taille_kb': round(taille / 1024, 1),\n",
    "                            'age_jours': (datetime.now() - date_mod).days,\n",
    "                            'classification_semantique': taille >= TAILLE_CLASSIFICATION  # True si > 2MB\n",
    "                        }\n",
    "                        \n",
    "                        fichiers_trouves.append(fichier_info)\n",
    "                        count_dossier += 1\n",
    "                        stats_filtrage['acceptes'] += 1\n",
    "                        \n",
    "                    except (OSError, PermissionError) as e:\n",
    "                        stats_filtrage['rejetes_acces'] += 1\n",
    "                        print(f\"   ⚠️ Ignoré (accès refusé): {os.path.basename(file_path)}\")\n",
    "            \n",
    "            dossiers_stats[item] = count_dossier\n",
    "            print(f\"   ✅ {count_dossier} fichier(s) retenu(s) après filtrage\")\n",
    "    \n",
    "    # Affichage des statistiques détaillées\n",
    "    print(f\"\\n📊 STATISTIQUES DE FILTRAGE:\")\n",
    "    print(f\"   • Fichiers examinés: {stats_filtrage['total_examines']}\")\n",
    "    print(f\"   • ❌ Rejetés - extension: {stats_filtrage['rejetes_extension']}\")\n",
    "    print(f\"   • ❌ Rejetés - taille: {stats_filtrage['rejetes_taille']}\")\n",
    "    print(f\"   • ❌ Rejetés - date: {stats_filtrage['rejetes_date']}\")\n",
    "    print(f\"   • ❌ Rejetés - accès: {stats_filtrage['rejetes_acces']}\")\n",
    "    print(f\"   • ✅ RETENUS: {stats_filtrage['acceptes']}\")\n",
    "    \n",
    "    print(f\"\\n📁 RÉPARTITION PAR DOSSIER:\")\n",
    "    for dossier, count in dossiers_stats.items():\n",
    "        print(f\"   • {dossier}: {count} fichier(s)\")\n",
    "    \n",
    "    return fichiers_trouves\n",
    "\n",
    "def trier_fichiers_par_criteres(fichiers_info):\n",
    "    \"\"\"Trie les fichiers par différents critères et affiche des statistiques\"\"\"\n",
    "    print(f\"\\n📈 PHASE 1.5: Analyse et tri des fichiers retenus...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    if not fichiers_info:\n",
    "        print(\"❌ Aucun fichier à analyser\")\n",
    "        return fichiers_info\n",
    "    \n",
    "    # Tri par taille (décroissant)\n",
    "    fichiers_par_taille = sorted(fichiers_info, key=lambda x: x['taille'], reverse=True)\n",
    "    print(f\"📏 Top 5 fichiers les plus volumineux:\")\n",
    "    for i, f in enumerate(fichiers_par_taille[:5], 1):\n",
    "        print(f\"   {i}. {f['nom'][:50]}... ({f['taille_mb']} MB)\")\n",
    "    \n",
    "    # Tri par date (plus récent d'abord) \n",
    "    fichiers_par_date = sorted(fichiers_info, key=lambda x: x['date_modification'], reverse=True)\n",
    "    print(f\"\\n📅 Top 5 fichiers les plus récents:\")\n",
    "    for i, f in enumerate(fichiers_par_date[:5], 1):\n",
    "        date_str = f['date_modification'].strftime('%d/%m/%Y')\n",
    "        print(f\"   {i}. {f['nom'][:50]}... ({date_str}, {f['age_jours']} jours)\")\n",
    "    \n",
    "    # Statistiques de répartition\n",
    "    tailles = [f['taille_mb'] for f in fichiers_info]\n",
    "    ages = [f['age_jours'] for f in fichiers_info]\n",
    "    \n",
    "    print(f\"\\n📊 STATISTIQUES DESCRIPTIVES:\")\n",
    "    print(f\"   Taille - Moyenne: {sum(tailles)/len(tailles):.1f} MB, Max: {max(tailles):.1f} MB, Min: {min(tailles):.1f} MB\")\n",
    "    print(f\"   Âge - Moyenne: {sum(ages)//len(ages)} jours, Max: {max(ages)} jours, Min: {min(ages)} jours\")\n",
    "    \n",
    "    # Répartition par tranches de taille et éligibilité classification\n",
    "    tranches_taille = {'< 1MB': 0, '1-2MB': 0, '2-5MB': 0, '5-20MB': 0, '> 20MB': 0}\n",
    "    eligibles_classification = 0\n",
    "    \n",
    "    for f in fichiers_info:\n",
    "        mb = f['taille_mb']\n",
    "        if mb < 1: tranches_taille['< 1MB'] += 1\n",
    "        elif mb < 2: tranches_taille['1-2MB'] += 1  \n",
    "        elif mb < 5: tranches_taille['2-5MB'] += 1\n",
    "        elif mb < 20: tranches_taille['5-20MB'] += 1\n",
    "        else: tranches_taille['> 20MB'] += 1\n",
    "        \n",
    "        if f['classification_semantique']:\n",
    "            eligibles_classification += 1\n",
    "    \n",
    "    print(f\"\\n📏 RÉPARTITION PAR TAILLE:\")\n",
    "    for tranche, count in tranches_taille.items():\n",
    "        pct = (count / len(fichiers_info) * 100) if fichiers_info else 0\n",
    "        print(f\"   • {tranche}: {count} fichiers ({pct:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n🧠 ÉLIGIBILITÉ CLASSIFICATION SÉMANTIQUE:\")\n",
    "    print(f\"   • Fichiers > 2MB (analysés): {eligibles_classification}\")\n",
    "    print(f\"   • Fichiers ≤ 2MB (→ AUTRES): {len(fichiers_info) - eligibles_classification}\")\n",
    "    \n",
    "    return fichiers_info\n",
    "\n",
    "def copier_fichiers(fichiers_info):\n",
    "    \"\"\"Phase 2: Copie vers Depots avec informations enrichies\"\"\"\n",
    "    print(f\"\\n📋 PHASE 2: Copie vers {CHEMIN_DEPOTS}...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Créer le dossier Depots\n",
    "    os.makedirs(CHEMIN_DEPOTS, exist_ok=True)\n",
    "    print(f\"✅ Dossier Depots prêt\")\n",
    "    \n",
    "    copied = 0\n",
    "    taille_totale = 0\n",
    "    \n",
    "    for fichier_info in fichiers_info:\n",
    "        file_path = fichier_info['chemin']\n",
    "        file_name = fichier_info['nom']\n",
    "        destination = os.path.join(CHEMIN_DEPOTS, file_name)\n",
    "        \n",
    "        try:\n",
    "            shutil.copy(file_path, destination)\n",
    "            copied += 1\n",
    "            taille_totale += fichier_info['taille']\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erreur copie {file_name}: {e}\")\n",
    "    \n",
    "    print(f\"✅ {copied}/{len(fichiers_info)} fichiers copiés\")\n",
    "    print(f\"📦 Taille totale copiée: {taille_totale/(1024*1024):.1f} MB\")\n",
    "    return copied\n",
    "\n",
    "def analyser_et_classer_semantique():\n",
    "    \"\"\"Phase 3: Analyse sémantique LLM et classification avancée (> 2MB seulement)\"\"\"\n",
    "    print(f\"\\n🧠 PHASE 3: Analyse sémantique et classification...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Mode LLM ou fallback\n",
    "    use_llm = llm is not None\n",
    "    print(f\"Mode: {'🔥 LLM Sémantique' if use_llm else '🔧 Mots-clés enrichis'}\")\n",
    "    \n",
    "    fichiers_a_analyser = [f for f in os.listdir(CHEMIN_DEPOTS) \n",
    "                          if f.lower().endswith(('.ppt', '.pptx', '.pdf'))]\n",
    "    \n",
    "    # Récupérer les infos de taille des fichiers copiés\n",
    "    fichiers_avec_taille = []\n",
    "    for file_name in fichiers_a_analyser:\n",
    "        file_path = os.path.join(CHEMIN_DEPOTS, file_name)\n",
    "        try:\n",
    "            taille = os.path.getsize(file_path)\n",
    "            fichiers_avec_taille.append({\n",
    "                'nom': file_name,\n",
    "                'taille': taille,\n",
    "                'taille_mb': round(taille / (1024*1024), 2),\n",
    "                'taille_kb': round(taille / 1024, 1),\n",
    "                'eligible_classification': taille >= (3000 * 1024)  # > 2MB\n",
    "            })\n",
    "        except:\n",
    "            # Si erreur, on considère comme petit fichier\n",
    "            fichiers_avec_taille.append({\n",
    "                'nom': file_name,\n",
    "                'taille': 0,\n",
    "                'taille_mb': 0,\n",
    "                'taille_kb': 0,\n",
    "                'eligible_classification': False\n",
    "            })\n",
    "    \n",
    "    analyses = []\n",
    "    medicaux = 0\n",
    "    contextes_stats = {}\n",
    "    petits_fichiers = 0\n",
    "    \n",
    "    print(f\"📊 Répartition:\")\n",
    "    eligibles = sum(1 for f in fichiers_avec_taille if f['eligible_classification'])\n",
    "    print(f\"   • Fichiers > 2MB (analyse sémantique): {eligibles}\")\n",
    "    print(f\"   • Fichiers ≤ 2MB (→ AUTRES direct): {len(fichiers_avec_taille) - eligibles}\")\n",
    "    print()\n",
    "    \n",
    "    for i, fichier_info in enumerate(fichiers_avec_taille, 1):\n",
    "        file_name = fichier_info['nom']\n",
    "        titre = os.path.splitext(file_name)[0]\n",
    "        taille_str = f\"{fichier_info['taille_kb']} KB\" if fichier_info['taille_mb'] < 1 else f\"{fichier_info['taille_mb']} MB\"\n",
    "        \n",
    "        print(f\"[{i:2d}/{len(fichiers_avec_taille)}] {file_name[:40]}... ({taille_str})\")\n",
    "        \n",
    "        # Vérifier si éligible à la classification sémantique\n",
    "        if not fichier_info['eligible_classification']:\n",
    "            # Fichier ≤ 2MB → Dossier AUTRES automatiquement\n",
    "            analyse = {\n",
    "                'nom_fichier': file_name,\n",
    "                'titre': titre,\n",
    "                'taille_mb': fichier_info['taille_mb'],\n",
    "                'contient_maladie': False,\n",
    "                'categorie_medicale': 'autres',\n",
    "                'contexte_principal': 'autre',\n",
    "                'confiance': 'faible',\n",
    "                'score_semantique': 0.1,\n",
    "                'justification': f'Fichier de petite taille ({taille_str}) → AUTRES automatiquement',\n",
    "                'clients_detectes': [],\n",
    "                'bases_detectees': [],\n",
    "                'maladies_detectees': []\n",
    "            }\n",
    "            analyses.append(analyse)\n",
    "            petits_fichiers += 1\n",
    "            print(f\"   📁 → AUTRES (taille < 2MB)\")\n",
    "            continue\n",
    "        \n",
    "        # Classification sémantique pour fichiers > 2MB\n",
    "        if use_llm:\n",
    "            analyse = analyser_titre_avec_llm_semantique(titre)\n",
    "        else:\n",
    "            analyse = detection_mots_cles_medicaux(titre)\n",
    "        \n",
    "        # Enrichir avec métadonnées\n",
    "        analyse['nom_fichier'] = file_name\n",
    "        analyse['taille_mb'] = fichier_info['taille_mb']\n",
    "        \n",
    "        # Compter et afficher résultats\n",
    "        if analyse.get('contient_maladie', False):\n",
    "            medicaux += 1\n",
    "        \n",
    "        contexte = analyse.get('contexte_principal', 'autre')\n",
    "        contextes_stats[contexte] = contextes_stats.get(contexte, 0) + 1\n",
    "        \n",
    "        # Affichage du résultat\n",
    "        categorie = analyse.get('categorie_medicale', 'aucune')\n",
    "        confiance = analyse.get('confiance', 'inconnue')\n",
    "        score = analyse.get('score_semantique', 0)\n",
    "        \n",
    "        if analyse.get('clients_detectes'):\n",
    "            client_info = f\" [Client: {', '.join(analyse['clients_detectes'])}]\"\n",
    "        elif analyse.get('bases_detectees'):\n",
    "            client_info = f\" [Base: {', '.join(analyse['bases_detectees'])}]\"\n",
    "        else:\n",
    "            client_info = \"\"\n",
    "        \n",
    "        print(f\"   🎯 {categorie.upper()} ({confiance}, score:{score:.2f}){client_info}\")\n",
    "        \n",
    "        if analyse.get('maladies_detectees'):\n",
    "            print(f\"   🏥 Termes: {', '.join(analyse['maladies_detectees'])}\")\n",
    "        \n",
    "        analyses.append(analyse)\n",
    "    \n",
    "    print(f\"\\n📊 RÉSULTATS DE CLASSIFICATION:\")\n",
    "    print(f\"   • Fichiers analysés sémantiquement: {len(analyses) - petits_fichiers}\")\n",
    "    print(f\"   • Fichiers → AUTRES (< 2MB): {petits_fichiers}\")\n",
    "    print(f\"   • Fichiers médicaux détectés: {medicaux}\")\n",
    "    print(f\"   • Score sémantique moyen: {sum(a.get('score_semantique', 0) for a in analyses) / len(analyses):.2f}\")\n",
    "    \n",
    "    print(f\"\\n📈 RÉPARTITION PAR CONTEXTE:\")\n",
    "    for contexte, count in sorted(contextes_stats.items()):\n",
    "        pct = (count / len(analyses) * 100) if analyses else 0\n",
    "        print(f\"   • {contexte}: {count} fichiers ({pct:.1f}%)\")\n",
    "    \n",
    "    return analyses\n",
    "\n",
    "def creer_dossiers_et_deplacer(analyses):\n",
    "    \"\"\"Phase 4: Création des dossiers de classification et déplacement\"\"\"\n",
    "    print(f\"\\n📁 PHASE 4: Création dossiers et déplacement...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Mapping des catégories vers dossiers\n",
    "    mapping_dossiers = {\n",
    "        'oncologie': 'ONCOLOGIE',\n",
    "        'cardiologie': 'CARDIOLOGIE', \n",
    "        'neurologie': 'NEUROLOGIE',\n",
    "        'diabetologie': 'DIABETOLOGIE',\n",
    "        'pneumologie': 'PNEUMOLOGIE',\n",
    "        'dermatologie': 'DERMATOLOGIE',\n",
    "        'pharmacovigilance': 'PHARMACOVIGILANCE',\n",
    "        'formation': 'FORMATION',\n",
    "        'commercial': 'COMMERCIAL',\n",
    "        'recherche': 'RECHERCHE',\n",
    "        'reglementaire': 'REGLEMENTAIRE',\n",
    "        'client': 'CLIENTS',\n",
    "        'base_donnees': 'BASES_DONNEES',\n",
    "        'autres': 'AUTRES',\n",
    "        'aucune': 'AUTRES'\n",
    "    }\n",
    "    \n",
    "    # Créer tous les dossiers nécessaires\n",
    "    dossiers_crees = set()\n",
    "    for categorie in mapping_dossiers.values():\n",
    "        dossier_path = os.path.join(CHEMIN_DEPOTS, categorie)\n",
    "        os.makedirs(dossier_path, exist_ok=True)\n",
    "        dossiers_crees.add(categorie)\n",
    "    \n",
    "    print(f\"✅ {len(dossiers_crees)} dossiers de classification créés\")\n",
    "    \n",
    "    # Déplacer les fichiers\n",
    "    stats_deplacement = {}\n",
    "    erreurs_deplacement = []\n",
    "    \n",
    "    for analyse in analyses:\n",
    "        file_name = analyse['nom_fichier']\n",
    "        categorie = analyse.get('categorie_medicale', 'autres')\n",
    "        dossier_cible = mapping_dossiers.get(categorie, 'AUTRES')\n",
    "        \n",
    "        source_path = os.path.join(CHEMIN_DEPOTS, file_name)\n",
    "        destination_path = os.path.join(CHEMIN_DEPOTS, dossier_cible, file_name)\n",
    "        \n",
    "        try:\n",
    "            if os.path.exists(source_path):\n",
    "                # Gérer les doublons\n",
    "                if os.path.exists(destination_path):\n",
    "                    base, ext = os.path.splitext(file_name)\n",
    "                    counter = 1\n",
    "                    while os.path.exists(destination_path):\n",
    "                        new_name = f\"{base}_({counter}){ext}\"\n",
    "                        destination_path = os.path.join(CHEMIN_DEPOTS, dossier_cible, new_name)\n",
    "                        counter += 1\n",
    "                \n",
    "                shutil.move(source_path, destination_path)\n",
    "                stats_deplacement[dossier_cible] = stats_deplacement.get(dossier_cible, 0) + 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            erreurs_deplacement.append(f\"{file_name}: {str(e)}\")\n",
    "            print(f\"❌ Erreur déplacement {file_name}: {e}\")\n",
    "    \n",
    "    print(f\"\\n📊 RÉSULTATS DE DÉPLACEMENT:\")\n",
    "    total_deplaces = sum(stats_deplacement.values())\n",
    "    for dossier, count in sorted(stats_deplacement.items()):\n",
    "        pct = (count / total_deplaces * 100) if total_deplaces > 0 else 0\n",
    "        print(f\"   • {dossier}: {count} fichiers ({pct:.1f}%)\")\n",
    "    \n",
    "    if erreurs_deplacement:\n",
    "        print(f\"\\n❌ ERREURS DE DÉPLACEMENT ({len(erreurs_deplacement)}):\")\n",
    "        for erreur in erreurs_deplacement[:5]:  # Limiter à 5 erreurs\n",
    "            print(f\"   • {erreur}\")\n",
    "        if len(erreurs_deplacement) > 5:\n",
    "            print(f\"   • ... et {len(erreurs_deplacement) - 5} autres erreurs\")\n",
    "    \n",
    "    return stats_deplacement\n",
    "\n",
    "def generer_rapport_final(analyses, stats_deplacement):\n",
    "    \"\"\"Phase 5: Génération du rapport final détaillé\"\"\"\n",
    "    print(f\"\\n📋 PHASE 5: Génération du rapport final...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Créer le rapport JSON détaillé\n",
    "    rapport = {\n",
    "        'metadata': {\n",
    "            'date_execution': datetime.now().isoformat(),\n",
    "            'version_script': '2.0',\n",
    "            'mode_llm': llm is not None,\n",
    "            'total_fichiers': len(analyses)\n",
    "        },\n",
    "        'statistiques_globales': {\n",
    "            'fichiers_medicaux': sum(1 for a in analyses if a.get('contient_maladie', False)),\n",
    "            'fichiers_clients': sum(1 for a in analyses if a.get('clients_detectes')),\n",
    "            'fichiers_bases_donnees': sum(1 for a in analyses if a.get('bases_detectees')),\n",
    "            'score_semantique_moyen': round(sum(a.get('score_semantique', 0) for a in analyses) / len(analyses), 3) if analyses else 0\n",
    "        },\n",
    "        'repartition_dossiers': stats_deplacement,\n",
    "        'analyses_detaillees': analyses\n",
    "    }\n",
    "    \n",
    "    # Sauvegarder le rapport JSON\n",
    "    rapport_path = os.path.join(CHEMIN_DEPOTS, 'rapport_classification.json')\n",
    "    try:\n",
    "        with open(rapport_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(rapport, f, indent=2, ensure_ascii=False, default=str)\n",
    "        print(f\"✅ Rapport JSON sauvegardé: {rapport_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur sauvegarde rapport: {e}\")\n",
    "    \n",
    "    # Générer rapport texte lisible\n",
    "    rapport_txt_path = os.path.join(CHEMIN_DEPOTS, 'rapport_classification.txt')\n",
    "    try:\n",
    "        with open(rapport_txt_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"=\" * 80 + \"\\n\")\n",
    "            f.write(\"RAPPORT DE CLASSIFICATION MÉDICALE AUTOMATIQUE\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "            \n",
    "            f.write(f\"📅 Date d'exécution: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\\n\")\n",
    "            f.write(f\"🔧 Mode: {'LLM Sémantique' if llm else 'Mots-clés enrichis'}\\n\")\n",
    "            f.write(f\"📁 Dossier source: {CHEMIN_SOURCE}\\n\")\n",
    "            f.write(f\"📁 Dossier dépôt: {CHEMIN_DEPOTS}\\n\\n\")\n",
    "            \n",
    "            f.write(\"📊 STATISTIQUES GLOBALES\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            f.write(f\"Total fichiers traités: {len(analyses)}\\n\")\n",
    "            f.write(f\"Fichiers médicaux: {rapport['statistiques_globales']['fichiers_medicaux']}\\n\")\n",
    "            f.write(f\"Fichiers clients: {rapport['statistiques_globales']['fichiers_clients']}\\n\")\n",
    "            f.write(f\"Fichiers bases données: {rapport['statistiques_globales']['fichiers_bases_donnees']}\\n\")\n",
    "            f.write(f\"Score sémantique moyen: {rapport['statistiques_globales']['score_semantique_moyen']}\\n\\n\")\n",
    "            \n",
    "            f.write(\"📁 RÉPARTITION PAR DOSSIERS\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            for dossier, count in sorted(stats_deplacement.items()):\n",
    "                pct = (count / len(analyses) * 100) if analyses else 0\n",
    "                f.write(f\"{dossier}: {count} fichiers ({pct:.1f}%)\\n\")\n",
    "            \n",
    "            f.write(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "            f.write(\"DÉTAIL DES CLASSIFICATIONS\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "            \n",
    "            # Grouper par dossier\n",
    "            analyses_par_dossier = {}\n",
    "            mapping_dossiers = {\n",
    "                'oncologie': 'ONCOLOGIE', 'cardiologie': 'CARDIOLOGIE', \n",
    "                'neurologie': 'NEUROLOGIE', 'diabetologie': 'DIABETOLOGIE',\n",
    "                'pneumologie': 'PNEUMOLOGIE', 'dermatologie': 'DERMATOLOGIE',\n",
    "                'pharmacovigilance': 'PHARMACOVIGILANCE', 'formation': 'FORMATION',\n",
    "                'commercial': 'COMMERCIAL', 'recherche': 'RECHERCHE',\n",
    "                'reglementaire': 'REGLEMENTAIRE', 'client': 'CLIENTS',\n",
    "                'base_donnees': 'BASES_DONNEES', 'autres': 'AUTRES', 'aucune': 'AUTRES'\n",
    "            }\n",
    "            \n",
    "            for analyse in analyses:\n",
    "                categorie = analyse.get('categorie_medicale', 'autres')\n",
    "                dossier = mapping_dossiers.get(categorie, 'AUTRES')\n",
    "                if dossier not in analyses_par_dossier:\n",
    "                    analyses_par_dossier[dossier] = []\n",
    "                analyses_par_dossier[dossier].append(analyse)\n",
    "            \n",
    "            for dossier, fichiers in sorted(analyses_par_dossier.items()):\n",
    "                f.write(f\"\\n📂 {dossier} ({len(fichiers)} fichiers)\\n\")\n",
    "                f.write(\"-\" * (len(dossier) + 20) + \"\\n\")\n",
    "                \n",
    "                for analyse in sorted(fichiers, key=lambda x: x.get('score_semantique', 0), reverse=True):\n",
    "                    f.write(f\"• {analyse['nom_fichier']}\\n\")\n",
    "                    f.write(f\"  Taille: {analyse.get('taille_mb', 0):.1f} MB\\n\")\n",
    "                    f.write(f\"  Catégorie: {analyse.get('categorie_medicale', 'N/A')}\\n\")\n",
    "                    f.write(f\"  Confiance: {analyse.get('confiance', 'N/A')}\\n\")\n",
    "                    f.write(f\"  Score: {analyse.get('score_semantique', 0):.2f}\\n\")\n",
    "                    \n",
    "                    if analyse.get('clients_detectes'):\n",
    "                        f.write(f\"  Clients: {', '.join(analyse['clients_detectes'])}\\n\")\n",
    "                    if analyse.get('bases_detectees'):\n",
    "                        f.write(f\"  Bases: {', '.join(analyse['bases_detectees'])}\\n\")\n",
    "                    if analyse.get('maladies_detectees'):\n",
    "                        f.write(f\"  Termes médicaux: {', '.join(analyse['maladies_detectees'])}\\n\")\n",
    "                    if analyse.get('justification'):\n",
    "                        f.write(f\"  Justification: {analyse['justification']}\\n\")\n",
    "                    \n",
    "                    f.write(\"\\n\")\n",
    "        \n",
    "        print(f\"✅ Rapport texte sauvegardé: {rapport_txt_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur sauvegarde rapport texte: {e}\")\n",
    "    \n",
    "    return rapport\n",
    "\n",
    "def main():\n",
    "    \"\"\"Fonction principale - orchestration complète\"\"\"\n",
    "    print(\"🏥 CLASSIFICATEUR MÉDICAL IQVIA - ANALYSE SÉMANTIQUE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"📅 Démarrage: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n",
    "    print()\n",
    "    \n",
    "    try:\n",
    "        # Phase 1: Recherche et filtrage\n",
    "        fichiers_info = rechercher_fichiers_filtres()\n",
    "        if not fichiers_info:\n",
    "            print(\"❌ Aucun fichier trouvé. Arrêt du processus.\")\n",
    "            return\n",
    "        \n",
    "        # Phase 1.5: Tri et analyse\n",
    "        fichiers_info = trier_fichiers_par_criteres(fichiers_info)\n",
    "        \n",
    "        # Phase 2: Copie\n",
    "        copied = copier_fichiers(fichiers_info)\n",
    "        if copied == 0:\n",
    "            print(\"❌ Aucun fichier copié. Arrêt du processus.\")\n",
    "            return\n",
    "        \n",
    "        # Phase 3: Analyse sémantique\n",
    "        analyses = analyser_et_classer_semantique()\n",
    "        if not analyses:\n",
    "            print(\"❌ Aucune analyse réalisée. Arrêt du processus.\")\n",
    "            return\n",
    "        \n",
    "        # Phase 4: Déplacement dans dossiers\n",
    "        stats_deplacement = creer_dossiers_et_deplacer(analyses)\n",
    "        \n",
    "        # Phase 5: Rapport final\n",
    "        rapport = generer_rapport_final(analyses, stats_deplacement)\n",
    "        \n",
    "        print(f\"\\n🎉 CLASSIFICATION TERMINÉE AVEC SUCCÈS!\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"✅ {len(analyses)} fichiers traités\")\n",
    "        print(f\"📁 {len(stats_deplacement)} dossiers créés\")\n",
    "        print(f\"📋 Rapports générés dans {CHEMIN_DEPOTS}\")\n",
    "        print(f\"⏱️ Durée totale: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ ERREUR CRITIQUE: {e}\")\n",
    "        print(\"🔧 Vérifiez la configuration et les chemins d'accès\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
