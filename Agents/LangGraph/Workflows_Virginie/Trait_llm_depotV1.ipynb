{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db916746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Classificateur m√©dical avec analyse s√©mantique am√©lior√©e\n",
    "Format simple - sans classes ni d√©pendances externes\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Configuration\n",
    "BASE_DIR = os.path.abspath(r\"C:\\Users\\kosmo\\pycode\\Iqvia_process\")\n",
    "CHEMIN_SOURCE = os.path.join(BASE_DIR, \"ProcessEx\")\n",
    "CHEMIN_DEPOTS = os.path.join(BASE_DIR, \"Depots\")\n",
    "\n",
    "# Initialiser le LLM\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not api_key:\n",
    "    print(\"‚ö†Ô∏è ATTENTION: Cl√© API non trouv√©e dans les variables d'environnement\")\n",
    "    api_key = \"votre_cl√©_api_openai_ici\"  # √Ä remplacer par votre vraie cl√©\n",
    "\n",
    "llm = None\n",
    "if api_key != \"votre_cl√©_api_openai_ici\":\n",
    "    try:\n",
    "        llm = ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0.1,\n",
    "            api_key=api_key\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur initialisation LLM: {e}\")\n",
    "        llm = None\n",
    "\n",
    "def detection_mots_cles_medicaux(titre: str) -> dict:\n",
    "    \"\"\"D√©tection fallback par mots-cl√©s enrichie avec clients et bases de donn√©es\"\"\"\n",
    "    titre_lower = titre.lower()\n",
    "    \n",
    "    # Dictionnaire des clients pharmaceutiques\n",
    "    clients_pharma = {\n",
    "        'iqvia': 'iqvia', 'abbvie': 'abbvie', 'sanofi': 'sanofi', 'pfizer': 'pfizer',\n",
    "        'novartis': 'novartis', 'roche': 'roche', 'merck': 'merck', 'gsk': 'gsk',\n",
    "        'glaxosmithkline': 'gsk', 'astrazeneca': 'astrazeneca', 'bayer': 'bayer',\n",
    "        'boehringer': 'boehringer', 'eli lilly': 'lilly', 'lilly': 'lilly',\n",
    "        'johnson': 'jnj', 'j&j': 'jnj', 'bristol': 'bms', 'takeda': 'takeda',\n",
    "        'amgen': 'amgen', 'gilead': 'gilead', 'biogen': 'biogen', 'celgene': 'celgene'\n",
    "    }\n",
    "    \n",
    "    # Dictionnaire des bases de donn√©es IQVIA\n",
    "    bases_donnees = {\n",
    "        'xponent': 'xponent', 'channel dynamics': 'channel_dynamics', 'channel': 'channel_dynamics',\n",
    "        'bilan march√©': 'bilan_marche', 'bilan': 'bilan_marche', 'marche': 'bilan_marche',\n",
    "        'midas': 'midas', 'pharmetrics': 'pharmetrics', 'longitudinal': 'longitudinal',\n",
    "        'plantrak': 'plantrak', 'hospital': 'hospital_data', 'retail': 'retail_data',\n",
    "        'oncology': 'oncology_monitor', 'monitor': 'monitor', 'therapy': 'therapy_monitor',\n",
    "        'payor': 'payor_data', 'claims': 'claims_data', 'ema': 'ema_data',\n",
    "        'sales force': 'sales_force', 'salesforce': 'sales_force'\n",
    "    }\n",
    "    \n",
    "    # Dictionnaire enrichi avec contextes s√©mantiques (pathologies)\n",
    "    termes_contextes = {\n",
    "        # Pathologies oncologiques\n",
    "        'cancer': ('oncologie', 'clinique'), 'tumeur': ('oncologie', 'clinique'), \n",
    "        'oncologie': ('oncologie', 'clinique'), 'prostate': ('oncologie', 'clinique'),\n",
    "        'sein': ('oncologie', 'clinique'), 'poumon': ('oncologie', 'clinique'),\n",
    "        'm√©tastase': ('oncologie', 'clinique'), 'chimioth√©rapie': ('oncologie', 'traitement'),\n",
    "        \n",
    "        # Pathologies endocriniennes\n",
    "        'diab√®te': ('diabetologie', 'clinique'), 'diabete': ('diabetologie', 'clinique'), \n",
    "        'glyc√©mie': ('diabetologie', 'clinique'), 'insuline': ('diabetologie', 'traitement'),\n",
    "        'hba1c': ('diabetologie', 'diagnostic'),\n",
    "        \n",
    "        # Pathologies cardiovasculaires\n",
    "        'cardiaque': ('cardiologie', 'clinique'), 'coeur': ('cardiologie', 'clinique'), \n",
    "        'c≈ìur': ('cardiologie', 'clinique'), 'hypertension': ('cardiologie', 'clinique'),\n",
    "        'infarctus': ('cardiologie', 'urgence'), 'arythmie': ('cardiologie', 'clinique'),\n",
    "        \n",
    "        # Pathologies neurologiques\n",
    "        'alzheimer': ('neurologie', 'clinique'), 'parkinson': ('neurologie', 'clinique'), \n",
    "        'neurologie': ('neurologie', 'clinique'), '√©pilepsie': ('neurologie', 'clinique'),\n",
    "        'avc': ('neurologie', 'urgence'), 'scl√©rose': ('neurologie', 'clinique'),\n",
    "        \n",
    "        # Pathologies respiratoires\n",
    "        'asthme': ('pneumologie', 'clinique'), 'pneumonie': ('pneumologie', 'urgence'), \n",
    "        'bronchite': ('pneumologie', 'clinique'), 'bpco': ('pneumologie', 'clinique'),\n",
    "        \n",
    "        # Pathologies dermatologiques\n",
    "        'ecz√©ma': ('dermatologie', 'clinique'), 'eczema': ('dermatologie', 'clinique'), \n",
    "        'psoriasis': ('dermatologie', 'clinique'), 'dermatite': ('dermatologie', 'clinique'),\n",
    "        \n",
    "        # Contextes non-pathologiques\n",
    "        'formation': ('formation', 'commercial'), 'training': ('formation', 'commercial'),\n",
    "        'vente': ('commercial', 'business'), 'marketing': ('commercial', 'business'),\n",
    "        '√©tude': ('recherche', 'scientifique'), 'essai': ('recherche', 'scientifique'),\n",
    "        'protocole': ('recherche', 'scientifique'), 'phase': ('recherche', 'scientifique'),\n",
    "        'r√©glementation': ('reglementaire', 'administratif'), 'autorisation': ('reglementaire', 'administratif'),\n",
    "        'surveillance': ('pharmacovigilance', 'securite'), 'observance': ('pharmacovigilance', 'therapeutique')\n",
    "    }\n",
    "    \n",
    "    # D√©tection des clients\n",
    "    clients_detectes = []\n",
    "    for client_terme, client_nom in clients_pharma.items():\n",
    "        if client_terme in titre_lower:\n",
    "            clients_detectes.append(client_nom)\n",
    "    \n",
    "    # D√©tection des bases de donn√©es\n",
    "    bases_detectees = []\n",
    "    for base_terme, base_nom in bases_donnees.items():\n",
    "        if base_terme in titre_lower:\n",
    "            bases_detectees.append(base_nom)\n",
    "    \n",
    "    # D√©tection des termes m√©dicaux/contextuels\n",
    "    termes_detectes = []\n",
    "    contextes_detectes = []\n",
    "    for terme, (domaine, contexte) in termes_contextes.items():\n",
    "        if terme in titre_lower:\n",
    "            termes_detectes.append(terme)\n",
    "            contextes_detectes.append((domaine, contexte))\n",
    "    \n",
    "    # Logique de classification prioritaire\n",
    "    if clients_detectes:\n",
    "        # Si client d√©tect√©, c'est prioritaire\n",
    "        client_principal = clients_detectes[0]\n",
    "        return {\n",
    "            \"contient_maladie\": False,\n",
    "            \"maladies_detectees\": termes_detectes,\n",
    "            \"categorie_medicale\": \"client\",\n",
    "            \"contexte_principal\": \"client\",\n",
    "            \"client_detecte\": client_principal,\n",
    "            \"clients_detectes\": clients_detectes,\n",
    "            \"bases_detectees\": bases_detectees,\n",
    "            \"confiance\": \"haute\" if len(clients_detectes) > 1 else \"moyenne\",\n",
    "            \"titre_normalise\": titre,\n",
    "            \"score_semantique\": 0.8\n",
    "        }\n",
    "    \n",
    "    elif bases_detectees:\n",
    "        # Si base de donn√©es d√©tect√©e\n",
    "        base_principale = bases_detectees[0]\n",
    "        return {\n",
    "            \"contient_maladie\": False,\n",
    "            \"maladies_detectees\": termes_detectes,\n",
    "            \"categorie_medicale\": \"base_donnees\",\n",
    "            \"contexte_principal\": \"base_donnees\",\n",
    "            \"base_detectee\": base_principale,\n",
    "            \"clients_detectes\": clients_detectes,\n",
    "            \"bases_detectees\": bases_detectees,\n",
    "            \"confiance\": \"haute\" if len(bases_detectees) > 1 else \"moyenne\",\n",
    "            \"titre_normalise\": titre,\n",
    "            \"score_semantique\": 0.8\n",
    "        }\n",
    "    \n",
    "    elif termes_detectes:\n",
    "        # Classification m√©dicale/contextuelle classique\n",
    "        domaine_principal, contexte_principal = contextes_detectes[0]\n",
    "        confiance = \"haute\" if len(termes_detectes) > 1 else \"moyenne\"\n",
    "        \n",
    "        # D√©terminer si c'est m√©dical\n",
    "        domaines_medicaux = ['oncologie', 'diabetologie', 'cardiologie', 'neurologie', 'pneumologie', 'dermatologie']\n",
    "        est_medical = domaine_principal in domaines_medicaux\n",
    "        \n",
    "        return {\n",
    "            \"contient_maladie\": est_medical,\n",
    "            \"maladies_detectees\": termes_detectes,\n",
    "            \"categorie_medicale\": domaine_principal,\n",
    "            \"contexte_principal\": contexte_principal,\n",
    "            \"clients_detectes\": clients_detectes,\n",
    "            \"bases_detectees\": bases_detectees,\n",
    "            \"confiance\": confiance,\n",
    "            \"titre_normalise\": titre,\n",
    "            \"score_semantique\": 0.7 if confiance == \"haute\" else 0.5\n",
    "        }\n",
    "    \n",
    "    # Aucune d√©tection\n",
    "    return {\n",
    "        \"contient_maladie\": False,\n",
    "        \"maladies_detectees\": [],\n",
    "        \"categorie_medicale\": \"aucune\",\n",
    "        \"contexte_principal\": \"autre\",\n",
    "        \"clients_detectes\": clients_detectes,\n",
    "        \"bases_detectees\": bases_detectees,\n",
    "        \"confiance\": \"faible\",\n",
    "        \"titre_normalise\": titre,\n",
    "        \"score_semantique\": 0.2\n",
    "    }\n",
    "\n",
    "def analyser_titre_avec_llm_semantique(titre: str) -> dict:\n",
    "    \"\"\"Analyse s√©mantique avanc√©e avec LLM - prompt enrichi\"\"\"\n",
    "    if not llm:\n",
    "        print(f\"‚ùå LLM non disponible, utilisation fallback pour '{titre[:30]}...'\")\n",
    "        return detection_mots_cles_medicaux(titre)\n",
    "    \n",
    "    try:\n",
    "        analysis_prompt_template = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"Tu es un expert en analyse s√©mantique m√©dicale et business pharmaceutique. \n",
    "            Ton r√¥le est de comprendre le CONTEXTE et l'INTENTION derri√®re chaque titre, pas seulement chercher des mots-cl√©s.\n",
    "            \n",
    "            Tu ma√Ætrises ces domaines :\n",
    "            - CLINIQUE : pathologies, diagnostics, traitements, soins patients\n",
    "            - RECHERCHE : √©tudes, essais, protocoles, d√©veloppement\n",
    "            - COMMERCIAL : formation √©quipes, marketing, ventes, business\n",
    "            - R√âGLEMENTAIRE : autorisations, compliance, pharmacovigilance\n",
    "            - FORMATION : training, √©ducation, guidelines\n",
    "            \"\"\"),\n",
    "            (\"human\", \"\"\"\n",
    "Analyse s√©mantique COMPL√àTE du titre : \"{titre}\"\n",
    "\n",
    "Ne te contente pas de chercher des mots-cl√©s ! Analyse le CONTEXTE :\n",
    "\n",
    "EXEMPLES D'ANALYSE S√âMANTIQUE :\n",
    "- \"Formation √©quipe vente oncologie Q1 2025\" ‚Üí CONTEXTE=commercial, DOMAINE=oncologie, TYPE=formation\n",
    "- \"Suivi patients diab√®te h√¥pital\" ‚Üí CONTEXTE=clinique, DOMAINE=diabetologie, TYPE=suivi_medical  \n",
    "- \"Protocole √©tude phase III cancer poumon\" ‚Üí CONTEXTE=recherche, DOMAINE=oncologie, TYPE=essai_clinique\n",
    "- \"Rapport Xponent Sanofi Q4 2024\" ‚Üí CONTEXTE=client, CLIENT=sanofi, BASE=xponent\n",
    "- \"Analyse Channel Dynamics ABBVIE\" ‚Üí CONTEXTE=base_donnees, CLIENT=abbvie, BASE=channel_dynamics\n",
    "- \"Pr√©sentation IQVIA Bilan March√©\" ‚Üí CONTEXTE=client, CLIENT=iqvia, BASE=bilan_marche\n",
    "\n",
    "MISSION : Identifie le CONTEXTE principal, d√©tecte les CLIENTS et BASES DE DONN√âES.\n",
    "\n",
    "**CLIENTS PRIORITAIRES :** IQVIA, ABBVIE, Sanofi, Pfizer, Novartis, Roche, Merck, GSK, AstraZeneca, Bayer, Boehringer, Lilly, J&J, Bristol, Takeda, Amgen, Gilead, Biogen\n",
    "\n",
    "**BASES DE DONN√âES :** Xponent, Channel Dynamics, Bilan March√©, MIDAS, Pharmetrics, Longitudinal, PlanTrak, Hospital, Retail, Oncology Monitor, Therapy Monitor, Claims Data, Sales Force\n",
    "\n",
    "R√©ponds UNIQUEMENT en JSON valide :\n",
    "{{\n",
    "    \"contexte_principal\": \"clinique/recherche/commercial/reglementaire/formation/client/base_donnees/autre\",\n",
    "    \"domaine_medical\": \"oncologie/cardiologie/neurologie/diabetologie/pneumologie/dermatologie/pharmacovigilance/business/client/base_donnees/aucun\",\n",
    "    \"type_document\": \"formation/etude/rapport/suivi/presentation/protocole/guide/autre\",\n",
    "    \"population_cible\": \"patients/professionnels/equipes_vente/chercheurs/regulateurs/clients/mixte\",\n",
    "    \"maladies_detectees\": [\"terme1\", \"terme2\"],\n",
    "    \"clients_detectes\": [\"client1\", \"client2\"],\n",
    "    \"bases_detectees\": [\"base1\", \"base2\"],\n",
    "    \"client_principal\": \"nom_client_principal_ou_null\",\n",
    "    \"base_principale\": \"nom_base_principale_ou_null\",\n",
    "    \"contient_maladie\": true,\n",
    "    \"confiance\": \"haute/moyenne/faible\",\n",
    "    \"score_semantique\": 0.8,\n",
    "    \"justification\": \"Explication courte du contexte d√©tect√©\",\n",
    "    \"titre_normalise\": \"{titre}\"\n",
    "}}\n",
    "            \"\"\")\n",
    "        ])\n",
    "        \n",
    "        prompt_value = analysis_prompt_template.invoke({\"titre\": titre})\n",
    "        response = llm.invoke(prompt_value.to_messages())\n",
    "        content = response.content.strip()\n",
    "        \n",
    "        # Nettoyer le JSON\n",
    "        if \"```json\" in content:\n",
    "            content = content.split(\"```json\")[1].split(\"```\")[0]\n",
    "        elif \"```\" in content:\n",
    "            content = content.split(\"```\")[1].split(\"```\")[0]\n",
    "        \n",
    "        resultat = json.loads(content.strip())\n",
    "        \n",
    "        # Validation et enrichissement\n",
    "        if \"categorie_medicale\" not in resultat:\n",
    "            resultat[\"categorie_medicale\"] = resultat.get(\"domaine_medical\", \"aucune\")\n",
    "        \n",
    "        # Assurer la coh√©rence clients/bases de donn√©es\n",
    "        if not resultat.get(\"clients_detectes\"):\n",
    "            resultat[\"clients_detectes\"] = []\n",
    "        if not resultat.get(\"bases_detectees\"):\n",
    "            resultat[\"bases_detectees\"] = []\n",
    "        \n",
    "        # D√©terminer le client/base principal\n",
    "        if resultat.get(\"clients_detectes\") and not resultat.get(\"client_principal\"):\n",
    "            resultat[\"client_principal\"] = resultat[\"clients_detectes\"][0]\n",
    "        if resultat.get(\"bases_detectees\") and not resultat.get(\"base_principale\"):\n",
    "            resultat[\"base_principale\"] = resultat[\"bases_detectees\"][0]\n",
    "        \n",
    "        # Ajuster le contexte si client ou base d√©tect√©\n",
    "        if resultat.get(\"client_principal\"):\n",
    "            resultat[\"contexte_principal\"] = \"client\"\n",
    "            resultat[\"categorie_medicale\"] = \"client\"\n",
    "        elif resultat.get(\"base_principale\"):\n",
    "            resultat[\"contexte_principal\"] = \"base_donnees\"\n",
    "            resultat[\"categorie_medicale\"] = \"base_donnees\"\n",
    "        \n",
    "        # Calculer score s√©mantique si absent\n",
    "        if \"score_semantique\" not in resultat or not isinstance(resultat[\"score_semantique\"], (int, float)):\n",
    "            contexte = resultat.get(\"contexte_principal\", \"autre\")\n",
    "            domaine = resultat.get(\"domaine_medical\", \"aucun\")\n",
    "            \n",
    "            score = 0.3  # Base\n",
    "            if contexte != \"autre\": score += 0.2\n",
    "            if domaine != \"aucun\": score += 0.2\n",
    "            if len(resultat.get(\"maladies_detectees\", [])) > 0: score += 0.2\n",
    "            if resultat.get(\"confiance\") == \"haute\": score += 0.1\n",
    "            \n",
    "            resultat[\"score_semantique\"] = min(0.95, score)\n",
    "        \n",
    "        return resultat\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur LLM s√©mantique pour '{titre[:30]}...': {e}\")\n",
    "        return detection_mots_cles_medicaux(titre)\n",
    "\n",
    "def rechercher_fichiers_filtres():\n",
    "    \"\"\"Phase 1: Recherche et filtre les fichiers par extension, date et taille\"\"\"\n",
    "    print(\"üîç PHASE 1: Recherche et filtrage des fichiers...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    if not os.path.exists(CHEMIN_SOURCE):\n",
    "        print(f\"‚ùå Erreur: {CHEMIN_SOURCE} n'existe pas\")\n",
    "        return []\n",
    "    \n",
    "    # D√©finition des filtres\n",
    "    DATE_LIMITE = datetime.now() - timedelta(days=365)  # Fichiers de moins d'un an\n",
    "    TAILLE_MIN_OCTETS = 1 * 1024         # 1 KB minimum (pour √©viter fichiers vides)\n",
    "    TAILLE_MAX_OCTETS = 50 * 1024 * 1024  # 50 MB maximum\n",
    "    TAILLE_CLASSIFICATION = 3000 * 1024   # 2 MB - Seuil pour classification s√©mantique\n",
    "    \n",
    "    print(f\"üìÖ Filtre date: fichiers modifi√©s apr√®s le {DATE_LIMITE.strftime('%d/%m/%Y')}\")\n",
    "    print(f\"üìè Filtre taille: entre {TAILLE_MIN_OCTETS//1024} KB et {TAILLE_MAX_OCTETS//1024//1024} MB\")\n",
    "    print(f\"üß† Classification s√©mantique: fichiers > {TAILLE_CLASSIFICATION//1024} KB seulement\")\n",
    "    print(f\"üìÅ Fichiers ‚â§ {TAILLE_CLASSIFICATION//1024} KB ‚Üí dossier AUTRES automatiquement\")\n",
    "    print()\n",
    "    \n",
    "    fichiers_trouves = []\n",
    "    dossiers_stats = {}\n",
    "    stats_filtrage = {\n",
    "        'total_examines': 0,\n",
    "        'rejetes_extension': 0,\n",
    "        'rejetes_taille': 0, \n",
    "        'rejetes_date': 0,\n",
    "        'rejetes_acces': 0,\n",
    "        'acceptes': 0\n",
    "    }\n",
    "    \n",
    "    for item in os.listdir(CHEMIN_SOURCE):\n",
    "        item_path = os.path.join(CHEMIN_SOURCE, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            print(f\"üìÇ Analyse du dossier: {item}\")\n",
    "            count_dossier = 0\n",
    "            \n",
    "            for root, dirs, files in os.walk(item_path):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    stats_filtrage['total_examines'] += 1\n",
    "                    \n",
    "                    # V√©rification de l'extension\n",
    "                    if not file.lower().endswith(('.ppt', '.pptx', '.pdf')):\n",
    "                        stats_filtrage['rejetes_extension'] += 1\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        # R√©cup√©ration des m√©tadonn√©es\n",
    "                        taille = os.path.getsize(file_path)\n",
    "                        date_mod = datetime.fromtimestamp(os.path.getmtime(file_path))\n",
    "                        \n",
    "                        # Filtrage par taille\n",
    "                        if not (TAILLE_MIN_OCTETS <= taille <= TAILLE_MAX_OCTETS):\n",
    "                            stats_filtrage['rejetes_taille'] += 1\n",
    "                            continue\n",
    "                        \n",
    "                        # Filtrage par date  \n",
    "                        if date_mod < DATE_LIMITE:\n",
    "                            stats_filtrage['rejetes_date'] += 1\n",
    "                            continue\n",
    "                        \n",
    "                        # Fichier accept√©\n",
    "                        fichier_info = {\n",
    "                            'chemin': file_path,\n",
    "                            'nom': file,\n",
    "                            'taille': taille,\n",
    "                            'date_modification': date_mod,\n",
    "                            'taille_mb': round(taille / (1024*1024), 2),\n",
    "                            'taille_kb': round(taille / 1024, 1),\n",
    "                            'age_jours': (datetime.now() - date_mod).days,\n",
    "                            'classification_semantique': taille >= TAILLE_CLASSIFICATION  # True si > 2MB\n",
    "                        }\n",
    "                        \n",
    "                        fichiers_trouves.append(fichier_info)\n",
    "                        count_dossier += 1\n",
    "                        stats_filtrage['acceptes'] += 1\n",
    "                        \n",
    "                    except (OSError, PermissionError) as e:\n",
    "                        stats_filtrage['rejetes_acces'] += 1\n",
    "                        print(f\"   ‚ö†Ô∏è Ignor√© (acc√®s refus√©): {os.path.basename(file_path)}\")\n",
    "            \n",
    "            dossiers_stats[item] = count_dossier\n",
    "            print(f\"   ‚úÖ {count_dossier} fichier(s) retenu(s) apr√®s filtrage\")\n",
    "    \n",
    "    # Affichage des statistiques d√©taill√©es\n",
    "    print(f\"\\nüìä STATISTIQUES DE FILTRAGE:\")\n",
    "    print(f\"   ‚Ä¢ Fichiers examin√©s: {stats_filtrage['total_examines']}\")\n",
    "    print(f\"   ‚Ä¢ ‚ùå Rejet√©s - extension: {stats_filtrage['rejetes_extension']}\")\n",
    "    print(f\"   ‚Ä¢ ‚ùå Rejet√©s - taille: {stats_filtrage['rejetes_taille']}\")\n",
    "    print(f\"   ‚Ä¢ ‚ùå Rejet√©s - date: {stats_filtrage['rejetes_date']}\")\n",
    "    print(f\"   ‚Ä¢ ‚ùå Rejet√©s - acc√®s: {stats_filtrage['rejetes_acces']}\")\n",
    "    print(f\"   ‚Ä¢ ‚úÖ RETENUS: {stats_filtrage['acceptes']}\")\n",
    "    \n",
    "    print(f\"\\nüìÅ R√âPARTITION PAR DOSSIER:\")\n",
    "    for dossier, count in dossiers_stats.items():\n",
    "        print(f\"   ‚Ä¢ {dossier}: {count} fichier(s)\")\n",
    "    \n",
    "    return fichiers_trouves\n",
    "\n",
    "def trier_fichiers_par_criteres(fichiers_info):\n",
    "    \"\"\"Trie les fichiers par diff√©rents crit√®res et affiche des statistiques\"\"\"\n",
    "    print(f\"\\nüìà PHASE 1.5: Analyse et tri des fichiers retenus...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    if not fichiers_info:\n",
    "        print(\"‚ùå Aucun fichier √† analyser\")\n",
    "        return fichiers_info\n",
    "    \n",
    "    # Tri par taille (d√©croissant)\n",
    "    fichiers_par_taille = sorted(fichiers_info, key=lambda x: x['taille'], reverse=True)\n",
    "    print(f\"üìè Top 5 fichiers les plus volumineux:\")\n",
    "    for i, f in enumerate(fichiers_par_taille[:5], 1):\n",
    "        print(f\"   {i}. {f['nom'][:50]}... ({f['taille_mb']} MB)\")\n",
    "    \n",
    "    # Tri par date (plus r√©cent d'abord) \n",
    "    fichiers_par_date = sorted(fichiers_info, key=lambda x: x['date_modification'], reverse=True)\n",
    "    print(f\"\\nüìÖ Top 5 fichiers les plus r√©cents:\")\n",
    "    for i, f in enumerate(fichiers_par_date[:5], 1):\n",
    "        date_str = f['date_modification'].strftime('%d/%m/%Y')\n",
    "        print(f\"   {i}. {f['nom'][:50]}... ({date_str}, {f['age_jours']} jours)\")\n",
    "    \n",
    "    # Statistiques de r√©partition\n",
    "    tailles = [f['taille_mb'] for f in fichiers_info]\n",
    "    ages = [f['age_jours'] for f in fichiers_info]\n",
    "    \n",
    "    print(f\"\\nüìä STATISTIQUES DESCRIPTIVES:\")\n",
    "    print(f\"   Taille - Moyenne: {sum(tailles)/len(tailles):.1f} MB, Max: {max(tailles):.1f} MB, Min: {min(tailles):.1f} MB\")\n",
    "    print(f\"   √Çge - Moyenne: {sum(ages)//len(ages)} jours, Max: {max(ages)} jours, Min: {min(ages)} jours\")\n",
    "    \n",
    "    # R√©partition par tranches de taille et √©ligibilit√© classification\n",
    "    tranches_taille = {'< 1MB': 0, '1-2MB': 0, '2-5MB': 0, '5-20MB': 0, '> 20MB': 0}\n",
    "    eligibles_classification = 0\n",
    "    \n",
    "    for f in fichiers_info:\n",
    "        mb = f['taille_mb']\n",
    "        if mb < 1: tranches_taille['< 1MB'] += 1\n",
    "        elif mb < 2: tranches_taille['1-2MB'] += 1  \n",
    "        elif mb < 5: tranches_taille['2-5MB'] += 1\n",
    "        elif mb < 20: tranches_taille['5-20MB'] += 1\n",
    "        else: tranches_taille['> 20MB'] += 1\n",
    "        \n",
    "        if f['classification_semantique']:\n",
    "            eligibles_classification += 1\n",
    "    \n",
    "    print(f\"\\nüìè R√âPARTITION PAR TAILLE:\")\n",
    "    for tranche, count in tranches_taille.items():\n",
    "        pct = (count / len(fichiers_info) * 100) if fichiers_info else 0\n",
    "        print(f\"   ‚Ä¢ {tranche}: {count} fichiers ({pct:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nüß† √âLIGIBILIT√â CLASSIFICATION S√âMANTIQUE:\")\n",
    "    print(f\"   ‚Ä¢ Fichiers > 2MB (analys√©s): {eligibles_classification}\")\n",
    "    print(f\"   ‚Ä¢ Fichiers ‚â§ 2MB (‚Üí AUTRES): {len(fichiers_info) - eligibles_classification}\")\n",
    "    \n",
    "    return fichiers_info\n",
    "\n",
    "def copier_fichiers(fichiers_info):\n",
    "    \"\"\"Phase 2: Copie vers Depots avec informations enrichies\"\"\"\n",
    "    print(f\"\\nüìã PHASE 2: Copie vers {CHEMIN_DEPOTS}...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Cr√©er le dossier Depots\n",
    "    os.makedirs(CHEMIN_DEPOTS, exist_ok=True)\n",
    "    print(f\"‚úÖ Dossier Depots pr√™t\")\n",
    "    \n",
    "    copied = 0\n",
    "    taille_totale = 0\n",
    "    \n",
    "    for fichier_info in fichiers_info:\n",
    "        file_path = fichier_info['chemin']\n",
    "        file_name = fichier_info['nom']\n",
    "        destination = os.path.join(CHEMIN_DEPOTS, file_name)\n",
    "        \n",
    "        try:\n",
    "            shutil.copy(file_path, destination)\n",
    "            copied += 1\n",
    "            taille_totale += fichier_info['taille']\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur copie {file_name}: {e}\")\n",
    "    \n",
    "    print(f\"‚úÖ {copied}/{len(fichiers_info)} fichiers copi√©s\")\n",
    "    print(f\"üì¶ Taille totale copi√©e: {taille_totale/(1024*1024):.1f} MB\")\n",
    "    return copied\n",
    "\n",
    "def analyser_et_classer_semantique():\n",
    "    \"\"\"Phase 3: Analyse s√©mantique LLM et classification avanc√©e (> 2MB seulement)\"\"\"\n",
    "    print(f\"\\nüß† PHASE 3: Analyse s√©mantique et classification...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Mode LLM ou fallback\n",
    "    use_llm = llm is not None\n",
    "    print(f\"Mode: {'üî• LLM S√©mantique' if use_llm else 'üîß Mots-cl√©s enrichis'}\")\n",
    "    \n",
    "    fichiers_a_analyser = [f for f in os.listdir(CHEMIN_DEPOTS) \n",
    "                          if f.lower().endswith(('.ppt', '.pptx', '.pdf'))]\n",
    "    \n",
    "    # R√©cup√©rer les infos de taille des fichiers copi√©s\n",
    "    fichiers_avec_taille = []\n",
    "    for file_name in fichiers_a_analyser:\n",
    "        file_path = os.path.join(CHEMIN_DEPOTS, file_name)\n",
    "        try:\n",
    "            taille = os.path.getsize(file_path)\n",
    "            fichiers_avec_taille.append({\n",
    "                'nom': file_name,\n",
    "                'taille': taille,\n",
    "                'taille_mb': round(taille / (1024*1024), 2),\n",
    "                'taille_kb': round(taille / 1024, 1),\n",
    "                'eligible_classification': taille >= (3000 * 1024)  # > 2MB\n",
    "            })\n",
    "        except:\n",
    "            # Si erreur, on consid√®re comme petit fichier\n",
    "            fichiers_avec_taille.append({\n",
    "                'nom': file_name,\n",
    "                'taille': 0,\n",
    "                'taille_mb': 0,\n",
    "                'taille_kb': 0,\n",
    "                'eligible_classification': False\n",
    "            })\n",
    "    \n",
    "    analyses = []\n",
    "    medicaux = 0\n",
    "    contextes_stats = {}\n",
    "    petits_fichiers = 0\n",
    "    \n",
    "    print(f\"üìä R√©partition:\")\n",
    "    eligibles = sum(1 for f in fichiers_avec_taille if f['eligible_classification'])\n",
    "    print(f\"   ‚Ä¢ Fichiers > 2MB (analyse s√©mantique): {eligibles}\")\n",
    "    print(f\"   ‚Ä¢ Fichiers ‚â§ 2MB (‚Üí AUTRES direct): {len(fichiers_avec_taille) - eligibles}\")\n",
    "    print()\n",
    "    \n",
    "    for i, fichier_info in enumerate(fichiers_avec_taille, 1):\n",
    "        file_name = fichier_info['nom']\n",
    "        titre = os.path.splitext(file_name)[0]\n",
    "        taille_str = f\"{fichier_info['taille_kb']} KB\" if fichier_info['taille_mb'] < 1 else f\"{fichier_info['taille_mb']} MB\"\n",
    "        \n",
    "        print(f\"[{i:2d}/{len(fichiers_avec_taille)}] {file_name[:40]}... ({taille_str})\")\n",
    "        \n",
    "        # V√©rifier si √©ligible √† la classification s√©mantique\n",
    "        if not fichier_info['eligible_classification']:\n",
    "            # Fichier ‚â§ 2MB ‚Üí Dossier AUTRES automatiquement\n",
    "            analyse = {\n",
    "                'nom_fichier': file_name,\n",
    "                'titre': titre,\n",
    "                'taille_mb': fichier_info['taille_mb'],\n",
    "                'contient_maladie': False,\n",
    "                'categorie_medicale': 'autres',\n",
    "                'contexte_principal': 'autre',\n",
    "                'confiance': 'faible',\n",
    "                'score_semantique': 0.1,\n",
    "                'justification': f'Fichier de petite taille ({taille_str}) ‚Üí AUTRES automatiquement',\n",
    "                'clients_detectes': [],\n",
    "                'bases_detectees': [],\n",
    "                'maladies_detectees': []\n",
    "            }\n",
    "            analyses.append(analyse)\n",
    "            petits_fichiers += 1\n",
    "            print(f\"   üìÅ ‚Üí AUTRES (taille < 2MB)\")\n",
    "            continue\n",
    "        \n",
    "        # Classification s√©mantique pour fichiers > 2MB\n",
    "        if use_llm:\n",
    "            analyse = analyser_titre_avec_llm_semantique(titre)\n",
    "        else:\n",
    "            analyse = detection_mots_cles_medicaux(titre)\n",
    "        \n",
    "        # Enrichir avec m√©tadonn√©es\n",
    "        analyse['nom_fichier'] = file_name\n",
    "        analyse['taille_mb'] = fichier_info['taille_mb']\n",
    "        \n",
    "        # Compter et afficher r√©sultats\n",
    "        if analyse.get('contient_maladie', False):\n",
    "            medicaux += 1\n",
    "        \n",
    "        contexte = analyse.get('contexte_principal', 'autre')\n",
    "        contextes_stats[contexte] = contextes_stats.get(contexte, 0) + 1\n",
    "        \n",
    "        # Affichage du r√©sultat\n",
    "        categorie = analyse.get('categorie_medicale', 'aucune')\n",
    "        confiance = analyse.get('confiance', 'inconnue')\n",
    "        score = analyse.get('score_semantique', 0)\n",
    "        \n",
    "        if analyse.get('clients_detectes'):\n",
    "            client_info = f\" [Client: {', '.join(analyse['clients_detectes'])}]\"\n",
    "        elif analyse.get('bases_detectees'):\n",
    "            client_info = f\" [Base: {', '.join(analyse['bases_detectees'])}]\"\n",
    "        else:\n",
    "            client_info = \"\"\n",
    "        \n",
    "        print(f\"   üéØ {categorie.upper()} ({confiance}, score:{score:.2f}){client_info}\")\n",
    "        \n",
    "        if analyse.get('maladies_detectees'):\n",
    "            print(f\"   üè• Termes: {', '.join(analyse['maladies_detectees'])}\")\n",
    "        \n",
    "        analyses.append(analyse)\n",
    "    \n",
    "    print(f\"\\nüìä R√âSULTATS DE CLASSIFICATION:\")\n",
    "    print(f\"   ‚Ä¢ Fichiers analys√©s s√©mantiquement: {len(analyses) - petits_fichiers}\")\n",
    "    print(f\"   ‚Ä¢ Fichiers ‚Üí AUTRES (< 2MB): {petits_fichiers}\")\n",
    "    print(f\"   ‚Ä¢ Fichiers m√©dicaux d√©tect√©s: {medicaux}\")\n",
    "    print(f\"   ‚Ä¢ Score s√©mantique moyen: {sum(a.get('score_semantique', 0) for a in analyses) / len(analyses):.2f}\")\n",
    "    \n",
    "    print(f\"\\nüìà R√âPARTITION PAR CONTEXTE:\")\n",
    "    for contexte, count in sorted(contextes_stats.items()):\n",
    "        pct = (count / len(analyses) * 100) if analyses else 0\n",
    "        print(f\"   ‚Ä¢ {contexte}: {count} fichiers ({pct:.1f}%)\")\n",
    "    \n",
    "    return analyses\n",
    "\n",
    "def creer_dossiers_et_deplacer(analyses):\n",
    "    \"\"\"Phase 4: Cr√©ation des dossiers de classification et d√©placement\"\"\"\n",
    "    print(f\"\\nüìÅ PHASE 4: Cr√©ation dossiers et d√©placement...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Mapping des cat√©gories vers dossiers\n",
    "    mapping_dossiers = {\n",
    "        'oncologie': 'ONCOLOGIE',\n",
    "        'cardiologie': 'CARDIOLOGIE', \n",
    "        'neurologie': 'NEUROLOGIE',\n",
    "        'diabetologie': 'DIABETOLOGIE',\n",
    "        'pneumologie': 'PNEUMOLOGIE',\n",
    "        'dermatologie': 'DERMATOLOGIE',\n",
    "        'pharmacovigilance': 'PHARMACOVIGILANCE',\n",
    "        'formation': 'FORMATION',\n",
    "        'commercial': 'COMMERCIAL',\n",
    "        'recherche': 'RECHERCHE',\n",
    "        'reglementaire': 'REGLEMENTAIRE',\n",
    "        'client': 'CLIENTS',\n",
    "        'base_donnees': 'BASES_DONNEES',\n",
    "        'autres': 'AUTRES',\n",
    "        'aucune': 'AUTRES'\n",
    "    }\n",
    "    \n",
    "    # Cr√©er tous les dossiers n√©cessaires\n",
    "    dossiers_crees = set()\n",
    "    for categorie in mapping_dossiers.values():\n",
    "        dossier_path = os.path.join(CHEMIN_DEPOTS, categorie)\n",
    "        os.makedirs(dossier_path, exist_ok=True)\n",
    "        dossiers_crees.add(categorie)\n",
    "    \n",
    "    print(f\"‚úÖ {len(dossiers_crees)} dossiers de classification cr√©√©s\")\n",
    "    \n",
    "    # D√©placer les fichiers\n",
    "    stats_deplacement = {}\n",
    "    erreurs_deplacement = []\n",
    "    \n",
    "    for analyse in analyses:\n",
    "        file_name = analyse['nom_fichier']\n",
    "        categorie = analyse.get('categorie_medicale', 'autres')\n",
    "        dossier_cible = mapping_dossiers.get(categorie, 'AUTRES')\n",
    "        \n",
    "        source_path = os.path.join(CHEMIN_DEPOTS, file_name)\n",
    "        destination_path = os.path.join(CHEMIN_DEPOTS, dossier_cible, file_name)\n",
    "        \n",
    "        try:\n",
    "            if os.path.exists(source_path):\n",
    "                # G√©rer les doublons\n",
    "                if os.path.exists(destination_path):\n",
    "                    base, ext = os.path.splitext(file_name)\n",
    "                    counter = 1\n",
    "                    while os.path.exists(destination_path):\n",
    "                        new_name = f\"{base}_({counter}){ext}\"\n",
    "                        destination_path = os.path.join(CHEMIN_DEPOTS, dossier_cible, new_name)\n",
    "                        counter += 1\n",
    "                \n",
    "                shutil.move(source_path, destination_path)\n",
    "                stats_deplacement[dossier_cible] = stats_deplacement.get(dossier_cible, 0) + 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            erreurs_deplacement.append(f\"{file_name}: {str(e)}\")\n",
    "            print(f\"‚ùå Erreur d√©placement {file_name}: {e}\")\n",
    "    \n",
    "    print(f\"\\nüìä R√âSULTATS DE D√âPLACEMENT:\")\n",
    "    total_deplaces = sum(stats_deplacement.values())\n",
    "    for dossier, count in sorted(stats_deplacement.items()):\n",
    "        pct = (count / total_deplaces * 100) if total_deplaces > 0 else 0\n",
    "        print(f\"   ‚Ä¢ {dossier}: {count} fichiers ({pct:.1f}%)\")\n",
    "    \n",
    "    if erreurs_deplacement:\n",
    "        print(f\"\\n‚ùå ERREURS DE D√âPLACEMENT ({len(erreurs_deplacement)}):\")\n",
    "        for erreur in erreurs_deplacement[:5]:  # Limiter √† 5 erreurs\n",
    "            print(f\"   ‚Ä¢ {erreur}\")\n",
    "        if len(erreurs_deplacement) > 5:\n",
    "            print(f\"   ‚Ä¢ ... et {len(erreurs_deplacement) - 5} autres erreurs\")\n",
    "    \n",
    "    return stats_deplacement\n",
    "\n",
    "def generer_rapport_final(analyses, stats_deplacement):\n",
    "    \"\"\"Phase 5: G√©n√©ration du rapport final d√©taill√©\"\"\"\n",
    "    print(f\"\\nüìã PHASE 5: G√©n√©ration du rapport final...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Cr√©er le rapport JSON d√©taill√©\n",
    "    rapport = {\n",
    "        'metadata': {\n",
    "            'date_execution': datetime.now().isoformat(),\n",
    "            'version_script': '2.0',\n",
    "            'mode_llm': llm is not None,\n",
    "            'total_fichiers': len(analyses)\n",
    "        },\n",
    "        'statistiques_globales': {\n",
    "            'fichiers_medicaux': sum(1 for a in analyses if a.get('contient_maladie', False)),\n",
    "            'fichiers_clients': sum(1 for a in analyses if a.get('clients_detectes')),\n",
    "            'fichiers_bases_donnees': sum(1 for a in analyses if a.get('bases_detectees')),\n",
    "            'score_semantique_moyen': round(sum(a.get('score_semantique', 0) for a in analyses) / len(analyses), 3) if analyses else 0\n",
    "        },\n",
    "        'repartition_dossiers': stats_deplacement,\n",
    "        'analyses_detaillees': analyses\n",
    "    }\n",
    "    \n",
    "    # Sauvegarder le rapport JSON\n",
    "    rapport_path = os.path.join(CHEMIN_DEPOTS, 'rapport_classification.json')\n",
    "    try:\n",
    "        with open(rapport_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(rapport, f, indent=2, ensure_ascii=False, default=str)\n",
    "        print(f\"‚úÖ Rapport JSON sauvegard√©: {rapport_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur sauvegarde rapport: {e}\")\n",
    "    \n",
    "    # G√©n√©rer rapport texte lisible\n",
    "    rapport_txt_path = os.path.join(CHEMIN_DEPOTS, 'rapport_classification.txt')\n",
    "    try:\n",
    "        with open(rapport_txt_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"=\" * 80 + \"\\n\")\n",
    "            f.write(\"RAPPORT DE CLASSIFICATION M√âDICALE AUTOMATIQUE\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "            \n",
    "            f.write(f\"üìÖ Date d'ex√©cution: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\\n\")\n",
    "            f.write(f\"üîß Mode: {'LLM S√©mantique' if llm else 'Mots-cl√©s enrichis'}\\n\")\n",
    "            f.write(f\"üìÅ Dossier source: {CHEMIN_SOURCE}\\n\")\n",
    "            f.write(f\"üìÅ Dossier d√©p√¥t: {CHEMIN_DEPOTS}\\n\\n\")\n",
    "            \n",
    "            f.write(\"üìä STATISTIQUES GLOBALES\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            f.write(f\"Total fichiers trait√©s: {len(analyses)}\\n\")\n",
    "            f.write(f\"Fichiers m√©dicaux: {rapport['statistiques_globales']['fichiers_medicaux']}\\n\")\n",
    "            f.write(f\"Fichiers clients: {rapport['statistiques_globales']['fichiers_clients']}\\n\")\n",
    "            f.write(f\"Fichiers bases donn√©es: {rapport['statistiques_globales']['fichiers_bases_donnees']}\\n\")\n",
    "            f.write(f\"Score s√©mantique moyen: {rapport['statistiques_globales']['score_semantique_moyen']}\\n\\n\")\n",
    "            \n",
    "            f.write(\"üìÅ R√âPARTITION PAR DOSSIERS\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            for dossier, count in sorted(stats_deplacement.items()):\n",
    "                pct = (count / len(analyses) * 100) if analyses else 0\n",
    "                f.write(f\"{dossier}: {count} fichiers ({pct:.1f}%)\\n\")\n",
    "            \n",
    "            f.write(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "            f.write(\"D√âTAIL DES CLASSIFICATIONS\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "            \n",
    "            # Grouper par dossier\n",
    "            analyses_par_dossier = {}\n",
    "            mapping_dossiers = {\n",
    "                'oncologie': 'ONCOLOGIE', 'cardiologie': 'CARDIOLOGIE', \n",
    "                'neurologie': 'NEUROLOGIE', 'diabetologie': 'DIABETOLOGIE',\n",
    "                'pneumologie': 'PNEUMOLOGIE', 'dermatologie': 'DERMATOLOGIE',\n",
    "                'pharmacovigilance': 'PHARMACOVIGILANCE', 'formation': 'FORMATION',\n",
    "                'commercial': 'COMMERCIAL', 'recherche': 'RECHERCHE',\n",
    "                'reglementaire': 'REGLEMENTAIRE', 'client': 'CLIENTS',\n",
    "                'base_donnees': 'BASES_DONNEES', 'autres': 'AUTRES', 'aucune': 'AUTRES'\n",
    "            }\n",
    "            \n",
    "            for analyse in analyses:\n",
    "                categorie = analyse.get('categorie_medicale', 'autres')\n",
    "                dossier = mapping_dossiers.get(categorie, 'AUTRES')\n",
    "                if dossier not in analyses_par_dossier:\n",
    "                    analyses_par_dossier[dossier] = []\n",
    "                analyses_par_dossier[dossier].append(analyse)\n",
    "            \n",
    "            for dossier, fichiers in sorted(analyses_par_dossier.items()):\n",
    "                f.write(f\"\\nüìÇ {dossier} ({len(fichiers)} fichiers)\\n\")\n",
    "                f.write(\"-\" * (len(dossier) + 20) + \"\\n\")\n",
    "                \n",
    "                for analyse in sorted(fichiers, key=lambda x: x.get('score_semantique', 0), reverse=True):\n",
    "                    f.write(f\"‚Ä¢ {analyse['nom_fichier']}\\n\")\n",
    "                    f.write(f\"  Taille: {analyse.get('taille_mb', 0):.1f} MB\\n\")\n",
    "                    f.write(f\"  Cat√©gorie: {analyse.get('categorie_medicale', 'N/A')}\\n\")\n",
    "                    f.write(f\"  Confiance: {analyse.get('confiance', 'N/A')}\\n\")\n",
    "                    f.write(f\"  Score: {analyse.get('score_semantique', 0):.2f}\\n\")\n",
    "                    \n",
    "                    if analyse.get('clients_detectes'):\n",
    "                        f.write(f\"  Clients: {', '.join(analyse['clients_detectes'])}\\n\")\n",
    "                    if analyse.get('bases_detectees'):\n",
    "                        f.write(f\"  Bases: {', '.join(analyse['bases_detectees'])}\\n\")\n",
    "                    if analyse.get('maladies_detectees'):\n",
    "                        f.write(f\"  Termes m√©dicaux: {', '.join(analyse['maladies_detectees'])}\\n\")\n",
    "                    if analyse.get('justification'):\n",
    "                        f.write(f\"  Justification: {analyse['justification']}\\n\")\n",
    "                    \n",
    "                    f.write(\"\\n\")\n",
    "        \n",
    "        print(f\"‚úÖ Rapport texte sauvegard√©: {rapport_txt_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur sauvegarde rapport texte: {e}\")\n",
    "    \n",
    "    return rapport\n",
    "\n",
    "def main():\n",
    "    \"\"\"Fonction principale - orchestration compl√®te\"\"\"\n",
    "    print(\"üè• CLASSIFICATEUR M√âDICAL IQVIA - ANALYSE S√âMANTIQUE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"üìÖ D√©marrage: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n",
    "    print()\n",
    "    \n",
    "    try:\n",
    "        # Phase 1: Recherche et filtrage\n",
    "        fichiers_info = rechercher_fichiers_filtres()\n",
    "        if not fichiers_info:\n",
    "            print(\"‚ùå Aucun fichier trouv√©. Arr√™t du processus.\")\n",
    "            return\n",
    "        \n",
    "        # Phase 1.5: Tri et analyse\n",
    "        fichiers_info = trier_fichiers_par_criteres(fichiers_info)\n",
    "        \n",
    "        # Phase 2: Copie\n",
    "        copied = copier_fichiers(fichiers_info)\n",
    "        if copied == 0:\n",
    "            print(\"‚ùå Aucun fichier copi√©. Arr√™t du processus.\")\n",
    "            return\n",
    "        \n",
    "        # Phase 3: Analyse s√©mantique\n",
    "        analyses = analyser_et_classer_semantique()\n",
    "        if not analyses:\n",
    "            print(\"‚ùå Aucune analyse r√©alis√©e. Arr√™t du processus.\")\n",
    "            return\n",
    "        \n",
    "        # Phase 4: D√©placement dans dossiers\n",
    "        stats_deplacement = creer_dossiers_et_deplacer(analyses)\n",
    "        \n",
    "        # Phase 5: Rapport final\n",
    "        rapport = generer_rapport_final(analyses, stats_deplacement)\n",
    "        \n",
    "        print(f\"\\nüéâ CLASSIFICATION TERMIN√âE AVEC SUCC√àS!\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"‚úÖ {len(analyses)} fichiers trait√©s\")\n",
    "        print(f\"üìÅ {len(stats_deplacement)} dossiers cr√©√©s\")\n",
    "        print(f\"üìã Rapports g√©n√©r√©s dans {CHEMIN_DEPOTS}\")\n",
    "        print(f\"‚è±Ô∏è Dur√©e totale: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå ERREUR CRITIQUE: {e}\")\n",
    "        print(\"üîß V√©rifiez la configuration et les chemins d'acc√®s\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
