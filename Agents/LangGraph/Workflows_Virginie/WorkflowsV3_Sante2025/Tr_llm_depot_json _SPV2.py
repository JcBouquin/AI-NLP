
#!/usr/bin/env python3
"""
Classificateur m√©dical avec analyse s√©mantique am√©lior√©e
Format simple - sans classes ni d√©pendances externes
"""

import os
import shutil
import json
from datetime import datetime, timedelta
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# Configuration
BASE_DIR = os.path.abspath(r"C:\Users\kosmo\pycode\Iqvia_process")
CHEMIN_SOURCE = os.path.join(BASE_DIR, "ProcessEx")
CHEMIN_DEPOTS = os.path.join(BASE_DIR, "Depots")

# Initialiser le LLM
api_key = os.getenv('OPENAI_API_KEY')
if not api_key:
    print("‚ö†Ô∏è ATTENTION: Cl√© API non trouv√©e dans les variables d'environnement")
    api_key = "votre_cl√©_api_openai_ici"  # √Ä remplacer par votre vraie cl√©

llm = None
if api_key != "votre_cl√©_api_openai_ici":
    try:
        llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.1,
            api_key=api_key
        )
    except Exception as e:
        print(f"‚ùå Erreur initialisation LLM: {e}")
        llm = None

def detection_mots_cles_medicaux(titre: str) -> dict:
    """D√©tection fallback  enrichie avec clients et bases de donn√©es"""
    

def analyser_titre_avec_llm_semantique(titre: str) -> dict:
    """Analyse s√©mantique avanc√©e avec LLM - prompt enrichi"""
    if not llm:
        print(f"‚ùå LLM non disponible, utilisation fallback pour '{titre[:30]}...'")
        return detection_mots_cles_medicaux(titre)
        
         try:
         1. Lecture du prompt depuis le fichier
        with open("Prompt_depot_json_sharepoint.txt", "r", encoding="utf-8") as file:
            human_prompt = file.read()

         2. Remplacement de l'ancien prompt par le contenu du fichier
        analysis_prompt_template = ChatPromptTemplate.from_messages([
            ("system", """Tu es un expert en analyse s√©mantique m√©dicale et business pharmaceutique.
            Ton r√¥le est de comprendre le CONTEXTE et l'INTENTION derri√®re chaque titre, pas seulement chercher des mots-cl√©s."""),
            ("human", human_prompt)
        ])
    
     
        
        prompt_value = analysis_prompt_template.invoke({"titre": titre})
        response = llm.invoke(prompt_value.to_messages())
        content = response.content.strip()
        
        # Nettoyer le JSON
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0]
        elif "```" in content:
            content = content.split("```")[1].split("```")[0]
        
        resultat = json.loads(content.strip())
        
        # Validation et enrichissement
        if "categorie_medicale" not in resultat:
            resultat["categorie_medicale"] = resultat.get("domaine_medical", "aucune")
        
        # Assurer la coh√©rence clients/bases de donn√©es
        if not resultat.get("clients_detectes"):
            resultat["clients_detectes"] = []
        if not resultat.get("bases_detectees"):
            resultat["bases_detectees"] = []
        
        # D√©terminer le client/base principal
        if resultat.get("clients_detectes") and not resultat.get("client_principal"):
            resultat["client_principal"] = resultat["clients_detectes"][0]
        if resultat.get("bases_detectees") and not resultat.get("base_principale"):
            resultat["base_principale"] = resultat["bases_detectees"][0]
        
        # Ajuster le contexte si client ou base d√©tect√©
        if resultat.get("client_principal"):
            resultat["contexte_principal"] = "client"
            resultat["categorie_medicale"] = "client"
        elif resultat.get("base_principale"):
            resultat["contexte_principal"] = "base_donnees"
            resultat["categorie_medicale"] = "base_donnees"
        
        # Calculer score s√©mantique si absent
        if "score_semantique" not in resultat or not isinstance(resultat["score_semantique"], (int, float)):
            contexte = resultat.get("contexte_principal", "autre")
            domaine = resultat.get("domaine_medical", "aucun")
            
            score = 0.3  # Base
            if contexte != "autre": score += 0.2
            if domaine != "aucun": score += 0.2
            if len(resultat.get("maladies_detectees", [])) > 0: score += 0.2
            if resultat.get("confiance") == "haute": score += 0.1
            
            resultat["score_semantique"] = min(0.95, score)
        
        return resultat
        
    except Exception as e:
        print(f"‚ùå Erreur LLM s√©mantique pour '{titre[:30]}...': {e}")
        return detection_mots_cles_medicaux(titre)

def rechercher_fichiers_filtres():
    """Phase 1: Recherche et filtre les fichiers par extension, date et taille"""
    print("üîç PHASE 1: Recherche et filtrage des fichiers...")
    print("-" * 50)
    
    if not os.path.exists(CHEMIN_SOURCE):
        print(f"‚ùå Erreur: {CHEMIN_SOURCE} n'existe pas")
        return []
    
    # D√©finition des filtres
    DATE_LIMITE = datetime.now() - timedelta(days=365)  # Fichiers de moins d'un an
    TAILLE_MIN_OCTETS = 1 * 1024         # 1 KB minimum (pour √©viter fichiers vides)
    TAILLE_MAX_OCTETS = 50 * 1024 * 1024  # 50 MB maximum
    TAILLE_CLASSIFICATION = 3000 * 1024   # 2 MB - Seuil pour classification s√©mantique
    
    print(f"üìÖ Filtre date: fichiers modifi√©s apr√®s le {DATE_LIMITE.strftime('%d/%m/%Y')}")
    print(f"üìè Filtre taille: entre {TAILLE_MIN_OCTETS//1024} KB et {TAILLE_MAX_OCTETS//1024//1024} MB")
    print(f"üß† Classification s√©mantique: fichiers > {TAILLE_CLASSIFICATION//1024} KB seulement")
    print(f"üìÅ Fichiers ‚â§ {TAILLE_CLASSIFICATION//1024} KB ‚Üí dossier AUTRES automatiquement")
    print()
    
    fichiers_trouves = []
    dossiers_stats = {}
    stats_filtrage = {
        'total_examines': 0,
        'rejetes_extension': 0,
        'rejetes_taille': 0, 
        'rejetes_date': 0,
        'rejetes_acces': 0,
        'acceptes': 0
    }
    
    for item in os.listdir(CHEMIN_SOURCE):
        item_path = os.path.join(CHEMIN_SOURCE, item)
        if os.path.isdir(item_path):
            print(f"üìÇ Analyse du dossier: {item}")
            count_dossier = 0
            
            for root, dirs, files in os.walk(item_path):
                for file in files:
                    file_path = os.path.join(root, file)
                    stats_filtrage['total_examines'] += 1
                    
                    # V√©rification de l'extension
                    if not file.lower().endswith(('.ppt', '.pptx', '.pdf')):
                        stats_filtrage['rejetes_extension'] += 1
                        continue
                    
                    try:
                        # R√©cup√©ration des m√©tadonn√©es
                        taille = os.path.getsize(file_path)
                        date_mod = datetime.fromtimestamp(os.path.getmtime(file_path))
                        
                        # Filtrage par taille
                        if not (TAILLE_MIN_OCTETS <= taille <= TAILLE_MAX_OCTETS):
                            stats_filtrage['rejetes_taille'] += 1
                            continue
                        
                        # Filtrage par date  
                        if date_mod < DATE_LIMITE:
                            stats_filtrage['rejetes_date'] += 1
                            continue
                        
                        # Fichier accept√©
                        fichier_info = {
                            'chemin': file_path,
                            'nom': file,
                            'taille': taille,
                            'date_modification': date_mod,
                            'taille_mb': round(taille / (1024*1024), 2),
                            'taille_kb': round(taille / 1024, 1),
                            'age_jours': (datetime.now() - date_mod).days,
                            'classification_semantique': taille >= TAILLE_CLASSIFICATION  # True si > 2MB
                        }
                        
                        fichiers_trouves.append(fichier_info)
                        count_dossier += 1
                        stats_filtrage['acceptes'] += 1
                        
                    except (OSError, PermissionError) as e:
                        stats_filtrage['rejetes_acces'] += 1
                        print(f"   ‚ö†Ô∏è Ignor√© (acc√®s refus√©): {os.path.basename(file_path)}")
            
            dossiers_stats[item] = count_dossier
            print(f"   ‚úÖ {count_dossier} fichier(s) retenu(s) apr√®s filtrage")
    
    # Affichage des statistiques d√©taill√©es
    print(f"\nüìä STATISTIQUES DE FILTRAGE:")
    print(f"   ‚Ä¢ Fichiers examin√©s: {stats_filtrage['total_examines']}")
    print(f"   ‚Ä¢ ‚ùå Rejet√©s - extension: {stats_filtrage['rejetes_extension']}")
    print(f"   ‚Ä¢ ‚ùå Rejet√©s - taille: {stats_filtrage['rejetes_taille']}")
    print(f"   ‚Ä¢ ‚ùå Rejet√©s - date: {stats_filtrage['rejetes_date']}")
    print(f"   ‚Ä¢ ‚ùå Rejet√©s - acc√®s: {stats_filtrage['rejetes_acces']}")
    print(f"   ‚Ä¢ ‚úÖ RETENUS: {stats_filtrage['acceptes']}")
    
    print(f"\nüìÅ R√âPARTITION PAR DOSSIER:")
    for dossier, count in dossiers_stats.items():
        print(f"   ‚Ä¢ {dossier}: {count} fichier(s)")
    
    return fichiers_trouves

def trier_fichiers_par_criteres(fichiers_info):
    """Trie les fichiers par diff√©rents crit√®res et affiche des statistiques"""
    print(f"\nüìà PHASE 1.5: Analyse et tri des fichiers retenus...")
    print("-" * 50)
    
    if not fichiers_info:
        print("‚ùå Aucun fichier √† analyser")
        return fichiers_info
    
    # Tri par taille (d√©croissant)
    fichiers_par_taille = sorted(fichiers_info, key=lambda x: x['taille'], reverse=True)
    print(f"üìè Top 5 fichiers les plus volumineux:")
    for i, f in enumerate(fichiers_par_taille[:5], 1):
        print(f"   {i}. {f['nom'][:50]}... ({f['taille_mb']} MB)")
    
    # Tri par date (plus r√©cent d'abord) 
    fichiers_par_date = sorted(fichiers_info, key=lambda x: x['date_modification'], reverse=True)
    print(f"\nüìÖ Top 5 fichiers les plus r√©cents:")
    for i, f in enumerate(fichiers_par_date[:5], 1):
        date_str = f['date_modification'].strftime('%d/%m/%Y')
        print(f"   {i}. {f['nom'][:50]}... ({date_str}, {f['age_jours']} jours)")
    
    # Statistiques de r√©partition
    tailles = [f['taille_mb'] for f in fichiers_info]
    ages = [f['age_jours'] for f in fichiers_info]
    
    print(f"\nüìä STATISTIQUES DESCRIPTIVES:")
    print(f"   Taille - Moyenne: {sum(tailles)/len(tailles):.1f} MB, Max: {max(tailles):.1f} MB, Min: {min(tailles):.1f} MB")
    print(f"   √Çge - Moyenne: {sum(ages)//len(ages)} jours, Max: {max(ages)} jours, Min: {min(ages)} jours")
    
    # R√©partition par tranches de taille et √©ligibilit√© classification
    tranches_taille = {'< 1MB': 0, '1-2MB': 0, '2-5MB': 0, '5-20MB': 0, '> 20MB': 0}
    eligibles_classification = 0
    
    for f in fichiers_info:
        mb = f['taille_mb']
        if mb < 1: tranches_taille['< 1MB'] += 1
        elif mb < 2: tranches_taille['1-2MB'] += 1  
        elif mb < 5: tranches_taille['2-5MB'] += 1
        elif mb < 20: tranches_taille['5-20MB'] += 1
        else: tranches_taille['> 20MB'] += 1
        
        if f['classification_semantique']:
            eligibles_classification += 1
    
    print(f"\nüìè R√âPARTITION PAR TAILLE:")
    for tranche, count in tranches_taille.items():
        pct = (count / len(fichiers_info) * 100) if fichiers_info else 0
        print(f"   ‚Ä¢ {tranche}: {count} fichiers ({pct:.1f}%)")
    
    print(f"\nüß† √âLIGIBILIT√â CLASSIFICATION S√âMANTIQUE:")
    print(f"   ‚Ä¢ Fichiers > 2MB (analys√©s): {eligibles_classification}")
    print(f"   ‚Ä¢ Fichiers ‚â§ 2MB (‚Üí AUTRES): {len(fichiers_info) - eligibles_classification}")
    
    return fichiers_info

def copier_fichiers(fichiers_info):
    """Phase 2: Copie vers Depots avec informations enrichies"""
    print(f"\nüìã PHASE 2: Copie vers {CHEMIN_DEPOTS}...")
    print("-" * 50)
    
    # Cr√©er le dossier Depots
    os.makedirs(CHEMIN_DEPOTS, exist_ok=True)
    print(f"‚úÖ Dossier Depots pr√™t")
    
    copied = 0
    taille_totale = 0
    
    for fichier_info in fichiers_info:
        file_path = fichier_info['chemin']
        file_name = fichier_info['nom']
        destination = os.path.join(CHEMIN_DEPOTS, file_name)
        
        try:
            shutil.copy(file_path, destination)
            copied += 1
            taille_totale += fichier_info['taille']
        except Exception as e:
            print(f"‚ùå Erreur copie {file_name}: {e}")
    
    print(f"‚úÖ {copied}/{len(fichiers_info)} fichiers copi√©s")
    print(f"üì¶ Taille totale copi√©e: {taille_totale/(1024*1024):.1f} MB")
    return copied

def analyser_et_classer_semantique():
    """Phase 3: Analyse s√©mantique LLM et classification avanc√©e (> 2MB seulement)"""
    print(f"\nüß† PHASE 3: Analyse s√©mantique et classification...")
    print("-" * 50)
    
    # Mode LLM ou fallback
    use_llm = llm is not None
    print(f"Mode: {'üî• LLM S√©mantique' if use_llm else 'üîß Mots-cl√©s enrichis'}")
    
    fichiers_a_analyser = [f for f in os.listdir(CHEMIN_DEPOTS) 
                          if f.lower().endswith(('.ppt', '.pptx', '.pdf'))]
    
    # R√©cup√©rer les infos de taille des fichiers copi√©s
    fichiers_avec_taille = []
    for file_name in fichiers_a_analyser:
        file_path = os.path.join(CHEMIN_DEPOTS, file_name)
        try:
            taille = os.path.getsize(file_path)
            fichiers_avec_taille.append({
                'nom': file_name,
                'taille': taille,
                'taille_mb': round(taille / (1024*1024), 2),
                'taille_kb': round(taille / 1024, 1),
                'eligible_classification': taille >= (3000 * 1024)  # > 2MB
            })
        except:
            # Si erreur, on consid√®re comme petit fichier
            fichiers_avec_taille.append({
                'nom': file_name,
                'taille': 0,
                'taille_mb': 0,
                'taille_kb': 0,
                'eligible_classification': False
            })
    
    analyses = []
    medicaux = 0
    contextes_stats = {}
    petits_fichiers = 0
    
    print(f"üìä R√©partition:")
    eligibles = sum(1 for f in fichiers_avec_taille if f['eligible_classification'])
    print(f"   ‚Ä¢ Fichiers > 2MB (analyse s√©mantique): {eligibles}")
    print(f"   ‚Ä¢ Fichiers ‚â§ 2MB (‚Üí AUTRES direct): {len(fichiers_avec_taille) - eligibles}")
    print()
    
    for i, fichier_info in enumerate(fichiers_avec_taille, 1):
        file_name = fichier_info['nom']
        titre = os.path.splitext(file_name)[0]
        taille_str = f"{fichier_info['taille_kb']} KB" if fichier_info['taille_mb'] < 1 else f"{fichier_info['taille_mb']} MB"
        
        print(f"[{i:2d}/{len(fichiers_avec_taille)}] {file_name[:40]}... ({taille_str})")
        
        # V√©rifier si √©ligible √† la classification s√©mantique
        if not fichier_info['eligible_classification']:
            # Fichier ‚â§ 2MB ‚Üí Dossier AUTRES automatiquement
            analyse = {
                'nom_fichier': file_name,
                'titre': titre,
                'taille_mb': fichier_info['taille_mb'],
                'contient_maladie': False,
                'categorie_medicale': 'autres',
                'contexte_principal': 'autre',
                'confiance': 'faible',
                'score_semantique': 0.1,
                'justification': f'Fichier de petite taille ({taille_str}) ‚Üí AUTRES automatiquement',
                'clients_detectes': [],
                'bases_detectees': [],
                'maladies_detectees': []
            }
            analyses.append(analyse)
            petits_fichiers += 1
            print(f"   üìÅ ‚Üí AUTRES (taille < 2MB)")
            continue
        
        # Classification s√©mantique pour fichiers > 2MB
        if use_llm:
            analyse = analyser_titre_avec_llm_semantique(titre)
        else:
            analyse = detection_mots_cles_medicaux(titre)
        
        # Enrichir avec m√©tadonn√©es
        analyse['nom_fichier'] = file_name
        analyse['taille_mb'] = fichier_info['taille_mb']
        
        # Compter et afficher r√©sultats
        if analyse.get('contient_maladie', False):
            medicaux += 1
        
        contexte = analyse.get('contexte_principal', 'autre')
        contextes_stats[contexte] = contextes_stats.get(contexte, 0) + 1
        
        # Affichage du r√©sultat
        categorie = analyse.get('categorie_medicale', 'aucune')
        confiance = analyse.get('confiance', 'inconnue')
        score = analyse.get('score_semantique', 0)
        
        if analyse.get('clients_detectes'):
            client_info = f" [Client: {', '.join(analyse['clients_detectes'])}]"
        elif analyse.get('bases_detectees'):
            client_info = f" [Base: {', '.join(analyse['bases_detectees'])}]"
        else:
            client_info = ""
        
        print(f"   üéØ {categorie.upper()} ({confiance}, score:{score:.2f}){client_info}")
        
        if analyse.get('maladies_detectees'):
            print(f"   üè• Termes: {', '.join(analyse['maladies_detectees'])}")
        
        analyses.append(analyse)
    
    print(f"\nüìä R√âSULTATS DE CLASSIFICATION:")
    print(f"   ‚Ä¢ Fichiers analys√©s s√©mantiquement: {len(analyses) - petits_fichiers}")
    print(f"   ‚Ä¢ Fichiers ‚Üí AUTRES (< 2MB): {petits_fichiers}")
    print(f"   ‚Ä¢ Fichiers m√©dicaux d√©tect√©s: {medicaux}")
    print(f"   ‚Ä¢ Score s√©mantique moyen: {sum(a.get('score_semantique', 0) for a in analyses) / len(analyses):.2f}")
    
    print(f"\nüìà R√âPARTITION PAR CONTEXTE:")
    for contexte, count in sorted(contextes_stats.items()):
        pct = (count / len(analyses) * 100) if analyses else 0
        print(f"   ‚Ä¢ {contexte}: {count} fichiers ({pct:.1f}%)")
    
    return analyses

def creer_dossiers_et_deplacer(analyses):
    """Phase 4: Cr√©ation des dossiers de classification et d√©placement"""
    print(f"\nüìÅ PHASE 4: Cr√©ation dossiers et d√©placement...")
    print("-" * 50)
    
    # Mapping des cat√©gories vers dossiers
    mapping_dossiers = {
    'oncologie': 'ONCOLOGIE',
    'cardiologie': 'CARDIOLOGIE',
    'neurologie': 'NEUROLOGIE',
    'diabetologie': 'DIABETOLOGIE',
    'pneumologie': 'PNEUMOLOGIE',
    'dermatologie': 'DERMATOLOGIE',
    'pharmacovigilance': 'PHARMACOVIGILANCE',
    # Contextes principaux
    'formation': 'FORMATION',
    'commercial': 'COMMERCIAL',
    'recherche': 'RECHERCHE',
    'reglementaire': 'REGLEMENTAIRE',
    'client': 'CLIENTS',
    'base_donnees': 'BASES_DONNEES',
    # Valeurs de repli
    'autre': 'AUTRES',
    'aucun': 'AUTRES',
    'null': 'AUTRES',
    'inconnu': 'AUTRES',
    'autres': 'AUTRES'
}
    
    # Cr√©er tous les dossiers n√©cessaires
    dossiers_crees = set()
    for categorie in mapping_dossiers.values():
        dossier_path = os.path.join(CHEMIN_DEPOTS, categorie)
        os.makedirs(dossier_path, exist_ok=True)
        dossiers_crees.add(categorie)
    
    print(f"‚úÖ {len(dossiers_crees)} dossiers de classification cr√©√©s")
    
    # D√©placer les fichiers
    stats_deplacement = {}
    erreurs_deplacement = []
    
    for analyse in analyses:
        file_name = analyse['nom_fichier']
        categorie = analyse.get('categorie_medicale', 'autres')
        dossier_cible = mapping_dossiers.get(categorie, 'AUTRES')
        
        source_path = os.path.join(CHEMIN_DEPOTS, file_name)
        destination_path = os.path.join(CHEMIN_DEPOTS, dossier_cible, file_name)
        
        try:
            if os.path.exists(source_path):
                # G√©rer les doublons
                if os.path.exists(destination_path):
                    base, ext = os.path.splitext(file_name)
                    counter = 1
                    while os.path.exists(destination_path):
                        new_name = f"{base}_({counter}){ext}"
                        destination_path = os.path.join(CHEMIN_DEPOTS, dossier_cible, new_name)
                        counter += 1
                
                shutil.move(source_path, destination_path)
                stats_deplacement[dossier_cible] = stats_deplacement.get(dossier_cible, 0) + 1
                
        except Exception as e:
            erreurs_deplacement.append(f"{file_name}: {str(e)}")
            print(f"‚ùå Erreur d√©placement {file_name}: {e}")
    
    print(f"\nüìä R√âSULTATS DE D√âPLACEMENT:")
    total_deplaces = sum(stats_deplacement.values())
    for dossier, count in sorted(stats_deplacement.items()):
        pct = (count / total_deplaces * 100) if total_deplaces > 0 else 0
        print(f"   ‚Ä¢ {dossier}: {count} fichiers ({pct:.1f}%)")
    
    if erreurs_deplacement:
        print(f"\n‚ùå ERREURS DE D√âPLACEMENT ({len(erreurs_deplacement)}):")
        for erreur in erreurs_deplacement[:5]:  # Limiter √† 5 erreurs
            print(f"   ‚Ä¢ {erreur}")
        if len(erreurs_deplacement) > 5:
            print(f"   ‚Ä¢ ... et {len(erreurs_deplacement) - 5} autres erreurs")
    
    return stats_deplacement

def generer_rapport_final(analyses, stats_deplacement):
    """Phase 5: G√©n√©ration du rapport final d√©taill√©"""
    print(f"\nüìã PHASE 5: G√©n√©ration du rapport final...")
    print("-" * 50)
    
    # Cr√©er le rapport JSON d√©taill√©
    rapport = {
        'metadata': {
            'date_execution': datetime.now().isoformat(),
            'version_script': '2.0',
            'mode_llm': llm is not None,
            'total_fichiers': len(analyses)
        },
        'statistiques_globales': {
            'fichiers_medicaux': sum(1 for a in analyses if a.get('contient_maladie', False)),
            'fichiers_clients': sum(1 for a in analyses if a.get('clients_detectes')),
            'fichiers_bases_donnees': sum(1 for a in analyses if a.get('bases_detectees')),
            'score_semantique_moyen': round(sum(a.get('score_semantique', 0) for a in analyses) / len(analyses), 3) if analyses else 0
        },
        'repartition_dossiers': stats_deplacement,
        'analyses_detaillees': analyses
    }
    
    # Sauvegarder le rapport JSON
    rapport_path = os.path.join(CHEMIN_DEPOTS, 'rapport_classification.json')
    try:
        with open(rapport_path, 'w', encoding='utf-8') as f:
            json.dump(rapport, f, indent=2, ensure_ascii=False, default=str)
        print(f"‚úÖ Rapport JSON sauvegard√©: {rapport_path}")
    except Exception as e:
        print(f"‚ùå Erreur sauvegarde rapport: {e}")
    
    # G√©n√©rer rapport texte lisible
    rapport_txt_path = os.path.join(CHEMIN_DEPOTS, 'rapport_classification.txt')
    try:
        with open(rapport_txt_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("RAPPORT DE CLASSIFICATION M√âDICALE AUTOMATIQUE\n")
            f.write("=" * 80 + "\n\n")
            
            f.write(f"üìÖ Date d'ex√©cution: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\n")
            f.write(f"üîß Mode: {'LLM S√©mantique' if llm else 'Mots-cl√©s enrichis'}\n")
            f.write(f"üìÅ Dossier source: {CHEMIN_SOURCE}\n")
            f.write(f"üìÅ Dossier d√©p√¥t: {CHEMIN_DEPOTS}\n\n")
            
            f.write("üìä STATISTIQUES GLOBALES\n")
            f.write("-" * 40 + "\n")
            f.write(f"Total fichiers trait√©s: {len(analyses)}\n")
            f.write(f"Fichiers m√©dicaux: {rapport['statistiques_globales']['fichiers_medicaux']}\n")
            f.write(f"Fichiers clients: {rapport['statistiques_globales']['fichiers_clients']}\n")
            f.write(f"Fichiers bases donn√©es: {rapport['statistiques_globales']['fichiers_bases_donnees']}\n")
            f.write(f"Score s√©mantique moyen: {rapport['statistiques_globales']['score_semantique_moyen']}\n\n")
            
            f.write("üìÅ R√âPARTITION PAR DOSSIERS\n")
            f.write("-" * 40 + "\n")
            for dossier, count in sorted(stats_deplacement.items()):
                pct = (count / len(analyses) * 100) if analyses else 0
                f.write(f"{dossier}: {count} fichiers ({pct:.1f}%)\n")
            
            f.write("\n" + "=" * 80 + "\n")
            f.write("D√âTAIL DES CLASSIFICATIONS\n")
            f.write("=" * 80 + "\n\n")
            
            # Grouper par dossier
            analyses_par_dossier = {}
            mapping_dossiers = {
                'oncologie': 'ONCOLOGIE', 'cardiologie': 'CARDIOLOGIE', 
                'neurologie': 'NEUROLOGIE', 'diabetologie': 'DIABETOLOGIE',
                'pneumologie': 'PNEUMOLOGIE', 'dermatologie': 'DERMATOLOGIE',
                'pharmacovigilance': 'PHARMACOVIGILANCE', 'formation': 'FORMATION',
                'commercial': 'COMMERCIAL', 'recherche': 'RECHERCHE',
                'reglementaire': 'REGLEMENTAIRE', 'client': 'CLIENTS',
                'base_donnees': 'BASES_DONNEES', 'autres': 'AUTRES', 'aucune': 'AUTRES'
            }
            
            for analyse in analyses:
                categorie = analyse.get('categorie_medicale', 'autres')
                dossier = mapping_dossiers.get(categorie, 'AUTRES')
                if dossier not in analyses_par_dossier:
                    analyses_par_dossier[dossier] = []
                analyses_par_dossier[dossier].append(analyse)
            
            for dossier, fichiers in sorted(analyses_par_dossier.items()):
                f.write(f"\nüìÇ {dossier} ({len(fichiers)} fichiers)\n")
                f.write("-" * (len(dossier) + 20) + "\n")
                
                for analyse in sorted(fichiers, key=lambda x: x.get('score_semantique', 0), reverse=True):
                    f.write(f"‚Ä¢ {analyse['nom_fichier']}\n")
                    f.write(f"  Taille: {analyse.get('taille_mb', 0):.1f} MB\n")
                    f.write(f"  Cat√©gorie: {analyse.get('categorie_medicale', 'N/A')}\n")
                    f.write(f"  Confiance: {analyse.get('confiance', 'N/A')}\n")
                    f.write(f"  Score: {analyse.get('score_semantique', 0):.2f}\n")
                    
                    if analyse.get('clients_detectes'):
                        f.write(f"  Clients: {', '.join(analyse['clients_detectes'])}\n")
                    if analyse.get('bases_detectees'):
                        f.write(f"  Bases: {', '.join(analyse['bases_detectees'])}\n")
                    if analyse.get('maladies_detectees'):
                        f.write(f"  Termes m√©dicaux: {', '.join(analyse['maladies_detectees'])}\n")
                    if analyse.get('justification'):
                        f.write(f"  Justification: {analyse['justification']}\n")
                    
                    f.write("\n")
        
        print(f"‚úÖ Rapport texte sauvegard√©: {rapport_txt_path}")
    except Exception as e:
        print(f"‚ùå Erreur sauvegarde rapport texte: {e}")
    
    return rapport

def main():
    """Fonction principale - orchestration compl√®te"""
    print("üè• CLASSIFICATEUR M√âDICAL IQVIA - ANALYSE S√âMANTIQUE")
    print("=" * 60)
    print(f"üìÖ D√©marrage: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}")
    print()
    
    try:
        # Phase 1: Recherche et filtrage
        fichiers_info = rechercher_fichiers_filtres()
        if not fichiers_info:
            print("‚ùå Aucun fichier trouv√©. Arr√™t du processus.")
            return
        
        # Phase 1.5: Tri et analyse
        fichiers_info = trier_fichiers_par_criteres(fichiers_info)
        
        # Phase 2: Copie
        copied = copier_fichiers(fichiers_info)
        if copied == 0:
            print("‚ùå Aucun fichier copi√©. Arr√™t du processus.")
            return
        
        # Phase 3: Analyse s√©mantique
        analyses = analyser_et_classer_semantique()
        if not analyses:
            print("‚ùå Aucune analyse r√©alis√©e. Arr√™t du processus.")
            return
        
        # Phase 4: D√©placement dans dossiers
        stats_deplacement = creer_dossiers_et_deplacer(analyses)
        
        # Phase 5: Rapport final
        rapport = generer_rapport_final(analyses, stats_deplacement)
        
        print(f"\nüéâ CLASSIFICATION TERMIN√âE AVEC SUCC√àS!")
        print("=" * 60)
        print(f"‚úÖ {len(analyses)} fichiers trait√©s")
        print(f"üìÅ {len(stats_deplacement)} dossiers cr√©√©s")
        print(f"üìã Rapports g√©n√©r√©s dans {CHEMIN_DEPOTS}")
        print(f"‚è±Ô∏è Dur√©e totale: {datetime.now().strftime('%H:%M:%S')}")
        
    except Exception as e:
        print(f"\n‚ùå ERREUR CRITIQUE: {e}")
        print("üîß V√©rifiez la configuration et les chemins d'acc√®s")
        raise

if __name__ == "__main__":

    main()
