{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db916746",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Classificateur m√©dical avec analyse s√©mantique am√©lior√©e\n",
    "Format simple - sans classes ni d√©pendances externes\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Configuration\n",
    "BASE_DIR = os.path.abspath(r\"C:\\Users\\kosmo\\pycode\\Iqvia_process\")\n",
    "CHEMIN_SOURCE = os.path.join(BASE_DIR, \"ProcessEx\")\n",
    "CHEMIN_DEPOTS = os.path.join(BASE_DIR, \"Depots\")\n",
    "\n",
    "# Initialiser le LLM\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not api_key:\n",
    "    print(\"‚ö†Ô∏è ATTENTION: Cl√© API non trouv√©e dans les variables d'environnement\")\n",
    "    api_key = \"votre_cl√©_api_openai_ici\"  # √Ä remplacer par votre vraie cl√©\n",
    "\n",
    "llm = None\n",
    "if api_key != \"votre_cl√©_api_openai_ici\":\n",
    "    try:\n",
    "        llm = ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0.1,\n",
    "            api_key=api_key\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur initialisation LLM: {e}\")\n",
    "        llm = None\n",
    "\n",
    "\n",
    "\n",
    "def analyser_titre_avec_llm_semantique(titre: str) -> dict:\n",
    "    \"\"\"Analyse s√©mantique avanc√©e avec LLM - prompt enrichi\"\"\"\n",
    "    if not llm:\n",
    "        print(f\"‚ùå LLM non disponible, impossible d'analyser '{titre[:30]}...'\")\n",
    "        return {\n",
    "            \"contient_maladie\": False,\n",
    "            \"maladies_detectees\": [],\n",
    "            \"categorie_medicale\": \"autres\",\n",
    "            \"contexte_principal\": \"autre\",\n",
    "            \"clients_detectes\": [],\n",
    "            \"bases_detectees\": [],\n",
    "            \"confiance\": \"faible\",\n",
    "            \"titre_normalise\": titre,\n",
    "            \"score_semantique\": 0.1\n",
    "        }\n",
    "         \n",
    "    \n",
    "    try:\n",
    "        analysis_prompt_template = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"Tu es un expert en analyse s√©mantique m√©dicale et business pharmaceutique. \n",
    "            Ton r√¥le est de comprendre le CONTEXTE et l'INTENTION derri√®re chaque titre, pas seulement chercher des mots-cl√©s.\n",
    "            \n",
    "             \n",
    "            \"\"\"),\n",
    "            (\"human\", \"\"\"\n",
    "Analyse s√©mantique COMPL√àTE du titre : \"{titre}\"\n",
    "\n",
    "Ne te contente pas de chercher des mots-cl√©s ! Analyse le CONTEXTE :\n",
    "\n",
    "Je commence par lire attentivement le titre pour en comprendre le contexte et la structure.\n",
    "             \n",
    "Exemple 1 :\n",
    "\n",
    "- \"3181844   pour ABBOTT DC_Suivi national du FSL2 par segment de pat diab√©tiques 2025\" \n",
    "               \n",
    "Je comprends que le terme ABBOTT est le nom d'un client , \"clients_detectes\" = ABBOTT  \n",
    "Je comprends que le terme diab√©tique est m√©dical , \"maladies_detectees\" = diabete \n",
    "Je comprends qu'il s'agit d'un document de type suivi , \"type_document\" = suivi \n",
    " \n",
    "             \n",
    "Json G√©n√©r√© : \n",
    "\n",
    "{{\n",
    "  \"contexte_principal\": \"Data\"\n",
    "  \"domaine_medical\": \"diabetologie\",\n",
    "  \"type_document\": \"suivi\",\n",
    "  \"client_detectees\": \"ABBOTT\",\n",
    "  \"base_detectees\": \"null\",\n",
    "  \"contient_maladie\": true,\n",
    "  \"confiance\": \"haute/moyenne/faible\",\n",
    "  \"score_semantique\": 0.8,\n",
    "  \"titre_normalise\": \"3181844 _ pour ABBOTT DC_Suivi national du FSL2 par segment de pat diab√©tiques 2025\"\n",
    "}}             \n",
    "\n",
    "Exemple 2 :\n",
    "\n",
    "Je commence par lire attentivement le titre pour en comprendre le contexte et la structure.\n",
    "\"3204851  pour AstraZeneca - Xponent EDS Benlysta 2025\"\n",
    "\n",
    " \n",
    "Json G√©n√©r√© : \n",
    "\n",
    "{{\n",
    "  \"contexte_principal\": \"Data\",\n",
    "  \"domaine_medical\": \"immunologie\",\n",
    "  \"type_document\": \"suivi\",\n",
    "  \"clients_detectes\": \"AstraZeneca\",\n",
    "  \"bases_detectees\": \"Xponent\",\n",
    "  \"contient_maladie\": true,\n",
    "  \"confiance\": \"haute\",\n",
    "  \"score_semantique\": 0.95,\n",
    "  \"justification\": \"Document de suivi bas√© sur la base Xponent pour le m√©dicament Benlysta (traitement du lupus), destin√© au client AstraZeneca dans un contexte commercial.\",\n",
    "  \"titre_normalise\": \"3204851   pour AstraZeneca - Xponent EDS Benlysta 2025\"\n",
    "}}                                        \n",
    "             \n",
    "Exemple 3 : \n",
    "\n",
    "Je commence par lire attentivement le titre pour en comprendre le contexte et la structure.\n",
    "\"PO-7101455030_v1_20241212\"\n",
    "\n",
    " \n",
    "Json G√©n√©r√© : \n",
    "\n",
    "{{\n",
    "  \"contexte_principal\": \"autre\",\n",
    "  \"domaine_medical\": \"null\",\n",
    "  \"type_document\": \"inconnu\",\n",
    "  \"clients_detectes\": \"null\",\n",
    "  \"bases_detectees\": \"null\",\n",
    "  \"contient_maladie\": false,\n",
    "  \"confiance\": \"faible\",\n",
    "  \"score_semantique\": 0.2,\n",
    "  \"justification\": \"Le titre est un identifiant technique sans indication explicite de client, maladie, base de donn√©es ou type de document. Impossible de d√©terminer un contexte clair.\",\n",
    "  \"titre_normalise\": \"PO-7101455030_v1_20241212\"\n",
    "}}\n",
    "\n",
    "R√©ponds UNIQUEMENT en JSON valide :\n",
    "{{\n",
    "    \"contexte_principal\": \"Data si client , base donn√©es ou domaine medical , autre si non \";\n",
    "    \"domaine_medical\": \"tous termes ou expression autour de la maladie ou null\",\n",
    "    \"type_document\": \"formation/etude/rapport/suivi/presentation/protocole/guide/autre\",\n",
    "    \"clients_detectes\": \"nom_client_principal_ou_null\",\n",
    "    \"bases_detectees\": \"Si IQVIA alors null ou nom_base_principale_ou_null Si IQVIA alors null \",\n",
    "    \"contient_maladie\": true,\n",
    "    \"confiance\": \"haute/moyenne/faible\",\n",
    "    \"score_semantique\": 0.8,\n",
    "    \"justification\": \"Explication courte du contexte d√©tect√©\",\n",
    "    \"titre_normalise\": \"{titre}\"\n",
    "}}\n",
    "            \"\"\")\n",
    "        ])\n",
    "        \n",
    "        prompt_value = analysis_prompt_template.invoke({\"titre\": titre})\n",
    "        response = llm.invoke(prompt_value.to_messages())\n",
    "        content = response.content.strip()\n",
    "        \n",
    "        # Nettoyer le JSON\n",
    "        if \"```json\" in content:\n",
    "            content = content.split(\"```json\")[1].split(\"```\")[0]\n",
    "        elif \"```\" in content:\n",
    "            content = content.split(\"```\")[1].split(\"```\")[0]\n",
    "        \n",
    "        resultat = json.loads(content.strip())\n",
    "        print(f\"üîç DEBUG resultat: {resultat}\")\n",
    "        \n",
    "        # Validation et enrichissement\n",
    "        if \"categorie_medicale\" not in resultat:\n",
    "            resultat[\"categorie_medicale\"] = \"autres\"  # Fallback par d√©faut\n",
    "\n",
    "         \n",
    "\n",
    "        # NOUVELLE logique simplifi√©e pour 4 dossiers - ORDRE CORRIG√â\n",
    "        if resultat.get(\"domaine_medical\") and resultat.get(\"domaine_medical\") != \"null\":\n",
    "            resultat[\"categorie_medicale\"] = \"domaine_medical\"                \n",
    "        elif resultat.get(\"bases_detectees\") and resultat.get(\"bases_detectees\") != \"null\":\n",
    "            resultat[\"categorie_medicale\"] = \"base_donnees\"          \n",
    "        elif resultat.get(\"clients_detectes\") and resultat.get(\"clients_detectes\") not in [\"null\", \"aucun\"]:\n",
    "            resultat[\"categorie_medicale\"] = \"client\"                                 \n",
    "        else:          \n",
    "            resultat[\"categorie_medicale\"] = \"autres\"\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        # Calculer score s√©mantique si absent\n",
    "        if \"score_semantique\" not in resultat or not isinstance(resultat[\"score_semantique\"], (int, float)):\n",
    "            contexte = resultat.get(\"contexte_principal\", \"autre\")\n",
    "            domaine = resultat.get(\"domaine_medical\", \"aucun\")\n",
    "            \n",
    "            score = 0.3  # Base\n",
    "            if contexte != \"autre\": score += 0.2\n",
    "            if domaine != \"aucun\": score += 0.2\n",
    "            if len(resultat.get(\"maladies_detectees\", [])) > 0: score += 0.2\n",
    "            if resultat.get(\"confiance\") == \"haute\": score += 0.1\n",
    "            \n",
    "            resultat[\"score_semantique\"] = min(0.95, score)\n",
    "        \n",
    "        return resultat\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur LLM s√©mantique pour '{titre[:30]}...': {e}\")\n",
    "        return {\n",
    "            \"contient_maladie\": False,\n",
    "            \"maladies_detectees\": [],\n",
    "            \"categorie_medicale\": \"autres\",\n",
    "            \"contexte_principal\": \"autre\",\n",
    "            \"clients_detectes\": [],\n",
    "            \"bases_detectees\": [],\n",
    "            \"confiance\": \"faible\",\n",
    "            \"titre_normalise\": titre,\n",
    "            \"score_semantique\": 0.1\n",
    "        }\n",
    "\n",
    "def rechercher_fichiers_filtres():\n",
    "    \"\"\"Phase 1: Recherche et filtre les fichiers par extension, date et taille\"\"\"\n",
    "    print(\"üîç PHASE 1: Recherche et filtrage des fichiers...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    if not os.path.exists(CHEMIN_SOURCE):\n",
    "        print(f\"‚ùå Erreur: {CHEMIN_SOURCE} n'existe pas\")\n",
    "        return []\n",
    "    \n",
    "    # D√©finition des filtres\n",
    "    DATE_LIMITE = datetime.now() - timedelta(days=365)  # Fichiers de moins d'un an\n",
    "    TAILLE_MIN_OCTETS = 1 * 1024         # 1 KB minimum (pour √©viter fichiers vides)\n",
    "    TAILLE_MAX_OCTETS = 50 * 1024 * 1024  # 50 MB maximum\n",
    "    TAILLE_CLASSIFICATION = 1000 * 1024   # 2 MB - Seuil pour classification s√©mantique\n",
    "    \n",
    "    print(f\"üìÖ Filtre date: fichiers modifi√©s apr√®s le {DATE_LIMITE.strftime('%d/%m/%Y')}\")\n",
    "    print(f\"üìè Filtre taille: entre {TAILLE_MIN_OCTETS//1024} KB et {TAILLE_MAX_OCTETS//1024//1024} MB\")\n",
    "    print(f\"üß† Classification s√©mantique: fichiers > {TAILLE_CLASSIFICATION//1024} KB seulement\")\n",
    "    print(f\"üìÅ Fichiers ‚â§ {TAILLE_CLASSIFICATION//1024} KB ‚Üí dossier AUTRES automatiquement\")\n",
    "    print()\n",
    "    \n",
    "    fichiers_trouves = []\n",
    "    dossiers_stats = {}\n",
    "    stats_filtrage = {\n",
    "        'total_examines': 0,\n",
    "        'rejetes_extension': 0,\n",
    "        'rejetes_taille': 0, \n",
    "        'rejetes_date': 0,\n",
    "        'rejetes_acces': 0,\n",
    "        'acceptes': 0\n",
    "    }\n",
    "    \n",
    "    for item in os.listdir(CHEMIN_SOURCE):\n",
    "        item_path = os.path.join(CHEMIN_SOURCE, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            print(f\"üìÇ Analyse du dossier: {item}\")\n",
    "            count_dossier = 0\n",
    "            \n",
    "            for root, dirs, files in os.walk(item_path):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    stats_filtrage['total_examines'] += 1\n",
    "                    \n",
    "                    # V√©rification de l'extension\n",
    "                    if not file.lower().endswith(('.ppt', '.pptx', '.pdf')):\n",
    "                        stats_filtrage['rejetes_extension'] += 1\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        # R√©cup√©ration des m√©tadonn√©es\n",
    "                        taille = os.path.getsize(file_path)\n",
    "                        date_mod = datetime.fromtimestamp(os.path.getmtime(file_path))\n",
    "                        \n",
    "                        # Filtrage par taille\n",
    "                        if not (TAILLE_MIN_OCTETS <= taille <= TAILLE_MAX_OCTETS):\n",
    "                            stats_filtrage['rejetes_taille'] += 1\n",
    "                            continue\n",
    "                        \n",
    "                        # Filtrage par date  \n",
    "                        if date_mod < DATE_LIMITE:\n",
    "                            stats_filtrage['rejetes_date'] += 1\n",
    "                            continue\n",
    "                        \n",
    "                        # Fichier accept√©\n",
    "                        fichier_info = {\n",
    "                            'chemin': file_path,\n",
    "                            'nom': file,\n",
    "                            'taille': taille,\n",
    "                            'date_modification': date_mod,\n",
    "                            'taille_mb': round(taille / (1024*1024), 2),\n",
    "                            'taille_kb': round(taille / 1024, 1),\n",
    "                            'age_jours': (datetime.now() - date_mod).days,\n",
    "                            'classification_semantique': taille >= TAILLE_CLASSIFICATION  # True si > 2MB\n",
    "                        }\n",
    "                        \n",
    "                        fichiers_trouves.append(fichier_info)\n",
    "                        count_dossier += 1\n",
    "                        stats_filtrage['acceptes'] += 1\n",
    "                        \n",
    "                    except (OSError, PermissionError) as e:\n",
    "                        stats_filtrage['rejetes_acces'] += 1\n",
    "                        print(f\"   ‚ö†Ô∏è Ignor√© (acc√®s refus√©): {os.path.basename(file_path)}\")\n",
    "            \n",
    "            dossiers_stats[item] = count_dossier\n",
    "            print(f\"   ‚úÖ {count_dossier} fichier(s) retenu(s) apr√®s filtrage\")\n",
    "    \n",
    "    # Affichage des statistiques d√©taill√©es\n",
    "    print(f\"\\nüìä STATISTIQUES DE FILTRAGE:\")\n",
    "    print(f\"   ‚Ä¢ Fichiers examin√©s: {stats_filtrage['total_examines']}\")\n",
    "    print(f\"   ‚Ä¢ ‚ùå Rejet√©s - extension: {stats_filtrage['rejetes_extension']}\")\n",
    "    print(f\"   ‚Ä¢ ‚ùå Rejet√©s - taille: {stats_filtrage['rejetes_taille']}\")\n",
    "    print(f\"   ‚Ä¢ ‚ùå Rejet√©s - date: {stats_filtrage['rejetes_date']}\")\n",
    "    print(f\"   ‚Ä¢ ‚ùå Rejet√©s - acc√®s: {stats_filtrage['rejetes_acces']}\")\n",
    "    print(f\"   ‚Ä¢ ‚úÖ RETENUS: {stats_filtrage['acceptes']}\")\n",
    "    \n",
    "    print(f\"\\nüìÅ R√âPARTITION PAR DOSSIER:\")\n",
    "    for dossier, count in dossiers_stats.items():\n",
    "        print(f\"   ‚Ä¢ {dossier}: {count} fichier(s)\")\n",
    "    \n",
    "    return fichiers_trouves\n",
    "\n",
    "def trier_fichiers_par_criteres(fichiers_info):\n",
    "    \"\"\"Trie les fichiers par diff√©rents crit√®res et affiche des statistiques\"\"\"\n",
    "    print(f\"\\nüìà PHASE 1.5: Analyse et tri des fichiers retenus...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    if not fichiers_info:\n",
    "        print(\"‚ùå Aucun fichier √† analyser\")\n",
    "        return fichiers_info\n",
    "    \n",
    "    # Tri par taille (d√©croissant)\n",
    "    fichiers_par_taille = sorted(fichiers_info, key=lambda x: x['taille'], reverse=True)\n",
    "    print(f\"üìè Top 5 fichiers les plus volumineux:\")\n",
    "    for i, f in enumerate(fichiers_par_taille[:5], 1):\n",
    "        print(f\"   {i}. {f['nom'][:50]}... ({f['taille_mb']} MB)\")\n",
    "    \n",
    "    # Tri par date (plus r√©cent d'abord) \n",
    "    fichiers_par_date = sorted(fichiers_info, key=lambda x: x['date_modification'], reverse=True)\n",
    "    print(f\"\\nüìÖ Top 5 fichiers les plus r√©cents:\")\n",
    "    for i, f in enumerate(fichiers_par_date[:5], 1):\n",
    "        date_str = f['date_modification'].strftime('%d/%m/%Y')\n",
    "        print(f\"   {i}. {f['nom'][:50]}... ({date_str}, {f['age_jours']} jours)\")\n",
    "    \n",
    "    # Statistiques de r√©partition\n",
    "    tailles = [f['taille_mb'] for f in fichiers_info]\n",
    "    ages = [f['age_jours'] for f in fichiers_info]\n",
    "    \n",
    "    print(f\"\\nüìä STATISTIQUES DESCRIPTIVES:\")\n",
    "    print(f\"   Taille - Moyenne: {sum(tailles)/len(tailles):.1f} MB, Max: {max(tailles):.1f} MB, Min: {min(tailles):.1f} MB\")\n",
    "    print(f\"   √Çge - Moyenne: {sum(ages)//len(ages)} jours, Max: {max(ages)} jours, Min: {min(ages)} jours\")\n",
    "    \n",
    "    # R√©partition par tranches de taille et √©ligibilit√© classification\n",
    "    tranches_taille = {'< 1MB': 0, '1-2MB': 0, '2-5MB': 0, '5-20MB': 0, '> 20MB': 0}\n",
    "    eligibles_classification = 0\n",
    "    \n",
    "    for f in fichiers_info:\n",
    "        mb = f['taille_mb']\n",
    "        if mb < 1: tranches_taille['< 1MB'] += 1\n",
    "        elif mb < 2: tranches_taille['1-2MB'] += 1  \n",
    "        elif mb < 5: tranches_taille['2-5MB'] += 1\n",
    "        elif mb < 20: tranches_taille['5-20MB'] += 1\n",
    "        else: tranches_taille['> 20MB'] += 1\n",
    "        \n",
    "        if f['classification_semantique']:\n",
    "            eligibles_classification += 1\n",
    "    \n",
    "    print(f\"\\nüìè R√âPARTITION PAR TAILLE:\")\n",
    "    for tranche, count in tranches_taille.items():\n",
    "        pct = (count / len(fichiers_info) * 100) if fichiers_info else 0\n",
    "        print(f\"   ‚Ä¢ {tranche}: {count} fichiers ({pct:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nüß† √âLIGIBILIT√â CLASSIFICATION S√âMANTIQUE:\")\n",
    "    print(f\"   ‚Ä¢ Fichiers > 2MB (analys√©s): {eligibles_classification}\")\n",
    "    print(f\"   ‚Ä¢ Fichiers ‚â§ 2MB (‚Üí AUTRES): {len(fichiers_info) - eligibles_classification}\")\n",
    "    \n",
    "    return fichiers_info\n",
    "\n",
    "def copier_fichiers(fichiers_info):\n",
    "    \"\"\"Phase 2: Copie vers Depots avec informations enrichies\"\"\"\n",
    "    print(f\"\\nüìã PHASE 2: Copie vers {CHEMIN_DEPOTS}...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Cr√©er le dossier Depots\n",
    "    os.makedirs(CHEMIN_DEPOTS, exist_ok=True)\n",
    "    print(f\"‚úÖ Dossier Depots pr√™t\")\n",
    "    \n",
    "    copied = 0\n",
    "    taille_totale = 0\n",
    "    \n",
    "    for fichier_info in fichiers_info:\n",
    "        file_path = fichier_info['chemin']\n",
    "        file_name = fichier_info['nom']\n",
    "        destination = os.path.join(CHEMIN_DEPOTS, file_name)\n",
    "        \n",
    "        try:\n",
    "            shutil.copy(file_path, destination)\n",
    "            copied += 1\n",
    "            taille_totale += fichier_info['taille']\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur copie {file_name}: {e}\")\n",
    "    \n",
    "    print(f\"‚úÖ {copied}/{len(fichiers_info)} fichiers copi√©s\")\n",
    "    print(f\"üì¶ Taille totale copi√©e: {taille_totale/(1024*1024):.1f} MB\")\n",
    "    return copied\n",
    "\n",
    "def analyser_et_classer_semantique():\n",
    "    \"\"\"Phase 3: Analyse s√©mantique LLM et classification avanc√©e (> 2MB seulement)\"\"\"\n",
    "    print(f\"\\nüß† PHASE 3: Analyse s√©mantique et classification...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Mode LLM ou fallback\n",
    "    use_llm = llm is not None\n",
    "    print(f\"Mode: {'üî• LLM S√©mantique' if use_llm else 'üîß Mots-cl√©s enrichis'}\")\n",
    "    \n",
    "    fichiers_a_analyser = [f for f in os.listdir(CHEMIN_DEPOTS) \n",
    "                          if f.lower().endswith(('.ppt', '.pptx', '.pdf'))]\n",
    "    \n",
    "    # R√©cup√©rer les infos de taille des fichiers copi√©s\n",
    "    fichiers_avec_taille = []\n",
    "    for file_name in fichiers_a_analyser:\n",
    "        file_path = os.path.join(CHEMIN_DEPOTS, file_name)\n",
    "        try:\n",
    "            taille = os.path.getsize(file_path)\n",
    "            fichiers_avec_taille.append({\n",
    "                'nom': file_name,\n",
    "                'taille': taille,\n",
    "                'taille_mb': round(taille / (1024*1024), 2),\n",
    "                'taille_kb': round(taille / 1024, 1),\n",
    "                'eligible_classification': taille >= (1000 * 1024)  # > 2MB\n",
    "            })\n",
    "        except:\n",
    "            # Si erreur, on consid√®re comme petit fichier\n",
    "            fichiers_avec_taille.append({\n",
    "                'nom': file_name,\n",
    "                'taille': 0,\n",
    "                'taille_mb': 0,\n",
    "                'taille_kb': 0,\n",
    "                'eligible_classification': False\n",
    "            })\n",
    "    \n",
    "    analyses = []\n",
    "    medicaux = 0\n",
    "    contextes_stats = {}\n",
    "    petits_fichiers = 0\n",
    "    \n",
    "    print(f\"üìä R√©partition:\")\n",
    "    eligibles = sum(1 for f in fichiers_avec_taille if f['eligible_classification'])\n",
    "    print(f\"   ‚Ä¢ Fichiers > 2MB (analyse s√©mantique): {eligibles}\")\n",
    "    print(f\"   ‚Ä¢ Fichiers ‚â§ 2MB (‚Üí AUTRES direct): {len(fichiers_avec_taille) - eligibles}\")\n",
    "    print()\n",
    "    \n",
    "    for i, fichier_info in enumerate(fichiers_avec_taille, 1):\n",
    "        file_name = fichier_info['nom']\n",
    "        titre = os.path.splitext(file_name)[0]\n",
    "        taille_str = f\"{fichier_info['taille_kb']} KB\" if fichier_info['taille_mb'] < 1 else f\"{fichier_info['taille_mb']} MB\"\n",
    "        \n",
    "        print(f\"[{i:2d}/{len(fichiers_avec_taille)}] {file_name[:40]}... ({taille_str})\")\n",
    "        \n",
    "        # V√©rifier si √©ligible √† la classification s√©mantique\n",
    "        if not fichier_info['eligible_classification']:\n",
    "            # Fichier ‚â§ 2MB ‚Üí Dossier AUTRES automatiquement\n",
    "            analyse = {\n",
    "                'nom_fichier': file_name,\n",
    "                'titre': titre,\n",
    "                'taille_mb': fichier_info['taille_mb'],\n",
    "                'contient_maladie': False,\n",
    "                'categorie_medicale': 'autres',\n",
    "                'contexte_principal': 'autre',\n",
    "                'confiance': 'faible',\n",
    "                'score_semantique': 0.1,\n",
    "                'justification': f'Fichier de petite taille ({taille_str}) ‚Üí AUTRES automatiquement',\n",
    "                'clients_detectes': [],\n",
    "                'bases_detectees': [],\n",
    "                'maladies_detectees': []\n",
    "            }\n",
    "            analyses.append(analyse)\n",
    "            petits_fichiers += 1\n",
    "            print(f\"   üìÅ ‚Üí AUTRES (taille < 2MB)\")\n",
    "            continue\n",
    "        \n",
    "        # Classification s√©mantique pour fichiers > 2MB\n",
    "        analyse = analyser_titre_avec_llm_semantique(titre)\n",
    "         \n",
    "        print(f\"üîç DEBUG ANALYSE: {analyse}\")\n",
    "        \n",
    "        # Enrichir avec m√©tadonn√©es\n",
    "        analyse['nom_fichier'] = file_name\n",
    "        analyse['taille_mb'] = fichier_info['taille_mb']\n",
    "        \n",
    "        # Compter et afficher r√©sultats\n",
    "        if analyse.get('contient_maladie', False):\n",
    "            medicaux += 1\n",
    "        \n",
    "        contexte = analyse.get('contexte_principal', 'autre')\n",
    "        contextes_stats[contexte] = contextes_stats.get(contexte, 0) + 1\n",
    "        \n",
    "        # Affichage du r√©sultat\n",
    "        categorie = analyse.get('categorie_medicale', 'aucune')\n",
    "        print(f\"üîç DEBUG !!!!!!!!!!!!!!!!!!!!! categorie: {categorie}\") \n",
    "      \n",
    "        confiance = analyse.get('confiance', 'inconnue')\n",
    "        score = analyse.get('score_semantique', 0)\n",
    "        \n",
    "        if analyse.get('clients_detectes'):\n",
    "            client_info = f\" [Client: {', '.join(analyse['clients_detectes'])}]\"\n",
    "        elif analyse.get('bases_detectees'):\n",
    "            client_info = f\" [Base: {', '.join(analyse['bases_detectees'])}]\"\n",
    "        else:\n",
    "            client_info = \"\"\n",
    "        \n",
    "        print(f\"   üéØ {categorie.upper()} ({confiance}, score:{score:.2f}){client_info}\")\n",
    "        \n",
    "        if analyse.get('maladies_detectees'):\n",
    "            print(f\"   üè• Termes: {', '.join(analyse['maladies_detectees'])}\")\n",
    "        \n",
    "        analyses.append(analyse)\n",
    "    \n",
    "    print(f\"\\nüìä R√âSULTATS DE CLASSIFICATION:\")\n",
    "    print(f\"   ‚Ä¢ Fichiers analys√©s s√©mantiquement: {len(analyses) - petits_fichiers}\")\n",
    "    print(f\"   ‚Ä¢ Fichiers ‚Üí AUTRES (< 2MB): {petits_fichiers}\")\n",
    "    print(f\"   ‚Ä¢ Fichiers m√©dicaux d√©tect√©s: {medicaux}\")\n",
    "    print(f\"   ‚Ä¢ Score s√©mantique moyen: {sum(a.get('score_semantique', 0) for a in analyses) / len(analyses):.2f}\")\n",
    "    \n",
    "    print(f\"\\nüìà R√âPARTITION PAR CONTEXTE:\")\n",
    "    for contexte, count in sorted(contextes_stats.items()):\n",
    "        pct = (count / len(analyses) * 100) if analyses else 0\n",
    "        print(f\"   ‚Ä¢ {contexte}: {count} fichiers ({pct:.1f}%)\")\n",
    "    \n",
    "    return analyses\n",
    "\n",
    "def creer_dossiers_et_deplacer(analyses):\n",
    "    \"\"\"Phase 4: Cr√©ation des dossiers de classification et d√©placement\"\"\"\n",
    "    print(f\"\\nüìÅ PHASE 4: Cr√©ation dossiers et d√©placement...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Mapping simplifi√© pour 4 dossiers\n",
    "    mapping_dossiers = {\n",
    "        'client': 'CLIENTS',\n",
    "        'base_donnees': 'BASES_DONNEES', \n",
    "        'domaine_medical': 'MALADIES',  # ‚Üê MAINTENANT √áA MARCHE !\n",
    "        'autres': 'AUTRES'\n",
    "    }\n",
    "    \n",
    "    # Cr√©er tous les dossiers n√©cessaires\n",
    "    dossiers_crees = set()\n",
    "    for categorie in mapping_dossiers.values():\n",
    "        dossier_path = os.path.join(CHEMIN_DEPOTS, categorie)\n",
    "        os.makedirs(dossier_path, exist_ok=True)\n",
    "        dossiers_crees.add(categorie)\n",
    "    \n",
    "    print(f\"‚úÖ {len(dossiers_crees)} dossiers de classification cr√©√©s\")\n",
    "    \n",
    "    # D√©placer les fichiers\n",
    "    stats_deplacement = {}\n",
    "    erreurs_deplacement = []\n",
    "    \n",
    "    for analyse in analyses:\n",
    "        file_name = analyse['nom_fichier']\n",
    "        categorie = analyse.get('categorie_medicale', 'autres')\n",
    "        dossier_cible = mapping_dossiers.get(categorie, 'AUTRES')\n",
    "        \n",
    "        source_path = os.path.join(CHEMIN_DEPOTS, file_name)\n",
    "        destination_path = os.path.join(CHEMIN_DEPOTS, dossier_cible, file_name)\n",
    "        \n",
    "        try:\n",
    "            if os.path.exists(source_path):\n",
    "                # G√©rer les doublons\n",
    "                if os.path.exists(destination_path):\n",
    "                    base, ext = os.path.splitext(file_name)\n",
    "                    counter = 1\n",
    "                    while os.path.exists(destination_path):\n",
    "                        new_name = f\"{base}_({counter}){ext}\"\n",
    "                        destination_path = os.path.join(CHEMIN_DEPOTS, dossier_cible, new_name)\n",
    "                        counter += 1\n",
    "                \n",
    "                shutil.move(source_path, destination_path)\n",
    "                stats_deplacement[dossier_cible] = stats_deplacement.get(dossier_cible, 0) + 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            erreurs_deplacement.append(f\"{file_name}: {str(e)}\")\n",
    "            print(f\"‚ùå Erreur d√©placement {file_name}: {e}\")\n",
    "    \n",
    "    print(f\"\\nüìä R√âSULTATS DE D√âPLACEMENT:\")\n",
    "    total_deplaces = sum(stats_deplacement.values())\n",
    "    for dossier, count in sorted(stats_deplacement.items()):\n",
    "        pct = (count / total_deplaces * 100) if total_deplaces > 0 else 0\n",
    "        print(f\"   ‚Ä¢ {dossier}: {count} fichiers ({pct:.1f}%)\")\n",
    "    \n",
    "    if erreurs_deplacement:\n",
    "        print(f\"\\n‚ùå ERREURS DE D√âPLACEMENT ({len(erreurs_deplacement)}):\")\n",
    "        for erreur in erreurs_deplacement[:5]:  # Limiter √† 5 erreurs\n",
    "            print(f\"   ‚Ä¢ {erreur}\")\n",
    "        if len(erreurs_deplacement) > 5:\n",
    "            print(f\"   ‚Ä¢ ... et {len(erreurs_deplacement) - 5} autres erreurs\")\n",
    "    \n",
    "    return stats_deplacement\n",
    "\n",
    "def consolider_resultats_globaux():\n",
    "    \"\"\"Phase 6: Consolidation de tous les fichiers class√©s dans un dossier global\"\"\"\n",
    "    print(f\"\\nüìÅ PHASE 6: Consolidation dans results_global...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Cr√©er le dossier results_global\n",
    "    results_path = os.path.join(CHEMIN_DEPOTS, \"results_global\")\n",
    "    os.makedirs(results_path, exist_ok=True)\n",
    "    \n",
    "    # Liste des dossiers √† consolider (tous sauf results_global)\n",
    "    dossiers_a_consolider = [\n",
    "        'CLIENTS', 'BASES_DONNEES', 'MALADIES' # ‚Üê 3 dossiers seulement\n",
    "    ]\n",
    "    \n",
    "    fichiers_copies = 0\n",
    "    erreurs_copie = []\n",
    "    \n",
    "    for dossier in dossiers_a_consolider:\n",
    "        dossier_path = os.path.join(CHEMIN_DEPOTS, dossier)\n",
    "        \n",
    "        if os.path.exists(dossier_path):\n",
    "            print(f\"üìÇ Consolidation de {dossier}...\")\n",
    "            \n",
    "            for file_name in os.listdir(dossier_path):\n",
    "                if file_name.lower().endswith(('.ppt', '.pptx', '.pdf')):\n",
    "                    source = os.path.join(dossier_path, file_name)\n",
    "                    destination = os.path.join(results_path, file_name)\n",
    "                    \n",
    "                    try:\n",
    "                        # G√©rer les doublons avec pr√©fixe du dossier source\n",
    "                        if os.path.exists(destination):\n",
    "                            name, ext = os.path.splitext(file_name)\n",
    "                            new_name = f\"{dossier}_{name}{ext}\"\n",
    "                            destination = os.path.join(results_path, new_name)\n",
    "                        \n",
    "                        shutil.copy2(source, destination)  # copy2 pr√©serve les m√©tadonn√©es\n",
    "                        fichiers_copies += 1\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        erreurs_copie.append(f\"{dossier}/{file_name}: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Consolidation termin√©e:\")\n",
    "    print(f\"   ‚Ä¢ {fichiers_copies} fichiers copi√©s dans {results_path}\")\n",
    "    print(f\"   ‚Ä¢ {len(erreurs_copie)} erreurs\" if erreurs_copie else \"   ‚Ä¢ Aucune erreur\")\n",
    "    \n",
    "    if erreurs_copie:\n",
    "        print(f\"\\n‚ùå ERREURS DE COPIE:\")\n",
    "        for erreur in erreurs_copie[:3]:\n",
    "            print(f\"   ‚Ä¢ {erreur}\")\n",
    "        if len(erreurs_copie) > 3:\n",
    "            print(f\"   ‚Ä¢ ... et {len(erreurs_copie) - 3} autres erreurs\")\n",
    "    \n",
    "    return fichiers_copies\n",
    "\n",
    "def generer_rapport_final(analyses, stats_deplacement):\n",
    "    \"\"\"Phase 5: G√©n√©ration du rapport final d√©taill√©\"\"\"\n",
    "    print(f\"\\nüìã PHASE 5: G√©n√©ration du rapport final...\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    TAILLE_CLASSIFICATION_MB = 1000 / 1024  # Convertir 1000 KB en MB = 0.98 MB\n",
    "    \n",
    "    # Cr√©er le rapport JSON d√©taill√©\n",
    "    rapport = {\n",
    "        'metadata': {\n",
    "            'date_execution': datetime.now().isoformat(),\n",
    "            'version_script': '2.0',\n",
    "            'mode_llm': llm is not None,\n",
    "            'total_fichiers': len(analyses)\n",
    "        },\n",
    "        'statistiques_globales': {\n",
    "            'fichiers_medicaux': sum(1 for a in analyses if a.get('contient_maladie', False)),\n",
    "            'fichiers_clients': sum(1 for a in analyses if a.get('clients_detectes')),\n",
    "            'fichiers_bases_donnees': sum(1 for a in analyses if a.get('bases_detectees')),\n",
    "            'score_semantique_moyen': round(sum(a.get('score_semantique', 0) for a in analyses) / len(analyses), 3) if analyses else 0\n",
    "        },\n",
    "        'repartition_dossiers': stats_deplacement,\n",
    "        'analyses_detaillees': [a for a in analyses if a.get('taille_mb', 0) >= TAILLE_CLASSIFICATION_MB]   \n",
    "    }\n",
    "    \n",
    "    # Sauvegarder le rapport JSON\n",
    "    rapport_path = os.path.join(CHEMIN_DEPOTS, 'rapport_classification.json')\n",
    "    try:\n",
    "        with open(rapport_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(rapport, f, indent=2, ensure_ascii=False, default=str)\n",
    "        print(f\"‚úÖ Rapport JSON sauvegard√©: {rapport_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur sauvegarde rapport: {e}\")\n",
    "    \n",
    "    # G√©n√©rer rapport texte lisible\n",
    "    rapport_txt_path = os.path.join(CHEMIN_DEPOTS, 'rapport_classification.txt')\n",
    "    try:\n",
    "        with open(rapport_txt_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"=\" * 80 + \"\\n\")\n",
    "            f.write(\"RAPPORT DE CLASSIFICATION M√âDICALE AUTOMATIQUE\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "            \n",
    "            f.write(f\"üìÖ Date d'ex√©cution: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\\n\")\n",
    "            f.write(f\"üîß Mode: {'LLM S√©mantique' if llm else 'Mots-cl√©s enrichis'}\\n\")\n",
    "            f.write(f\"üìÅ Dossier source: {CHEMIN_SOURCE}\\n\")\n",
    "            f.write(f\"üìÅ Dossier d√©p√¥t: {CHEMIN_DEPOTS}\\n\\n\")\n",
    "            \n",
    "            f.write(\"üìä STATISTIQUES GLOBALES\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            f.write(f\"Total fichiers trait√©s: {len(analyses)}\\n\")\n",
    "            f.write(f\"Fichiers m√©dicaux: {rapport['statistiques_globales']['fichiers_medicaux']}\\n\")\n",
    "            f.write(f\"Fichiers clients: {rapport['statistiques_globales']['fichiers_clients']}\\n\")\n",
    "            f.write(f\"Fichiers bases donn√©es: {rapport['statistiques_globales']['fichiers_bases_donnees']}\\n\")\n",
    "            f.write(f\"Score s√©mantique moyen: {rapport['statistiques_globales']['score_semantique_moyen']}\\n\\n\")\n",
    "            \n",
    "            f.write(\"üìÅ R√âPARTITION PAR DOSSIERS\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            for dossier, count in sorted(stats_deplacement.items()):\n",
    "                pct = (count / len(analyses) * 100) if analyses else 0\n",
    "                f.write(f\"{dossier}: {count} fichiers ({pct:.1f}%)\\n\")\n",
    "            \n",
    "            # Comptage du dossier results_global\n",
    "            results_global_path = os.path.join(CHEMIN_DEPOTS, \"results_global\")\n",
    "            if os.path.exists(results_global_path):\n",
    "                fichiers_consolides = len([f for f in os.listdir(results_global_path) \n",
    "                                         if f.lower().endswith(('.ppt', '.pptx', '.pdf'))])\n",
    "                f.write(f\"\\nüìÇ CONSOLIDATION GLOBALE\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\n\")\n",
    "                f.write(f\"RESULTS_GLOBAL: {fichiers_consolides} fichiers consolid√©s\\n\")\n",
    "        \n",
    "        print(f\"‚úÖ Rapport texte sauvegard√©: {rapport_txt_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur sauvegarde rapport texte: {e}\")\n",
    "    \n",
    "    return rapport\n",
    "\n",
    "def deposer_vers_sharepoint():\n",
    "    \"\"\"Phase 7: D√©p√¥t du dossier results_global vers SharePoint\"\"\"\n",
    "    print(f\"\\n‚òÅÔ∏è PHASE 7: D√©p√¥t vers SharePoint...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        from office365.runtime.auth.authentication_context import AuthenticationContext\n",
    "        from office365.sharepoint.client_context import ClientContext\n",
    "        import requests\n",
    "        \n",
    "        # Configuration SharePoint (√† adapter selon vos param√®tres)\n",
    "        SHAREPOINT_URL = \"https://votreentreprise.sharepoint.com/sites/votre-site\"\n",
    "        SHAREPOINT_USERNAME = os.getenv('SHAREPOINT_USERNAME')\n",
    "        SHAREPOINT_PASSWORD = os.getenv('SHAREPOINT_PASSWORD')\n",
    "        SHAREPOINT_LIBRARY = \"Documents Partag√©s/Classification_Medicale\"  # Dossier de destination\n",
    "        \n",
    "        # V√©rifier les param√®tres\n",
    "        if not all([SHAREPOINT_USERNAME, SHAREPOINT_PASSWORD]):\n",
    "            print(\"‚ùå Erreur: Variables d'environnement SharePoint manquantes\")\n",
    "            print(\"   D√©finissez SHAREPOINT_USERNAME et SHAREPOINT_PASSWORD\")\n",
    "            return False\n",
    "        \n",
    "        # Chemin du dossier local √† uploader\n",
    "        results_path = os.path.join(CHEMIN_DEPOTS, \"results_global\")\n",
    "        if not os.path.exists(results_path):\n",
    "            print(f\"‚ùå Erreur: Dossier {results_path} n'existe pas\")\n",
    "            return False\n",
    "        \n",
    "        # Authentification SharePoint\n",
    "        print(\"üîê Authentification SharePoint...\")\n",
    "        auth_context = AuthenticationContext(url=SHAREPOINT_URL)\n",
    "        auth_context.acquire_token_for_user(username=SHAREPOINT_USERNAME, password=SHAREPOINT_PASSWORD)\n",
    "        \n",
    "        ctx = ClientContext(SHAREPOINT_URL, auth_context)\n",
    "        web = ctx.web\n",
    "        ctx.load(web)\n",
    "        ctx.execute_query()\n",
    "        \n",
    "        print(f\"‚úÖ Connect√© √† SharePoint: {web.properties['Title']}\")\n",
    "        \n",
    "        # Cr√©er le dossier de destination si n√©cessaire\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        destination_folder = f\"{SHAREPOINT_LIBRARY}/Classification_{timestamp}\"\n",
    "        \n",
    "        # Upload des fichiers\n",
    "        fichiers_uploades = 0\n",
    "        erreurs_upload = []\n",
    "        \n",
    "        print(f\"üìÇ Upload vers: {destination_folder}\")\n",
    "        \n",
    "        for file_name in os.listdir(results_path):\n",
    "            if file_name.lower().endswith(('.ppt', '.pptx', '.pdf')):\n",
    "                file_path = os.path.join(results_path, file_name)\n",
    "                \n",
    "                try:\n",
    "                    with open(file_path, 'rb') as file_content:\n",
    "                        # Upload du fichier\n",
    "                        target_folder = ctx.web.get_folder_by_server_relative_url(destination_folder)\n",
    "                        target_folder.upload_file(file_name, file_content.read())\n",
    "                        ctx.execute_query()\n",
    "                        \n",
    "                        fichiers_uploades += 1\n",
    "                        print(f\"   ‚úÖ {file_name}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    erreurs_upload.append(f\"{file_name}: {str(e)}\")\n",
    "                    print(f\"   ‚ùå {file_name}: {e}\")\n",
    "        \n",
    "        print(f\"\\nüìä R√âSULTATS UPLOAD SHAREPOINT:\")\n",
    "        print(f\"   ‚Ä¢ {fichiers_uploades} fichiers upload√©s\")\n",
    "        print(f\"   ‚Ä¢ {len(erreurs_upload)} erreurs\")\n",
    "        print(f\"   ‚Ä¢ Destination: {destination_folder}\")\n",
    "        \n",
    "        if erreurs_upload:\n",
    "            print(f\"\\n‚ùå ERREURS D'UPLOAD:\")\n",
    "            for erreur in erreurs_upload[:3]:\n",
    "                print(f\"   ‚Ä¢ {erreur}\")\n",
    "            if len(erreurs_upload) > 3:\n",
    "                print(f\"   ‚Ä¢ ... et {len(erreurs_upload) - 3} autres erreurs\")\n",
    "        \n",
    "        return fichiers_uploades > 0\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ùå Erreur: Biblioth√®que Office365 manquante\")\n",
    "        print(\"   Installez avec: pip install Office365-REST-Python-Client\")\n",
    "        return False\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur SharePoint: {e}\")\n",
    "        return False\n",
    "    \n",
    "def main():\n",
    "    \"\"\"Fonction principale - orchestration compl√®te\"\"\"\n",
    "    print(\"üè• CLASSIFICATEUR M√âDICAL   - ANALYSE S√âMANTIQUE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"üìÖ D√©marrage: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n",
    "    print()\n",
    "    \n",
    "    try:\n",
    "        # Phase 1: Recherche et filtrage\n",
    "        fichiers_info = rechercher_fichiers_filtres()\n",
    "        if not fichiers_info:\n",
    "            print(\"‚ùå Aucun fichier trouv√©. Arr√™t du processus.\")\n",
    "            return\n",
    "        \n",
    "        # Phase 1.5: Tri et analyse\n",
    "        fichiers_info = trier_fichiers_par_criteres(fichiers_info)\n",
    "        \n",
    "        # Phase 2: Copie\n",
    "        copied = copier_fichiers(fichiers_info)\n",
    "        if copied == 0:\n",
    "            print(\"‚ùå Aucun fichier copi√©. Arr√™t du processus.\")\n",
    "            return\n",
    "        \n",
    "        # Phase 3: Analyse s√©mantique\n",
    "        analyses = analyser_et_classer_semantique()\n",
    "        if not analyses:\n",
    "            print(\"‚ùå Aucune analyse r√©alis√©e. Arr√™t du processus.\")\n",
    "            return\n",
    "        \n",
    "        # Phase 4: D√©placement dans dossiers\n",
    "        stats_deplacement = creer_dossiers_et_deplacer(analyses)\n",
    "\n",
    "        # Phase 5: Consolidation\n",
    "        fichiers_consolides = consolider_resultats_globaux()\n",
    "        \n",
    "        # Phase 6: D√©p√¥t SharePoint (d√©commentez quand les acc√®s seront configur√©s)\n",
    "        # success_sharepoint = deposer_vers_sharepoint()\n",
    "\n",
    "        # Phase 7: Rapport final\n",
    "        rapport = generer_rapport_final(analyses, stats_deplacement)\n",
    "\n",
    "        \n",
    "        print(f\"\\nüéâ CLASSIFICATION TERMIN√âE AVEC SUCC√àS!\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"‚úÖ {len(analyses)} fichiers trait√©s\")\n",
    "        print(f\"üìÅ {len(stats_deplacement)} dossiers cr√©√©s\")\n",
    "        print(f\"üìÇ {fichiers_consolides} fichiers consolid√©s\")\n",
    "        print(f\"üìã Rapports g√©n√©r√©s dans {CHEMIN_DEPOTS}\")\n",
    "        print(f\"‚è±Ô∏è Dur√©e totale: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå ERREUR CRITIQUE: {e}\")\n",
    "        print(\"üîß V√©rifiez la configuration et les chemins d'acc√®s\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
