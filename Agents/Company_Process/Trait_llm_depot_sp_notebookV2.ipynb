{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0426806",
   "metadata": {
    "id": "d0426806"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "Ce notebook impl√©mente l'analyse s√©mantique avec un mod√®le de langage (LLM) pour organiser selon leur titre des documents .\n",
    "\n",
    "#### Objectifs\n",
    "\n",
    "- **Automatiser** la classification de documents m√©dicaux (PDF, PPT, PPTX)\n",
    "- **Identifier** automatiquement les clients, bases de donn√©es et domaines m√©dicaux\n",
    "- **Organiser** les fichiers en 4 cat√©gories principales : CLIENTS, BASES_DONNEES, MALADIES, AUTRES\n",
    "- **G√©n√©rer** des rapports d√©taill√©s avec justifications s√©mantiques\n",
    "- **Consolider** tous les fichiers pertinents dans un dossier global\n",
    "\n",
    "#### Workflow du Processus\n",
    "\n",
    "1. **Phase 1** : Recherche et filtrage des fichiers (extension, date, taille)\n",
    "2. **Phase 2** : Copie vers le dossier de travail\n",
    "3. **Phase 3** : Analyse s√©mantique avec LLM (GPT-4o-mini)\n",
    "4. **Phase 4** : Classification et d√©placement dans les dossiers\n",
    "5. **Phase 5** : Consolidation dans `results_global`\n",
    "6. **Phase 6** : G√©n√©ration des rapports JSON et TXT\n",
    "7. **Phase 7** : D√©p√¥t SharePoint (optionnel)\n",
    "\n",
    "\n",
    "Le syst√®me utilise **GPT-4o-mini** pour :\n",
    "- Comprendre le **contexte** les titres de documents\n",
    "- D√©tecter les **clients pharmaceutiques** (ABBVIE, IQVIA, etc.)\n",
    "- Identifier les **bases de donn√©es** (Xponent, MIDAS, etc.)\n",
    "- Reconna√Ætre les **domaines m√©dicaux** (oncologie, diab√©tologie, etc.)\n",
    "- Fournir une **justification** pour chaque classification\n",
    "\n",
    "#### Avantages par rapport aux m√©thodes classiques\n",
    "\n",
    "- **Flexibilit√©** : S'adapte automatiquement aux nouveaux termes\n",
    "- **Contexte** : Comprend l'intention derri√®re les titres\n",
    "- **Tra√ßabilit√©** : Justification de chaque d√©cision\n",
    "- **Pr√©cision** : Classification multi-crit√®res intelligente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XiCJ2WftfPa9",
   "metadata": {
    "collapsed": true,
    "id": "XiCJ2WftfPa9"
   },
   "outputs": [],
   "source": [
    "pip install  langchain-openai langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VZ_3svpZfR3d",
   "metadata": {
    "id": "VZ_3svpZfR3d"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "clG65qf1it0u",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18744,
     "status": "ok",
     "timestamp": 1755861962332,
     "user": {
      "displayName": "Jean-Christophe Bouquin",
      "userId": "01239462757709491259"
     },
     "user_tz": -120
    },
    "id": "clG65qf1it0u",
    "outputId": "553a056d-ba1a-40ab-bd5e-9b57af67b7c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "436a9287",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3422,
     "status": "ok",
     "timestamp": 1755862017912,
     "user": {
      "displayName": "Jean-Christophe Bouquin",
      "userId": "01239462757709491259"
     },
     "user_tz": -120
    },
    "id": "436a9287",
    "outputId": "f40aee7e-6ab5-43b8-959a-a74bdd1cd829"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports r√©alis√©s avec succ√®s\n",
      "üìÅ Dossier source: C:\\Users\\kosmo\\pycode\\Iqvia_process\\ProcessEx\n",
      "üìÅ Dossier d√©p√¥t: C:\\Users\\kosmo\\pycode\\Iqvia_process\\Depots\n",
      "‚úÖ Chemins configur√©s\n",
      "‚úÖ Fichier prompt lu avec succ√®s, taille: 3008 caract√®res\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "\n",
    "# ================================\n",
    "# üì¶ IMPORTS DES BIBLIOTH√àQUES\n",
    "# ================================\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "print(\"‚úÖ Imports r√©alis√©s avec succ√®s\")\n",
    "\n",
    "# ================================\n",
    "# üìÅ CONFIGURATION DES CHEMINS\n",
    "# ================================\n",
    "\n",
    "# Configuration des chemins\n",
    "BASE_DIR = os.path.abspath(r\"C:\\Users\\kosmo\\pycode\\Iqvia_process\")\n",
    "CHEMIN_SOURCE = os.path.join(BASE_DIR, \"ProcessEx\")\n",
    "CHEMIN_DEPOTS = os.path.join(BASE_DIR, \"Depots\")\n",
    "\n",
    "print(f\"üìÅ Dossier source: {CHEMIN_SOURCE}\")\n",
    "print(f\"üìÅ Dossier d√©p√¥t: {CHEMIN_DEPOTS}\")\n",
    "print(f\"‚úÖ Chemins configur√©s\")\n",
    "\n",
    "# ================================\n",
    "# üìÑ LECTURE DU FICHIER PROMPT\n",
    "# ================================\n",
    "\n",
    "try:\n",
    "    with open(\n",
    "        os.path.join(BASE_DIR, \"ProcessEx\", \"Prompt_depot_json_sharepoint.txt\"),\n",
    "        \"r\",\n",
    "        encoding=\"utf-8\",\n",
    "    ) as file:\n",
    "        prompt = file.read()\n",
    "    print(f\"‚úÖ Fichier prompt lu avec succ√®s, taille: {len(prompt)} caract√®res\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Fichier Prompt_depot_json_sharepoint.txt non trouv√©\")\n",
    "    prompt = \"\"\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur lors de la lecture du fichier: {e}\")\n",
    "    prompt = \"\"\n",
    "\n",
    "# ================================\n",
    "# üß† INITIALISATION DU LLM\n",
    "# ================================\n",
    "\n",
    "# R√©cup√©ration de la cl√© API\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not api_key:\n",
    "   print(\"‚ö†Ô∏è ATTENTION: Cl√© API non trouv√©e dans les variables d'environnement\")\n",
    "   api_key = \" 0yCT3BlbkFJ6nuNH0hRFwAy9HhFfHS_cUMhXQMX6_U0pycw_XiZUUtZ4V6Gc5xEwhMZOsYA6xKN4HruNnPRcA\"  # √Ä remplacer par votre vraie cl√©\n",
    "\n",
    "# Initialiser le LLM\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not api_key:\n",
    "    print(\"‚ö†Ô∏è ATTENTION: Utilisez une variable d'environnement  \")\n",
    "    api_key = \" eoK0yCT3BlbkFJ6nuNH0hRFwAy9HhFfHS_cUMhXQMX6_U0pycw_XiZUUtZ4V6Gc5xEwhMZOsYA6xKN4HruNnPRcA\"\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96066b0c",
   "metadata": {
    "id": "96066b0c"
   },
   "source": [
    "\n",
    "\n",
    "#### Descriptif des fonctions par phases\n",
    "\n",
    "##### PHASE EXTRACTION (Phases 1-2)\n",
    "\n",
    "| Fonction | Description | Phase d√©taill√©e |\n",
    "|----------|-------------|-----------------|\n",
    "| `rechercher_fichiers_filtres()` | Recherche et filtre les fichiers par extension, date, taille | Phase 1 |\n",
    "| `copier_fichiers()` | Copie les fichiers vers le dossier de travail | Phase 2 |\n",
    "\n",
    "##### PHASE ANALYSE S√âMANTIQUE (Core + Phase 3)\n",
    "\n",
    "| Fonction | Description | Phase d√©taill√©e |\n",
    "|----------|-------------|-----------------|\n",
    "| `analyser_titre_avec_llm_semantique()` | Analyse s√©mantique d'un titre avec GPT-4o-mini | Core |\n",
    "| `analyser_et_classer_semantique()` | Classification automatique avec LLM | Phase 3 |\n",
    "\n",
    "##### PHASE RAPPORT (Phases 4-7)\n",
    "\n",
    "| Fonction | Description | Phase d√©taill√©e |\n",
    "|----------|-------------|-----------------|\n",
    "| `creer_dossiers_et_deplacer()` | Cr√©ation des dossiers et d√©placement des fichiers | Phase 4 |\n",
    "| `consolider_resultats_globaux()` | Consolidation dans le dossier `results_global` | Phase 5 |\n",
    "| `generer_rapport_final()` | G√©n√©ration des rapports JSON et TXT | Phase 6 |\n",
    "| `deposer_vers_sharepoint()` | D√©p√¥t des fichiers vers SharePoint (optionnel) | Phase 7 |\n",
    "\n",
    "---\n",
    " Fonction Cl√© : `analyser_titre_avec_llm_semantique()`\n",
    " \n",
    "- **Input** : Titre de document (string)\n",
    "- **Output** : Dictionnaire avec classification, clients d√©tect√©s, domaines m√©dicaux, justification\n",
    "\n",
    "#### Flux global du processus\n",
    "```\n",
    "EXTRACTION ‚Üí ANALYSE S√âMANTIQUE ‚Üí RAPPORT\n",
    "   ‚Üì              ‚Üì                 ‚Üì\n",
    "Phase 1-2      Core + Phase 3    Phase 4-7\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2134608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finition des filtres\n",
    "DATE_LIMITE = datetime.now() - timedelta(days=365)  # Fichiers de moins d'un an\n",
    "TAILLE_CLASSIFICATION = (\n",
    "        2000 * 1024\n",
    ")  # MODIFIEZ CETTE VALEUR pour changer la taille minimum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7ec276eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rechercher_fichiers_filtres():\n",
    "    \"\"\"Phase 1: Recherche et filtre les fichiers par extension, date et taille\"\"\"\n",
    "    print(\"üîç PHASE 1: Recherche et filtrage des fichiers...\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    if not os.path.exists(CHEMIN_SOURCE):\n",
    "        print(f\"‚ùå Erreur: {CHEMIN_SOURCE} n'existe pas\")\n",
    "        return []\n",
    "\n",
    "    \n",
    "    print(\n",
    "        f\"üìÖ Filtre date: fichiers modifi√©s apr√®s le {DATE_LIMITE.strftime('%d/%m/%Y')}\"\n",
    "    )\n",
    "    print(f\"üìè Filtre taille: minimum {TAILLE_CLASSIFICATION//1024} KB\")\n",
    "    print(f\"üìÅ Seuls les fichiers > {TAILLE_CLASSIFICATION//1024} KB sont retenus\")\n",
    "    print()\n",
    "\n",
    "    fichiers_trouves = []\n",
    "    dossiers_stats = {}\n",
    "    stats_filtrage = {\n",
    "        \"total_examines\": 0,\n",
    "        \"rejetes_extension\": 0,\n",
    "        \"rejetes_taille\": 0,\n",
    "        \"rejetes_date\": 0,\n",
    "        \"rejetes_acces\": 0,\n",
    "        \"acceptes\": 0,\n",
    "    }\n",
    "\n",
    "    for item in os.listdir(CHEMIN_SOURCE):\n",
    "        item_path = os.path.join(CHEMIN_SOURCE, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            print(f\"üìÇ Analyse du dossier: {item}\")\n",
    "            count_dossier = 0\n",
    "\n",
    "            for root, dirs, files in os.walk(item_path):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    stats_filtrage[\"total_examines\"] += 1\n",
    "\n",
    "                    # V√©rification de l'extension\n",
    "                    if not file.lower().endswith((\".ppt\", \".pptx\", \".pdf\")):\n",
    "                        stats_filtrage[\"rejetes_extension\"] += 1\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        # R√©cup√©ration des m√©tadonn√©es\n",
    "                        taille = os.path.getsize(file_path)\n",
    "                        date_mod = datetime.fromtimestamp(os.path.getmtime(file_path))\n",
    "\n",
    "                        # Filtrage par taille - SEUL FILTRE DE TAILLE\n",
    "                        if taille < TAILLE_CLASSIFICATION:\n",
    "                            stats_filtrage[\"rejetes_taille\"] += 1\n",
    "                            continue\n",
    "\n",
    "                        # Filtrage par date\n",
    "                        if date_mod < DATE_LIMITE:\n",
    "                            stats_filtrage[\"rejetes_date\"] += 1\n",
    "                            continue\n",
    "\n",
    "                        # Fichier accept√©\n",
    "                        fichier_info = {\n",
    "                            \"chemin\": file_path,\n",
    "                            \"nom\": file,\n",
    "                            \"taille\": taille,\n",
    "                            \"date_modification\": date_mod,\n",
    "                            \"taille_mb\": round(taille / (1024 * 1024), 2),\n",
    "                            \"taille_kb\": round(taille / 1024, 1),\n",
    "                            \"age_jours\": (datetime.now() - date_mod).days,\n",
    "                            \"classification_semantique\": True,  # Tous les fichiers retenus sont > TAILLE_CLASSIFICATION\n",
    "                        }\n",
    "\n",
    "                        fichiers_trouves.append(fichier_info)\n",
    "                        count_dossier += 1\n",
    "                        stats_filtrage[\"acceptes\"] += 1\n",
    "\n",
    "                    except (OSError, PermissionError) as e:\n",
    "                        stats_filtrage[\"rejetes_acces\"] += 1\n",
    "                        print(\n",
    "                            f\"   ‚ö†Ô∏è Ignor√© (acc√®s refus√©): {os.path.basename(file_path)}\"\n",
    "                        )\n",
    "\n",
    "            dossiers_stats[item] = count_dossier\n",
    "            print(f\"   ‚úÖ {count_dossier} fichier(s) retenu(s) apr√®s filtrage\")\n",
    "\n",
    "    # Affichage des statistiques d√©taill√©es\n",
    "    print(f\"\\nüìä STATISTIQUES DE FILTRAGE:\")\n",
    "    print(f\"   ‚Ä¢ Fichiers examin√©s: {stats_filtrage['total_examines']}\")\n",
    "    print(f\"   ‚Ä¢ ‚ùå Rejet√©s - extension: {stats_filtrage['rejetes_extension']}\")\n",
    "    print(f\"   ‚Ä¢ ‚ùå Rejet√©s - taille: {stats_filtrage['rejetes_taille']}\")\n",
    "    print(f\"   ‚Ä¢ ‚ùå Rejet√©s - date: {stats_filtrage['rejetes_date']}\")\n",
    "    print(f\"   ‚Ä¢ ‚ùå Rejet√©s - acc√®s: {stats_filtrage['rejetes_acces']}\")\n",
    "    print(f\"   ‚Ä¢ ‚úÖ RETENUS: {stats_filtrage['acceptes']}\")\n",
    "\n",
    "    print(f\"\\nüìÅ R√âPARTITION PAR DOSSIER:\")\n",
    "    for dossier, count in dossiers_stats.items():\n",
    "        print(f\"   ‚Ä¢ {dossier}: {count} fichier(s)\")\n",
    "\n",
    "    return fichiers_trouves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b2ab56",
   "metadata": {},
   "outputs": [],
   "source": [
    "fichiers_info = rechercher_fichiers_filtres()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eff6b2",
   "metadata": {},
   "source": [
    "### Fonction `copier_fichiers`\n",
    "\n",
    "#### Description\n",
    "Copie une liste de fichiers vers le dossier `Depots` avec affichage des informations de progression.\n",
    "\n",
    "1. **Cr√©ation du dossier** : Cr√©e le dossier `CHEMIN_DEPOTS` s'il n'existe pas\n",
    "2. **Copie des fichiers** : It√®re sur chaque fichier et le copie vers la destination\n",
    "3. **Gestion d'erreurs** : Affiche les erreurs de copie sans interrompre le processus\n",
    "4. **Statistiques** : Affiche le nombre de fichiers copi√©s et la taille totale\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "277ae04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copier_fichiers(fichiers_info):\n",
    "    \"\"\"Phase 2: Copie vers Depots avec informations enrichies\"\"\"\n",
    "    print(f\"\\nüìã PHASE 2: Copie vers {CHEMIN_DEPOTS}...\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Cr√©er le dossier Depots\n",
    "    os.makedirs(CHEMIN_DEPOTS, exist_ok=True)\n",
    "    print(f\"‚úÖ Dossier Depots pr√™t\")\n",
    "\n",
    "    copied = 0\n",
    "    taille_totale = 0\n",
    "\n",
    "    for fichier_info in fichiers_info:\n",
    "        file_path = fichier_info[\"chemin\"]\n",
    "        file_name = fichier_info[\"nom\"]\n",
    "        destination = os.path.join(CHEMIN_DEPOTS, file_name)\n",
    "\n",
    "        try:\n",
    "            shutil.copy(file_path, destination)\n",
    "            copied += 1\n",
    "            taille_totale += fichier_info[\"taille\"]\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur copie {file_name}: {e}\")\n",
    "\n",
    "    print(f\"‚úÖ {copied}/{len(fichiers_info)} fichiers copi√©s\")\n",
    "    print(f\"üì¶ Taille totale copi√©e: {taille_totale/(1024*1024):.1f} MB\")\n",
    "    return copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc64879",
   "metadata": {},
   "outputs": [],
   "source": [
    "copied = copier_fichiers(fichiers_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321bd412",
   "metadata": {},
   "source": [
    "### Fonction `analyser_titre_avec_llm_semantique`\n",
    "\n",
    "#### Description\n",
    "Analyse le titre d'un document avec un LLM pour extraire automatiquement les informations m√©tier (client, maladie, base de donn√©es, type de document).\n",
    "\n",
    " \n",
    "#### Valeur de retour\n",
    "Dictionnaire contenant :\n",
    "- `contexte_principal` : \"Data\" ou \"autre\"\n",
    "- `domaine_medical` : Domaine m√©dical d√©tect√© ou \"null\"\n",
    "- `type_document` : Type de document (suivi, rapport, √©tude, etc.)\n",
    "- `clients_detectes` : Nom du client principal ou \"null\"\n",
    "- `bases_detectees` : Base de donn√©es d√©tect√©e ou \"null\"\n",
    "- `contient_maladie` : Boolean\n",
    "- `confiance` : \"haute\", \"moyenne\" ou \"faible\"\n",
    "- `score_semantique` : Score de 0.1 √† 0.95\n",
    "- `categorie_medicale` : Cat√©gorie pour le classement\n",
    "\n",
    "#### Fonctionnement\n",
    "\n",
    "1. **V√©rification LLM** : V√©rifie si le LLM est disponible\n",
    "2. **Analyse s√©mantique** : Envoie un prompt d√©taill√© au LLM pour analyser le contexte\n",
    "3. **Classification** : D√©termine la cat√©gorie selon la priorit√© :\n",
    "   - `domaine_medical` si maladie d√©tect√©e\n",
    "   - `base_donnees` si base d√©tect√©e  \n",
    "   - `client` si client d√©tect√©\n",
    "   - `autres` sinon\n",
    "4. **Calcul du score** : √âvalue la qualit√© de l'analyse\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082690e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys  # Ajout de la biblioth√®que sys\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def analyser_titre_avec_llm_semantique(titre: str) -> dict:\n",
    "\n",
    "        \"\"\"Analyse s√©mantique avanc√©e avec LLM - prompt enrichi\"\"\"\n",
    "        if not llm:\n",
    "            print(f\"‚ùå LLM non disponible, impossible d'analyser '{titre[:30]}...'\")\n",
    "            return {\n",
    "                \"contient_maladie\": False,\n",
    "                \"maladies_detectees\": [],\n",
    "                \"categorie_medicale\": \"autres\",\n",
    "                \"contexte_principal\": \"autre\",\n",
    "                \"clients_detectes\": [],\n",
    "                \"bases_detectees\": [],\n",
    "                \"confiance\": \"faible\",\n",
    "                \"titre_normalise\": titre,\n",
    "                \"score_semantique\": 0.1,\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            analysis_prompt_template = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    (\n",
    "                        \"system\",\n",
    "                        \"\"\"Tu es un expert en analyse s√©mantique m√©dicale et business pharmaceutique.\n",
    "                Ton r√¥le est de comprendre le CONTEXTE et l'INTENTION derri√®re chaque titre, pas seulement chercher des mots-cl√©s.\n",
    "\n",
    "\n",
    "                \"\"\",\n",
    "                    ),\n",
    "                    (\n",
    "                        \"human\",\n",
    "                        prompt,\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            prompt_value = analysis_prompt_template.invoke({\"titre\": titre})\n",
    "            response = llm.invoke(prompt_value.to_messages())\n",
    "            content = response.content.strip()\n",
    "\n",
    "            # Nettoyer le JSON\n",
    "            if \"```json\" in content:\n",
    "                content = content.split(\"```json\")[1].split(\"```\")[0]\n",
    "            elif \"```\" in content:\n",
    "                content = content.split(\"```\")[1].split(\"```\")[0]\n",
    "\n",
    "            resultat = json.loads(content.strip())\n",
    "            print(f\"üîç DEBUG resultat: {resultat}\")\n",
    "\n",
    "            # Validation et enrichissement\n",
    "            if \"categorie_medicale\" not in resultat:\n",
    "                resultat[\"categorie_medicale\"] = \"autres\"  # Fallback par d√©faut\n",
    "\n",
    "            # NOUVELLE logique simplifi√©e pour 4 dossiers - ORDRE CORRIG√â\n",
    "            if (\n",
    "                resultat.get(\"domaine_medical\")\n",
    "                and resultat.get(\"domaine_medical\") != \"null\"\n",
    "            ):\n",
    "                resultat[\"categorie_medicale\"] = \"domaine_medical\"\n",
    "            elif (\n",
    "                resultat.get(\"bases_detectees\")\n",
    "                and resultat.get(\"bases_detectees\") != \"null\"\n",
    "            ):\n",
    "                resultat[\"categorie_medicale\"] = \"base_donnees\"\n",
    "            elif resultat.get(\"clients_detectes\") and resultat.get(\n",
    "                \"clients_detectes\"\n",
    "            ) not in [\"null\", \"aucun\"]:\n",
    "                resultat[\"categorie_medicale\"] = \"client\"\n",
    "            else:\n",
    "                resultat[\"categorie_medicale\"] = \"autres\"\n",
    "\n",
    "            # Calculer score s√©mantique si absent\n",
    "            if \"score_semantique\" not in resultat or not isinstance(\n",
    "                resultat[\"score_semantique\"], (int, float)\n",
    "            ):\n",
    "                contexte = resultat.get(\"contexte_principal\", \"autre\")\n",
    "                domaine = resultat.get(\"domaine_medical\", \"aucun\")\n",
    "\n",
    "                score = 0.3  # Base\n",
    "                if contexte != \"autre\":\n",
    "                    score += 0.2\n",
    "                if domaine != \"aucun\":\n",
    "                    score += 0.2\n",
    "                if len(resultat.get(\"maladies_detectees\", [])) > 0:\n",
    "                    score += 0.2\n",
    "                if resultat.get(\"confiance\") == \"haute\":\n",
    "                    score += 0.1\n",
    "\n",
    "                resultat[\"score_semantique\"] = min(0.95, score)\n",
    "\n",
    "            return resultat\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur LLM s√©mantique pour '{titre[:30]}...': {e}\")\n",
    "            return {\n",
    "                \"contient_maladie\": False,\n",
    "                \"maladies_detectees\": [],\n",
    "                \"categorie_medicale\": \"autres\",\n",
    "                \"contexte_principal\": \"autre\",\n",
    "                \"clients_detectes\": [],\n",
    "                \"bases_detectees\": [],\n",
    "                \"confiance\": \"faible\",\n",
    "                \"titre_normalise\": titre,\n",
    "                \"score_semantique\": 0.1,\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea002970",
   "metadata": {},
   "source": [
    "### Fonction `analyser_et_classer_semantique`\n",
    "\n",
    "#### Description\n",
    "Analyse et classe automatiquement les documents selon leur taille : analyse s√©mantique LLM pour les gros fichiers (> 2MB), classement automatique dans \"AUTRES\" pour les petits fichiers.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b271a14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyser_et_classer_semantique():\n",
    "    \"\"\"Phase 3: Analyse s√©mantique et classification (sans filtrage redondant)\"\"\"\n",
    "    print(f\"\\nüß† PHASE 3: Analyse s√©mantique et classification...\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Mode LLM ou fallback\n",
    "    use_llm = llm is not None\n",
    "    print(f\"Mode: {'üî• LLM S√©mantique' if use_llm else 'üîß Mots-cl√©s enrichis'}\")\n",
    "\n",
    "    fichiers_a_analyser = [\n",
    "        f\n",
    "        for f in os.listdir(CHEMIN_DEPOTS)\n",
    "        if f.lower().endswith((\".ppt\", \".pptx\", \".pdf\"))\n",
    "    ]\n",
    "\n",
    "    analyses = []\n",
    "    medicaux = 0\n",
    "    contextes_stats = {}\n",
    "\n",
    "    print(f\"üìä Fichiers √† analyser: {len(fichiers_a_analyser)}\")\n",
    "    print()\n",
    "\n",
    "    for i, file_name in enumerate(fichiers_a_analyser, 1):\n",
    "        titre = os.path.splitext(file_name)[0]\n",
    "\n",
    "        # R√©cup√©rer la taille pour affichage (optionnel)\n",
    "        try:\n",
    "            file_path = os.path.join(CHEMIN_DEPOTS, file_name)\n",
    "            taille = os.path.getsize(file_path)\n",
    "            taille_mb = round(taille / (1024 * 1024), 2)\n",
    "            taille_kb = round(taille / 1024, 1)\n",
    "            taille_str = f\"{taille_kb} KB\" if taille_mb < 1 else f\"{taille_mb} MB\"\n",
    "        except:\n",
    "            taille_str = \"Taille inconnue\"\n",
    "            taille_mb = 0\n",
    "\n",
    "        print(f\"[{i:2d}/{len(fichiers_a_analyser)}] {file_name[:40]}... ({taille_str})\")\n",
    "\n",
    "        # Classification s√©mantique directe (plus de filtrage)\n",
    "        analyse = analyser_titre_avec_llm_semantique(titre)\n",
    "\n",
    "        # Enrichir avec m√©tadonn√©es\n",
    "        analyse[\"nom_fichier\"] = file_name\n",
    "        analyse[\"taille_mb\"] = taille_mb\n",
    "\n",
    "        # Compter et afficher r√©sultats\n",
    "        if analyse.get(\"contient_maladie\", False):\n",
    "            medicaux += 1\n",
    "\n",
    "        contexte = analyse.get(\"contexte_principal\", \"autre\")\n",
    "        contextes_stats[contexte] = contextes_stats.get(contexte, 0) + 1\n",
    "\n",
    "        # Affichage du r√©sultat\n",
    "        categorie = analyse.get(\"categorie_medicale\", \"aucune\")\n",
    "        print(f\"üîç DEBUG !!!!!!!!!!!!!!!!!!!!! categorie: {categorie}\")\n",
    "\n",
    "        confiance = analyse.get(\"confiance\", \"inconnue\")\n",
    "        score = analyse.get(\"score_semantique\", 0)\n",
    "\n",
    "        if analyse.get(\"clients_detectes\"):\n",
    "            client_info = f\" [Client: {', '.join(analyse['clients_detectes'])}]\"\n",
    "        elif analyse.get(\"bases_detectees\"):\n",
    "            client_info = f\" [Base: {', '.join(analyse['bases_detectees'])}]\"\n",
    "        else:\n",
    "            client_info = \"\"\n",
    "\n",
    "        print(\n",
    "            f\"   üéØ {categorie.upper()} ({confiance}, score:{score:.2f}){client_info}\"\n",
    "        )\n",
    "\n",
    "        if analyse.get(\"maladies_detectees\"):\n",
    "            print(f\"   üè• Termes: {', '.join(analyse['maladies_detectees'])}\")\n",
    "\n",
    "        analyses.append(analyse)\n",
    "\n",
    "    print(f\"\\nüìä R√âSULTATS DE CLASSIFICATION:\")\n",
    "    print(f\"   ‚Ä¢ Fichiers analys√©s: {len(analyses)}\")\n",
    "    print(f\"   ‚Ä¢ Fichiers m√©dicaux d√©tect√©s: {medicaux}\")\n",
    "    print(\n",
    "        f\"   ‚Ä¢ Score s√©mantique moyen: {sum(a.get('score_semantique', 0) for a in analyses) / len(analyses):.2f}\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nüìà R√âPARTITION PAR CONTEXTE:\")\n",
    "    for contexte, count in sorted(contextes_stats.items()):\n",
    "        pct = (count / len(analyses) * 100) if analyses else 0\n",
    "        print(f\"   ‚Ä¢ {contexte}: {count} fichiers ({pct:.1f}%)\")\n",
    "\n",
    "    return analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de605468",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyses = analyser_et_classer_semantique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fda6fc",
   "metadata": {},
   "source": [
    "### Fonction `creer_dossiers_et_deplacer`\n",
    "\n",
    "#### Description\n",
    "Cr√©e les dossiers de classification et d√©place les fichiers analys√©s dans les bonnes cat√©gories selon leur analyse s√©mantique.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e84d104a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creer_dossiers_et_deplacer(analyses):\n",
    "    \"\"\"Phase 4: Cr√©ation des dossiers de classification et d√©placement\"\"\"\n",
    "    print(f\"\\nüìÅ PHASE 4: Cr√©ation dossiers et d√©placement...\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Mapping simplifi√© pour 4 dossiers\n",
    "    mapping_dossiers = {\n",
    "        \"client\": \"CLIENTS\",\n",
    "        \"base_donnees\": \"BASES_DONNEES\",\n",
    "        \"domaine_medical\": \"MALADIES\",  # ‚Üê MAINTENANT √áA MARCHE !\n",
    "        \"autres\": \"AUTRES\",\n",
    "    }\n",
    "\n",
    "    # Cr√©er tous les dossiers n√©cessaires\n",
    "    dossiers_crees = set()\n",
    "    for categorie in mapping_dossiers.values():\n",
    "        dossier_path = os.path.join(CHEMIN_DEPOTS, categorie)\n",
    "        os.makedirs(dossier_path, exist_ok=True)\n",
    "        dossiers_crees.add(categorie)\n",
    "\n",
    "    print(f\"‚úÖ {len(dossiers_crees)} dossiers de classification cr√©√©s\")\n",
    "\n",
    "    # D√©placer les fichiers\n",
    "    stats_deplacement = {}\n",
    "    erreurs_deplacement = []\n",
    "\n",
    "    for analyse in analyses:\n",
    "        file_name = analyse[\"nom_fichier\"]\n",
    "        categorie = analyse.get(\"categorie_medicale\", \"autres\")\n",
    "        dossier_cible = mapping_dossiers.get(categorie, \"AUTRES\")\n",
    "\n",
    "        source_path = os.path.join(CHEMIN_DEPOTS, file_name)\n",
    "        destination_path = os.path.join(CHEMIN_DEPOTS, dossier_cible, file_name)\n",
    "\n",
    "        try:\n",
    "            if os.path.exists(source_path):\n",
    "                # G√©rer les doublons\n",
    "                if os.path.exists(destination_path):\n",
    "                    base, ext = os.path.splitext(file_name)\n",
    "                    counter = 1\n",
    "                    while os.path.exists(destination_path):\n",
    "                        new_name = f\"{base}_({counter}){ext}\"\n",
    "                        destination_path = os.path.join(\n",
    "                            CHEMIN_DEPOTS, dossier_cible, new_name\n",
    "                        )\n",
    "                        counter += 1\n",
    "\n",
    "                shutil.move(source_path, destination_path)\n",
    "                stats_deplacement[dossier_cible] = (\n",
    "                    stats_deplacement.get(dossier_cible, 0) + 1\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            erreurs_deplacement.append(f\"{file_name}: {str(e)}\")\n",
    "            print(f\"‚ùå Erreur d√©placement {file_name}: {e}\")\n",
    "\n",
    "    print(f\"\\nüìä R√âSULTATS DE D√âPLACEMENT:\")\n",
    "    total_deplaces = sum(stats_deplacement.values())\n",
    "    for dossier, count in sorted(stats_deplacement.items()):\n",
    "        pct = (count / total_deplaces * 100) if total_deplaces > 0 else 0\n",
    "        print(f\"   ‚Ä¢ {dossier}: {count} fichiers ({pct:.1f}%)\")\n",
    "\n",
    "    if erreurs_deplacement:\n",
    "        print(f\"\\n‚ùå ERREURS DE D√âPLACEMENT ({len(erreurs_deplacement)}):\")\n",
    "        for erreur in erreurs_deplacement[:5]:  # Limiter √† 5 erreurs\n",
    "            print(f\"   ‚Ä¢ {erreur}\")\n",
    "        if len(erreurs_deplacement) > 5:\n",
    "            print(f\"   ‚Ä¢ ... et {len(erreurs_deplacement) - 5} autres erreurs\")\n",
    "\n",
    "    return stats_deplacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89536f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_deplacement = creer_dossiers_et_deplacer(analyses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a67e47",
   "metadata": {},
   "source": [
    "### Fonction `consolider_resultats_globaux`\n",
    "\n",
    "#### Description\n",
    "Copie tous les fichiers class√©s des diff√©rents dossiers vers un dossier unique `results_global` pour faciliter l'acc√®s et la consultation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "aafffb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolider_resultats_globaux():\n",
    "    \"\"\"Phase 6: Consolidation de tous les fichiers class√©s dans un dossier global\"\"\"\n",
    "    print(f\"\\nüìÅ PHASE 6: Consolidation dans results_global...\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Cr√©er le dossier results_global\n",
    "    results_path = os.path.join(CHEMIN_DEPOTS, \"results_global\")\n",
    "    os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "    # Liste des dossiers √† consolider (tous sauf results_global)\n",
    "    dossiers_a_consolider = [\n",
    "        \"CLIENTS\",\n",
    "        \"BASES_DONNEES\",\n",
    "        \"MALADIES\",  # ‚Üê 3 dossiers seulement\n",
    "    ]\n",
    "\n",
    "    fichiers_copies = 0\n",
    "    erreurs_copie = []\n",
    "\n",
    "    for dossier in dossiers_a_consolider:\n",
    "        dossier_path = os.path.join(CHEMIN_DEPOTS, dossier)\n",
    "\n",
    "        if os.path.exists(dossier_path):\n",
    "            print(f\"üìÇ Consolidation de {dossier}...\")\n",
    "\n",
    "            for file_name in os.listdir(dossier_path):\n",
    "                if file_name.lower().endswith((\".ppt\", \".pptx\", \".pdf\")):\n",
    "                    source = os.path.join(dossier_path, file_name)\n",
    "                    destination = os.path.join(results_path, file_name)\n",
    "\n",
    "                    try:\n",
    "                        # G√©rer les doublons avec pr√©fixe du dossier source\n",
    "                        if os.path.exists(destination):\n",
    "                            name, ext = os.path.splitext(file_name)\n",
    "                            new_name = f\"{dossier}_{name}{ext}\"\n",
    "                            destination = os.path.join(results_path, new_name)\n",
    "\n",
    "                        shutil.copy2(\n",
    "                            source, destination\n",
    "                        )  # copy2 pr√©serve les m√©tadonn√©es\n",
    "                        fichiers_copies += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        erreurs_copie.append(f\"{dossier}/{file_name}: {str(e)}\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Consolidation termin√©e:\")\n",
    "    print(f\"   ‚Ä¢ {fichiers_copies} fichiers copi√©s dans {results_path}\")\n",
    "    print(\n",
    "        f\"   ‚Ä¢ {len(erreurs_copie)} erreurs\" if erreurs_copie else \"   ‚Ä¢ Aucune erreur\"\n",
    "    )\n",
    "\n",
    "    if erreurs_copie:\n",
    "        print(f\"\\n‚ùå ERREURS DE COPIE:\")\n",
    "        for erreur in erreurs_copie[:3]:\n",
    "            print(f\"   ‚Ä¢ {erreur}\")\n",
    "        if len(erreurs_copie) > 3:\n",
    "            print(f\"   ‚Ä¢ ... et {len(erreurs_copie) - 3} autres erreurs\")\n",
    "\n",
    "    return fichiers_copies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52a8517",
   "metadata": {},
   "outputs": [],
   "source": [
    "fichiers_consolides = consolider_resultats_globaux()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb159842",
   "metadata": {},
   "source": [
    "### Fonction `generer_rapport_final`\n",
    "\n",
    "#### Description\n",
    "G√©n√®re un rapport d√©taill√© de l'ensemble du processus de classification sous deux formats : JSON (donn√©es structur√©es) et TXT (rapport lisible).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e38c3480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generer_rapport_final(analyses, stats_deplacement):\n",
    "    \"\"\"Phase 5: G√©n√©ration du rapport final d√©taill√©\"\"\"\n",
    "    print(f\"\\nüìã PHASE 5: G√©n√©ration du rapport final...\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    TAILLE_CLASSIFICATION_MB = 2700 / 1024  # Convertir 1000 KB en MB = 0.98 MB\n",
    "\n",
    "    # Cr√©er le rapport JSON d√©taill√©\n",
    "    rapport = {\n",
    "        \"metadata\": {\n",
    "            \"date_execution\": datetime.now().isoformat(),\n",
    "            \"version_script\": \"2.0\",\n",
    "            \"mode_llm\": llm is not None,\n",
    "            \"total_fichiers\": len(analyses),\n",
    "        },\n",
    "        \"statistiques_globales\": {\n",
    "            \"fichiers_medicaux\": sum(\n",
    "                1 for a in analyses if a.get(\"contient_maladie\", False)\n",
    "            ),\n",
    "            \"fichiers_clients\": sum(1 for a in analyses if a.get(\"clients_detectes\")),\n",
    "            \"fichiers_bases_donnees\": sum(\n",
    "                1 for a in analyses if a.get(\"bases_detectees\")\n",
    "            ),\n",
    "            \"score_semantique_moyen\": (\n",
    "                round(\n",
    "                    sum(a.get(\"score_semantique\", 0) for a in analyses) / len(analyses),\n",
    "                    3,\n",
    "                )\n",
    "                if analyses\n",
    "                else 0\n",
    "            ),\n",
    "        },\n",
    "        \"repartition_dossiers\": stats_deplacement,\n",
    "        \"analyses_detaillees\": [\n",
    "            a for a in analyses if a.get(\"taille_mb\", 0) >= TAILLE_CLASSIFICATION_MB\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Sauvegarder le rapport JSON\n",
    "    rapport_path = os.path.join(CHEMIN_DEPOTS, \"rapport_classification.json\")\n",
    "    try:\n",
    "        with open(rapport_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(rapport, f, indent=2, ensure_ascii=False, default=str)\n",
    "        print(f\"‚úÖ Rapport JSON sauvegard√©: {rapport_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur sauvegarde rapport: {e}\")\n",
    "\n",
    "    # G√©n√©rer rapport texte lisible\n",
    "    rapport_txt_path = os.path.join(CHEMIN_DEPOTS, \"rapport_classification.txt\")\n",
    "    try:\n",
    "        with open(rapport_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"=\" * 80 + \"\\n\")\n",
    "            f.write(\"RAPPORT DE CLASSIFICATION M√âDICALE AUTOMATIQUE\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "\n",
    "            f.write(\n",
    "                f\"üìÖ Date d'ex√©cution: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\\n\"\n",
    "            )\n",
    "            f.write(f\"üîß Mode: {'LLM S√©mantique' if llm else 'Mots-cl√©s enrichis'}\\n\")\n",
    "            f.write(f\"üìÅ Dossier source: {CHEMIN_SOURCE}\\n\")\n",
    "            f.write(f\"üìÅ Dossier d√©p√¥t: {CHEMIN_DEPOTS}\\n\\n\")\n",
    "\n",
    "            f.write(\"üìä STATISTIQUES GLOBALES\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            f.write(f\"Total fichiers trait√©s: {len(analyses)}\\n\")\n",
    "            f.write(\n",
    "                f\"Fichiers m√©dicaux: {rapport['statistiques_globales']['fichiers_medicaux']}\\n\"\n",
    "            )\n",
    "            f.write(\n",
    "                f\"Fichiers clients: {rapport['statistiques_globales']['fichiers_clients']}\\n\"\n",
    "            )\n",
    "            f.write(\n",
    "                f\"Fichiers bases donn√©es: {rapport['statistiques_globales']['fichiers_bases_donnees']}\\n\"\n",
    "            )\n",
    "            f.write(\n",
    "                f\"Score s√©mantique moyen: {rapport['statistiques_globales']['score_semantique_moyen']}\\n\\n\"\n",
    "            )\n",
    "\n",
    "            f.write(\"üìÅ R√âPARTITION PAR DOSSIERS\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            for dossier, count in sorted(stats_deplacement.items()):\n",
    "                pct = (count / len(analyses) * 100) if analyses else 0\n",
    "                f.write(f\"{dossier}: {count} fichiers ({pct:.1f}%)\\n\")\n",
    "\n",
    "            # Comptage du dossier results_global\n",
    "            results_global_path = os.path.join(CHEMIN_DEPOTS, \"results_global\")\n",
    "            if os.path.exists(results_global_path):\n",
    "                fichiers_consolides = len(\n",
    "                    [\n",
    "                        f\n",
    "                        for f in os.listdir(results_global_path)\n",
    "                        if f.lower().endswith((\".ppt\", \".pptx\", \".pdf\"))\n",
    "                    ]\n",
    "                )\n",
    "                f.write(f\"\\nüìÇ CONSOLIDATION GLOBALE\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\n\")\n",
    "                f.write(f\"RESULTS_GLOBAL: {fichiers_consolides} fichiers consolid√©s\\n\")\n",
    "\n",
    "        print(f\"‚úÖ Rapport texte sauvegard√©: {rapport_txt_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur sauvegarde rapport texte: {e}\")\n",
    "\n",
    "    return rapport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6002f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "rapport = generer_rapport_final(analyses, stats_deplacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc99b615",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381692e0",
   "metadata": {
    "id": "381692e0"
   },
   "outputs": [],
   "source": [
    "#def deposer_vers_sharepoint():\n",
    "    \"\"\"Phase 7: D√©p√¥t du dossier results_global vers SharePoint\"\"\"\n",
    "    print(f\"\\n‚òÅÔ∏è PHASE 7: D√©p√¥t vers SharePoint...\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    try:\n",
    "        from office365.runtime.auth.authentication_context import AuthenticationContext\n",
    "        from office365.sharepoint.client_context import ClientContext\n",
    "        import requests\n",
    "\n",
    "        # Configuration SharePoint (√† adapter selon vos param√®tres)\n",
    "        SHAREPOINT_URL = \"https://votreentreprise.sharepoint.com/sites/votre-site\"\n",
    "        SHAREPOINT_USERNAME = os.getenv(\"SHAREPOINT_USERNAME\")\n",
    "        SHAREPOINT_PASSWORD = os.getenv(\"SHAREPOINT_PASSWORD\")\n",
    "        SHAREPOINT_LIBRARY = (\n",
    "            \"Documents Partag√©s/Classification_Medicale\"  # Dossier de destination\n",
    "        )\n",
    "\n",
    "        # V√©rifier les param√®tres\n",
    "        if not all([SHAREPOINT_USERNAME, SHAREPOINT_PASSWORD]):\n",
    "            print(\"‚ùå Erreur: Variables d'environnement SharePoint manquantes\")\n",
    "            print(\"   D√©finissez SHAREPOINT_USERNAME et SHAREPOINT_PASSWORD\")\n",
    "            return False\n",
    "\n",
    "        # Chemin du dossier local √† uploader\n",
    "        results_path = os.path.join(CHEMIN_DEPOTS, \"results_global\")\n",
    "        if not os.path.exists(results_path):\n",
    "            print(f\"‚ùå Erreur: Dossier {results_path} n'existe pas\")\n",
    "            return False\n",
    "\n",
    "        # Authentification SharePoint\n",
    "        print(\"üîê Authentification SharePoint...\")\n",
    "        auth_context = AuthenticationContext(url=SHAREPOINT_URL)\n",
    "        auth_context.acquire_token_for_user(\n",
    "            username=SHAREPOINT_USERNAME, password=SHAREPOINT_PASSWORD\n",
    "        )\n",
    "\n",
    "        ctx = ClientContext(SHAREPOINT_URL, auth_context)\n",
    "        web = ctx.web\n",
    "        ctx.load(web)\n",
    "        ctx.execute_query()\n",
    "\n",
    "        print(f\"‚úÖ Connect√© √† SharePoint: {web.properties['Title']}\")\n",
    "\n",
    "        # Cr√©er le dossier de destination si n√©cessaire\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        destination_folder = f\"{SHAREPOINT_LIBRARY}/Classification_{timestamp}\"\n",
    "\n",
    "        # Upload des fichiers\n",
    "        fichiers_uploades = 0\n",
    "        erreurs_upload = []\n",
    "\n",
    "        print(f\"üìÇ Upload vers: {destination_folder}\")\n",
    "\n",
    "        for file_name in os.listdir(results_path):\n",
    "            if file_name.lower().endswith((\".ppt\", \".pptx\", \".pdf\")):\n",
    "                file_path = os.path.join(results_path, file_name)\n",
    "\n",
    "                try:\n",
    "                    with open(file_path, \"rb\") as file_content:\n",
    "                        # Upload du fichier\n",
    "                        target_folder = ctx.web.get_folder_by_server_relative_url(\n",
    "                            destination_folder\n",
    "                        )\n",
    "                        target_folder.upload_file(file_name, file_content.read())\n",
    "                        ctx.execute_query()\n",
    "\n",
    "                        fichiers_uploades += 1\n",
    "                        print(f\"   ‚úÖ {file_name}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    erreurs_upload.append(f\"{file_name}: {str(e)}\")\n",
    "                    print(f\"   ‚ùå {file_name}: {e}\")\n",
    "\n",
    "        print(f\"\\nüìä R√âSULTATS UPLOAD SHAREPOINT:\")\n",
    "        print(f\"   ‚Ä¢ {fichiers_uploades} fichiers upload√©s\")\n",
    "        print(f\"   ‚Ä¢ {len(erreurs_upload)} erreurs\")\n",
    "        print(f\"   ‚Ä¢ Destination: {destination_folder}\")\n",
    "\n",
    "        if erreurs_upload:\n",
    "            print(f\"\\n‚ùå ERREURS D'UPLOAD:\")\n",
    "            for erreur in erreurs_upload[:3]:\n",
    "                print(f\"   ‚Ä¢ {erreur}\")\n",
    "            if len(erreurs_upload) > 3:\n",
    "                print(f\"   ‚Ä¢ ... et {len(erreurs_upload) - 3} autres erreurs\")\n",
    "\n",
    "        return fichiers_uploades > 0\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"‚ùå Erreur: Biblioth√®que Office365 manquante\")\n",
    "        print(\"   Installez avec: pip install Office365-REST-Python-Client\")\n",
    "        return False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur SharePoint: {e}\")\n",
    "        return False"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
